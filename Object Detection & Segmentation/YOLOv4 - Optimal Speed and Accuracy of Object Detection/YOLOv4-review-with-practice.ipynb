{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **YOLOv4: Optimal Speed and Accuracy of Object Detection**\n",
    "\n",
    "**Authors: Alexey Bochkovskiy {alexeyab84@gmail.com}, Chien-Yao Wang {kinyiu@iis.sinica.edu.tw}, Hong-Yuan Mark Liao {liao@iis.sinica.edu.tw}**\n",
    "\n",
    "**Official Github**: https://github.com/AlexeyAB/darknet\n",
    "\n",
    "---\n",
    "\n",
    "**Edited By Su Hyung Choi - [Computer Vision Paper Reviews]**\n",
    "\n",
    "**[Github: @JonyChoi]** https://github.com/jonychoi/Computer-Vision-Paper-Reviews\n",
    "\n",
    "Edited Jan 7 2022\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Abstract\n",
    "\n",
    "There are a huge number of features which are said to\n",
    "improve Convolutional Neural Network (CNN) accuracy.\n",
    "Practical testing of combinations of such features on large\n",
    "datasets, and theoretical justification of the result, is required. Some features operate on certain models exclusively\n",
    "and for certain problems exclusively, or only for small-scale\n",
    "datasets; while some features, such as batch-normalization\n",
    "and residual-connections, are applicable to the majority of\n",
    "models, tasks, and datasets. We assume that such universal\n",
    "features include Weighted-Residual-Connections (WRC),\n",
    "Cross-Stage-Partial-connections (CSP), Cross mini-Batch\n",
    "Normalization (CmBN), Self-adversarial-training (SAT)\n",
    "and Mish-activation. We use new features: WRC, CSP,\n",
    "CmBN, SAT, Mish activation, Mosaic data augmentation,\n",
    "CmBN, DropBlock regularization, and CIoU loss, and combine some of them to achieve state-of-the-art results: 43.5%\n",
    "AP (65.7% AP50) for the MS COCO dataset at a realtime speed of ∼65 FPS on Tesla V100. Source code is at\n",
    "https://github.com/AlexeyAB/darknet.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Introduction\n",
    "\n",
    "The majority of CNN-based object detectors are largely\n",
    "applicable only for recommendation systems. For example,\n",
    "searching for free parking spaces via urban video cameras\n",
    "is executed by slow accurate models, whereas car collision\n",
    "warning is related to fast inaccurate models. Improving\n",
    "the real-time object detector accuracy enables using them\n",
    "not only for hint generating recommendation systems, but\n",
    "also for stand-alone process management and human input\n",
    "reduction. Real-time object detector operation on conventional Graphics Processing Units (GPU) allows their mass\n",
    "usage at an affordable price. The most accurate modern\n",
    "neural networks do not operate in real time and require large\n",
    "number of GPUs for training with a large mini-batch-size.\n",
    "We address such problems through creating a CNN that operates in real-time on a conventional GPU, and for which\n",
    "training requires only one conventional GPU.\n",
    "\n",
    "<table>\n",
    "  <tr>\n",
    "      <td>\n",
    "        <img src=\"./figure1.png\" style=\"width: 350px\">\n",
    "      </td>\n",
    "      <td valign=\"bottom\">\n",
    "        Figure 1: Comparison of the proposed YOLOv4 and other\n",
    "        state-of-the-art object detectors. YOLOv4 runs twice faster\n",
    "        than EfficientDet with comparable performance. Improves\n",
    "        YOLOv3’s AP and FPS by 10% and 12%, respectively.\n",
    "    </td>\n",
    "  </tr>\n",
    "</table>\n",
    "\n",
    "The main goal of this work is designing a fast operating speed of an object detector in production systems and optimization for parallel computations, rather than the low computation volume theoretical indicator (BFLOP). We hope that the designed object can be easily trained and used. For example, anyone who uses a conventional GPU to train and test can achieve real-time, high quality, and convincing object detection results, as the YOLOv4 results shown in Figure 1. Our contributions are summarized as follows:\n",
    "\n",
    "1. We develope an efficient and powerful object detection model. It makes everyone can use a 1080 Ti or 2080 Ti GPU to train a super fast and accurate object detector.\n",
    "\n",
    "2. We verify the influence of state-of-the-art Bag-ofFreebies and Bag-of-Specials methods of object detection during the detector training.\n",
    "\n",
    "3. We modify state-of-the-art methods and make them more effecient and suitable for single GPU training, \n",
    "including CBN [89], PAN [49], SAM [85], etc.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./figure2.png\" width=\"90%\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Related work\n",
    "#### 2.1. Object detection models\n",
    "\n",
    "A modern detector is usually composed of two parts,\n",
    "a backbone which is pre-trained on ImageNet and a head\n",
    "which is used to predict classes and bounding boxes of objects. For those detectors running on GPU platform, their\n",
    "backbone could be VGG [68], ResNet [26], ResNeXt [86],\n",
    "or DenseNet [30]. For those detectors running on CPU platform, their backbone could be SqueezeNet [31], MobileNet\n",
    "[28, 66, 27, 74], or ShuffleNet [97, 53]. As to the head part,\n",
    "it is usually categorized into two kinds, i.e., one-stage object\n",
    "detector and two-stage object detector. The most representative two-stage object detector is the R-CNN [19] series,\n",
    "including fast R-CNN [18], faster R-CNN [64], R-FCN [9],\n",
    "and Libra R-CNN [58]. It is also possible to make a twostage object detector an anchor-free object detector, such as\n",
    "RepPoints [87]. As for one-stage object detector, the most\n",
    "representative models are YOLO [61, 62, 63], SSD [50],\n",
    "and RetinaNet [45]. In recent years, anchor-free one-stage\n",
    "object detectors are developed. The detectors of this sort are\n",
    "CenterNet [13], CornerNet [37, 38], FCOS [78], etc. Object\n",
    "detectors developed in recent years often insert some layers between backbone and head, and these layers are usually used to collect feature maps from different stages. We\n",
    "can call it the neck of an object detector. Usually, a neck\n",
    "is composed of several bottom-up paths and several topdown paths. Networks equipped with this mechanism include Feature Pyramid Network (FPN) [44], Path Aggregation Network (PAN) [49], BiFPN [77], and NAS-FPN [17].\n",
    "In addition to the above models, some researchers put their\n",
    "emphasis on directly building a new backbone (DetNet [43],\n",
    "DetNAS [7]) or a new whole model (SpineNet [12], HitDetector [20]) for object detection.\n",
    "To sum up, an ordinary object detector is composed of\n",
    "several parts:\n",
    "• Input: Image, Patches, Image Pyramid\n",
    "• Backbones: VGG16 [68], ResNet-50 [26], SpineNet\n",
    "[12], EfficientNet-B0/B7 [75], CSPResNeXt50 [81],\n",
    "CSPDarknet53 [81]\n",
    "• Neck:\n",
    "• Additional blocks: SPP [25], ASPP [5], RFB\n",
    "[47], SAM [85]\n",
    "• Path-aggregation blocks: FPN [44], PAN [49],\n",
    "NAS-FPN [17], Fully-connected FPN, BiFPN\n",
    "[77], ASFF [48], SFAM [98]\n",
    "• Heads::\n",
    "• Dense Prediction (one-stage):\n",
    "◦ RPN [64], SSD [50], YOLO [61], RetinaNet\n",
    "[45] (anchor based)\n",
    "◦ CornerNet [37], CenterNet [13], MatrixNet\n",
    "[60], FCOS [78] (anchor free)\n",
    "• Sparse Prediction (two-stage):\n",
    "◦ Faster R-CNN [64], R-FCN [9], Mask RCNN [23] (anchor based)\n",
    "◦ RepPoints [87] (anchor free)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "f8574845d26991fb924b9b73a047d47daa16a02e6e1ac35bb3c12f8621974ea3"
  },
  "kernelspec": {
   "display_name": "Python 3.6.13 64-bit ('buddhalight3.6': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.6.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
