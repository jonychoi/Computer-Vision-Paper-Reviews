{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **YOLOv4: Optimal Speed and Accuracy of Object Detection**\n",
    "\n",
    "**Authors: Alexey Bochkovskiy {alexeyab84@gmail.com}, Chien-Yao Wang {kinyiu@iis.sinica.edu.tw}, Hong-Yuan Mark Liao {liao@iis.sinica.edu.tw}**\n",
    "\n",
    "**Official Github**: https://github.com/AlexeyAB/darknet\n",
    "\n",
    "---\n",
    "\n",
    "**Edited By Su Hyung Choi - [Computer Vision Paper Reviews]**\n",
    "\n",
    "**[Github: @JonyChoi]** https://github.com/jonychoi/Computer-Vision-Paper-Reviews\n",
    "\n",
    "Edited Jan 7 2022\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Abstract**\n",
    "\n",
    "There are a huge number of features which are said to\n",
    "improve Convolutional Neural Network (CNN) accuracy.\n",
    "Practical testing of combinations of such features on large\n",
    "datasets, and theoretical justification of the result, is required. Some features operate on certain models exclusively\n",
    "and for certain problems exclusively, or only for small-scale\n",
    "datasets; while some features, such as batch-normalization\n",
    "and residual-connections, are applicable to the majority of\n",
    "models, tasks, and datasets. We assume that such universal\n",
    "features include Weighted-Residual-Connections (WRC),\n",
    "Cross-Stage-Partial-connections (CSP), Cross mini-Batch\n",
    "Normalization (CmBN), Self-adversarial-training (SAT)\n",
    "and Mish-activation. We use new features: WRC, CSP,\n",
    "CmBN, SAT, Mish activation, Mosaic data augmentation,\n",
    "CmBN, DropBlock regularization, and CIoU loss, and combine some of them to achieve state-of-the-art results: 43.5%\n",
    "AP (65.7% AP50) for the MS COCO dataset at a realtime speed of ∼65 FPS on Tesla V100. Source code is at\n",
    "https://github.com/AlexeyAB/darknet.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **1. Introduction**\n",
    "\n",
    "The majority of CNN-based object detectors are largely\n",
    "applicable only for recommendation systems. For example,\n",
    "searching for free parking spaces via urban video cameras\n",
    "is executed by slow accurate models, whereas car collision\n",
    "warning is related to fast inaccurate models. Improving\n",
    "the real-time object detector accuracy enables using them\n",
    "not only for hint generating recommendation systems, but\n",
    "also for stand-alone process management and human input\n",
    "reduction. Real-time object detector operation on conventional Graphics Processing Units (GPU) allows their mass\n",
    "usage at an affordable price. The most accurate modern\n",
    "neural networks do not operate in real time and require large\n",
    "number of GPUs for training with a large mini-batch-size.\n",
    "We address such problems through creating a CNN that operates in real-time on a conventional GPU, and for which\n",
    "training requires only one conventional GPU.\n",
    "\n",
    "<table>\n",
    "  <tr>\n",
    "      <td>\n",
    "        <img src=\"./figure1.png\" style=\"width: 350px\">\n",
    "      </td>\n",
    "      <td valign=\"bottom\">\n",
    "        Figure 1: Comparison of the proposed YOLOv4 and other\n",
    "        state-of-the-art object detectors. YOLOv4 runs twice faster\n",
    "        than EfficientDet with comparable performance. Improves\n",
    "        YOLOv3’s AP and FPS by 10% and 12%, respectively.\n",
    "    </td>\n",
    "  </tr>\n",
    "</table>\n",
    "\n",
    "The main goal of this work is designing a fast operating speed of an object detector in production systems and optimization for parallel computations, rather than the low computation volume theoretical indicator (BFLOP). We hope that the designed object can be easily trained and used. For example, anyone who uses a conventional GPU to train and test can achieve real-time, high quality, and convincing object detection results, as the YOLOv4 results shown in Figure 1. Our contributions are summarized as follows:\n",
    "\n",
    "1. **We develope an efficient and powerful object detection model. It makes everyone can use a 1080 Ti or 2080 Ti GPU to train a super fast and accurate object detector.**\n",
    "\n",
    "2. **We verify the influence of state-of-the-art Bag-ofFreebies and Bag-of-Specials methods of object detection during the detector training.**\n",
    "\n",
    "3. **We modify state-of-the-art methods and make them more effecient and suitable for single GPU training, \n",
    "including CBN [89], PAN [49], SAM [85], etc.**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **2. Related work**\n",
    "#### **2.1. Object detection models**\n",
    "\n",
    "A modern detector is usually composed of two parts,\n",
    "a backbone which is pre-trained on ImageNet and a head\n",
    "which is used to predict classes and bounding boxes of objects. For those detectors running on GPU platform, their\n",
    "backbone could be VGG [68], ResNet [26], ResNeXt [86],\n",
    "or DenseNet [30]. For those detectors running on CPU platform, their backbone could be SqueezeNet [31], MobileNet\n",
    "[28, 66, 27, 74], or ShuffleNet [97, 53]. As to the head part,\n",
    "it is usually categorized into two kinds, i.e., one-stage object\n",
    "detector and two-stage object detector. The most representative two-stage object detector is the R-CNN [19] series,\n",
    "including fast R-CNN [18], faster R-CNN [64], R-FCN [9],\n",
    "and Libra R-CNN [58]. It is also possible to make a twostage object detector an anchor-free object detector, such as\n",
    "RepPoints [87]. As for one-stage object detector, the most\n",
    "representative models are YOLO [61, 62, 63], SSD [50],\n",
    "and RetinaNet [45]. In recent years, anchor-free one-stage\n",
    "object detectors are developed. The detectors of this sort are\n",
    "CenterNet [13], CornerNet [37, 38], FCOS [78], etc. Object\n",
    "detectors developed in recent years often insert some layers between backbone and head, and these layers are usually used to collect feature maps from different stages. We\n",
    "can call it the neck of an object detector. Usually, a neck\n",
    "is composed of several bottom-up paths and several topdown paths. Networks equipped with this mechanism include Feature Pyramid Network (FPN) [44], Path Aggregation Network (PAN) [49], BiFPN [77], and NAS-FPN [17].\n",
    "In addition to the above models, some researchers put their\n",
    "emphasis on directly building a new backbone (DetNet [43],\n",
    "DetNAS [7]) or a new whole model (SpineNet [12], HitDetector [20]) for object detection.\n",
    "To sum up, an ordinary object detector is composed of\n",
    "several parts:\n",
    "\n",
    "<img src=\"./figure2.png\" width=\"85%\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **Input**: Image, Patches, Image Pyramid\n",
    "\n",
    "- **Backbones**: VGG16 [68], ResNet-50 [26], SpineNet [12], EfficientNet-B0/B7 [75], CSPResNeXt50 [81], CSPDarknet53 [81]\n",
    "\n",
    "- **Neck**:\n",
    "    - Additional blocks: SPP [25], ASPP [5], RFB [47], SAM [85]\n",
    "    - Path-aggregation blocks: FPN [44], PAN [49], NAS-FPN [17], Fully-connected FPN, BiFPN [77], ASFF [48], SFAM [98]\n",
    "\n",
    "- **Heads::**\n",
    "    - **Dense Prediction (one-stage)**:\n",
    "        - RPN [64], SSD [50], YOLO [61], RetinaNet [45] (anchor based)\n",
    "        - CornerNet [37], CenterNet [13], MatrixNet [60], FCOS [78] (anchor free)\n",
    "    - **Sparse Prediction (two-stage)**:\n",
    "        - Faster R-CNN [64], R-FCN [9], Mask RCNN [23] (anchor based)\n",
    "        - RepPoints [87] (anchor free)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **2.2 Bag of freebies**\n",
    "\n",
    "Usually, a conventional object detector is trained offline. Therefore, researchers always like to take this advantage and develop better training methods which can make\n",
    "the object detector receive better accuracy without increasing the inference cost. We call these methods that only\n",
    "change the training strategy or only increase the training\n",
    "cost as “bag of freebies.” What is often adopted by object\n",
    "detection methods and meets the definition of bag of freebies is data augmentation. The purpose of data augmentation is to increase the variability of the input images, so that\n",
    "the designed object detection model has higher robustness\n",
    "to the images obtained from different environments. For\n",
    "examples, photometric distortions and geometric distortions\n",
    "are two commonly used data augmentation method and they\n",
    "definitely benefit the object detection task. In dealing with\n",
    "photometric distortion, we adjust the brightness, contrast,\n",
    "hue, saturation, and noise of an image. For geometric distortion, we add random scaling, cropping, flipping, and rotating.\n",
    "\n",
    "\n",
    "The data augmentation methods mentioned above are all\n",
    "pixel-wise adjustments, and all original pixel information in\n",
    "the adjusted area is retained. In addition, some researchers\n",
    "engaged in data augmentation put their emphasis on simulating object occlusion issues. They have achieved good\n",
    "results in image classification and object detection. For example, random erase [100] and CutOut [11] can randomly\n",
    "select the rectangle region in an image and fill in a random\n",
    "or complementary value of zero. As for hide-and-seek [69]\n",
    "and grid mask [6], they randomly or evenly select multiple\n",
    "rectangle regions in an image and replace them to all zeros. If similar concepts are applied to feature maps, there\n",
    "are DropOut [71], DropConnect [80], and DropBlock [16]\n",
    "methods. In addition, some researchers have proposed the\n",
    "methods of using multiple images together to perform data\n",
    "augmentation. For example, MixUp [92] uses two images\n",
    "to multiply and superimpose with different coefficient ratios, and then adjusts the label with these superimposed ratios. As for CutMix [91], it is to cover the cropped image\n",
    "to rectangle region of other images, and adjusts the label\n",
    "according to the size of the mix area. In addition to the\n",
    "above mentioned methods, style transfer GAN [15] is also\n",
    "used for data augmentation, and such usage can effectively\n",
    "reduce the texture bias learned by CNN.\n",
    "\n",
    "\n",
    "Different from the various approaches proposed above,\n",
    "some other bag of freebies methods are dedicated to solving\n",
    "the problem that the semantic distribution in the dataset may\n",
    "have bias. In dealing with the problem of semantic distribution bias, a very important issue is that there is a problem\n",
    "of data imbalance between different classes, and this problem is often solved by hard negative example mining [72]\n",
    "or online hard example mining [67] in two-stage object detector. But the example mining method is not applicable to one-stage object detector, because this kind of detector\n",
    "belongs to the dense prediction architecture. Therefore Lin\n",
    "et al. [45] proposed focal loss to deal with the problem\n",
    "of data imbalance existing between various classes. Another very important issue is that it is difficult to express the\n",
    "relationship of the degree of association between different\n",
    "categories with the one-hot hard representation. This representation scheme is often used when executing labeling.\n",
    "The label smoothing proposed in [73] is to convert hard label into soft label for training, which can make model more\n",
    "robust. In order to obtain a better soft label, Islam et al. [33]\n",
    "introduced the concept of knowledge distillation to design\n",
    "the label refinement network.\n",
    "\n",
    "The last bag of freebies is the objective function of\n",
    "Bounding Box (BBox) regression. The traditional object\n",
    "detector usually uses Mean Square Error (MSE) to directly perform regression on the center point coordinates\n",
    "and height and width of the BBox, i.e., {xcenter, ycenter,\n",
    "w, h}, or the upper left point and the lower right point,\n",
    "i.e., {xtop lef t, ytop lef t, xbottom right, ybottom right}. As\n",
    "for anchor-based method, it is to estimate the corresponding offset, for example {xcenter of f set, ycenter of f set,\n",
    "wof f set, hof f set} and {xtop lef t of f set, ytop lef t of f set,\n",
    "xbottom right of f set, ybottom right of f set}. However, to directly estimate the coordinate values of each point of the\n",
    "BBox is to treat these points as independent variables, but\n",
    "in fact does not consider the integrity of the object itself. In\n",
    "order to make this issue processed better, some researchers\n",
    "recently proposed IoU loss [90], which puts the coverage of\n",
    "predicted BBox area and ground truth BBox area into consideration. The IoU loss computing process will trigger the\n",
    "calculation of the four coordinate points of the BBox by executing IoU with the ground truth, and then connecting the\n",
    "generated results into a whole code. Because IoU is a scale\n",
    "invariant representation, it can solve the problem that when\n",
    "traditional methods calculate the l1 or l2 loss of {x, y, w,\n",
    "h}, the loss will increase with the scale. Recently, some\n",
    "researchers have continued to improve IoU loss. For example, GIoU loss [65] is to include the shape and orientation\n",
    "of object in addition to the coverage area. They proposed to\n",
    "find the smallest area BBox that can simultaneously cover\n",
    "the predicted BBox and ground truth BBox, and use this\n",
    "BBox as the denominator to replace the denominator originally used in IoU loss. As for DIoU loss [99], it additionally\n",
    "considers the distance of the center of an object, and CIoU\n",
    "loss [99], on the other hand simultaneously considers the\n",
    "overlapping area, the distance between center points, and\n",
    "the aspect ratio. CIoU can achieve better convergence speed\n",
    "and accuracy on the BBox regression problem.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **2.3. Bag of specials**\n",
    "\n",
    "For those plugin modules and post-processing methods\n",
    "that only increase the inference cost by a small amount\n",
    "but can significantly improve the accuracy of object detection, we call them “bag of specials”. Generally speaking,\n",
    "these plugin modules are for enhancing certain attributes in\n",
    "a model, such as enlarging receptive field, introducing attention mechanism, or strengthening feature integration capability, etc., and post-processing is a method for screening\n",
    "model prediction results.\n",
    "\n",
    "Common modules that can be used to enhance receptive field are SPP [25], ASPP [5], and RFB [47]. The\n",
    "SPP module was originated from Spatial Pyramid Matching (SPM) [39], and SPMs original method was to split feature map into several d × d equal blocks, where d can be\n",
    "{1, 2, 3, ...}, thus forming spatial pyramid, and then extracting bag-of-word features. SPP integrates SPM into CNN\n",
    "and use max-pooling operation instead of bag-of-word operation. Since the SPP module proposed by He et al. [25]\n",
    "will output one dimensional feature vector, it is infeasible to\n",
    "be applied in Fully Convolutional Network (FCN). Thus in\n",
    "the design of YOLOv3 [63], Redmon and Farhadi improve\n",
    "SPP module to the concatenation of max-pooling outputs\n",
    "with kernel size k × k, where k = {1, 5, 9, 13}, and stride\n",
    "equals to 1. Under this design, a relatively large k × k maxpooling effectively increase the receptive field of backbone\n",
    "feature. After adding the improved version of SPP module,\n",
    "YOLOv3-608 upgrades AP50 by 2.7% on the MS COCO\n",
    "object detection task at the cost of 0.5% extra computation.\n",
    "The difference in operation between ASPP [5] module and\n",
    "improved SPP module is mainly from the original k×k kernel size, max-pooling of stride equals to 1 to several 3 × 3\n",
    "kernel size, dilated ratio equals to k, and stride equals to 1\n",
    "in dilated convolution operation. RFB module is to use several dilated convolutions of k×k kernel, dilated ratio equals\n",
    "to k, and stride equals to 1 to obtain a more comprehensive\n",
    "spatial coverage than ASPP. RFB [47] only costs 7% extra\n",
    "inference time to increase the AP50 of SSD on MS COCO\n",
    "by 5.7%.\n",
    "\n",
    "The attention module that is often used in object detection is mainly divided into channel-wise attention and pointwise attention, and the representatives of these two attention models are Squeeze-and-Excitation (SE) [29] and Spatial Attention Module (SAM) [85], respectively. Although\n",
    "SE module can improve the power of ResNet50 in the ImageNet image classification task 1% top-1 accuracy at the\n",
    "cost of only increasing the computational effort by 2%, but\n",
    "on a GPU usually it will increase the inference time by\n",
    "about 10%, so it is more appropriate to be used in mobile\n",
    "devices. But for SAM, it only needs to pay 0.1% extra calculation and it can improve ResNet50-SE 0.5% top-1 accuracy on the ImageNet image classification task. Best of all,\n",
    "it does not affect the speed of inference on the GPU at all.\n",
    "\n",
    "In terms of feature integration, the early practice is to use\n",
    "skip connection [51] or hyper-column [22] to integrate lowlevel physical feature to high-level semantic feature. Since\n",
    "multi-scale prediction methods such as FPN have become\n",
    "popular, many lightweight modules that integrate different\n",
    "feature pyramid have been proposed. The modules of this\n",
    "sort include SFAM [98], ASFF [48], and BiFPN [77]. The\n",
    "main idea of SFAM is to use SE module to execute channelwise level re-weighting on multi-scale concatenated feature\n",
    "maps. As for ASFF, it uses softmax as point-wise level reweighting and then adds feature maps of different scales.\n",
    "In BiFPN, the multi-input weighted residual connections is\n",
    "proposed to execute scale-wise level re-weighting, and then\n",
    "add feature maps of different scales.\n",
    "\n",
    "In the research of deep learning, some people put their\n",
    "focus on searching for good activation function. A good\n",
    "activation function can make the gradient more efficiently\n",
    "propagated, and at the same time it will not cause too\n",
    "much extra computational cost. In 2010, Nair and Hinton [56] propose ReLU to substantially solve the gradient\n",
    "vanish problem which is frequently encountered in traditional tanh and sigmoid activation function. Subsequently,\n",
    "LReLU [54], PReLU [24], ReLU6 [28], Scaled Exponential\n",
    "Linear Unit (SELU) [35], Swish [59], hard-Swish [27], and\n",
    "Mish [55], etc., which are also used to solve the gradient\n",
    "vanish problem, have been proposed. The main purpose of\n",
    "LReLU and PReLU is to solve the problem that the gradient of ReLU is zero when the output is less than zero. As\n",
    "for ReLU6 and hard-Swish, they are specially designed for\n",
    "quantization networks. For self-normalizing a neural network, the SELU activation function is proposed to satisfy\n",
    "the goal. One thing to be noted is that both Swish and Mish\n",
    "are continuously differentiable activation function.\n",
    "\n",
    "The post-processing method commonly used in deeplearning-based object detection is NMS, which can be used\n",
    "to filter those BBoxes that badly predict the same object, and only retain the candidate BBoxes with higher response. The way NMS tries to improve is consistent with\n",
    "the method of optimizing an objective function. The original method proposed by NMS does not consider the context information, so Girshick et al. [19] added classification\n",
    "confidence score in R-CNN as a reference, and according to\n",
    "the order of confidence score, greedy NMS was performed\n",
    "in the order of high score to low score. As for soft NMS [1],\n",
    "it considers the problem that the occlusion of an object may\n",
    "cause the degradation of confidence score in greedy NMS\n",
    "with IoU score. The DIoU NMS [99] developers way of\n",
    "thinking is to add the information of the center point distance to the BBox screening process on the basis of soft\n",
    "NMS. It is worth mentioning that, since none of above postprocessing methods directly refer to the captured image features, post-processing is no longer required in the subsequent development of an anchor-free method.\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](./table1.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **3. Methodology**\n",
    "\n",
    "The basic aim is fast operating speed of neural network,\n",
    "in production systems and optimization for parallel computations, rather than the low computation volume theoretical indicator (BFLOP). We present two options of real-time\n",
    "neural networks:\n",
    "\n",
    "- For GPU we use a small number of groups (1 - 8) in\n",
    "convolutional layers: CSPResNeXt50 / CSPDarknet53\n",
    "- For VPU - we use grouped-convolution, but we refrain from using Squeeze-and-excitement (SE) blocks - specifically this includes the following models:\n",
    "EfficientNet-lite / MixNet [76] / GhostNet [21] / MobileNetV3\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **3.1. Selection of architecture**\n",
    "\n",
    "Our objective is to find the optimal balance among the input network resolution, the convolutional layer number, the\n",
    "parameter number (filter size2 * filters * channel / groups),\n",
    "and the number of layer outputs (filters). For instance, our\n",
    "numerous studies demonstrate that the CSPResNext50 is\n",
    "considerably better compared to CSPDarknet53 in terms\n",
    "of object classification on the ILSVRC2012 (ImageNet)\n",
    "dataset [10]. However, conversely, the CSPDarknet53 is\n",
    "better compared to CSPResNext50 in terms of detecting objects on the MS COCO dataset [46].\n",
    "\n",
    "The next objective is to select additional blocks for increasing the receptive field and the best method of parameter aggregation from different backbone levels for different\n",
    "detector levels: e.g. FPN, PAN, ASFF, BiFPN.\n",
    "\n",
    "A reference model which is optimal for classification is\n",
    "not always optimal for a detector. In contrast to the classifier, the detector requires the following:\n",
    "\n",
    "- Higher input network size (resolution) – for detecting\n",
    "multiple small-sized objects\n",
    "\n",
    "- More layers – for a higher receptive field to cover the\n",
    "increased size of input network\n",
    "\n",
    "- More parameters – for greater capacity of a model to\n",
    "detect multiple objects of different sizes in a single image\n",
    "\n",
    "Hypothetically speaking, we can assume that a model\n",
    "with a larger receptive field size (with a larger number of\n",
    "convolutional layers 3 × 3) and a larger number of parameters should be selected as the backbone. Table 1 shows the\n",
    "information of CSPResNeXt50, CSPDarknet53, and EfficientNet B3. The CSPResNext50 contains only 16 convolutional layers 3 × 3, a 425 × 425 receptive field and 20.6\n",
    "M parameters, while CSPDarknet53 contains 29 convolutional layers 3 × 3, a 725 × 725 receptive field and 27.6\n",
    "M parameters. This theoretical justification, together with\n",
    "our numerous experiments, show that CSPDarknet53 neural network is the optimal model of the two as the backbone\n",
    "for a detector.\n",
    "The influence of the receptive field with different sizes is\n",
    "summarized as follows:\n",
    "\n",
    "- Up to the object size - allows viewing the entire object\n",
    "- Up to network size - allows viewing the context around\n",
    "the object\n",
    "- Exceeding the network size - increases the number of\n",
    "connections between the image point and the final activation\n",
    "\n",
    "We add the SPP block over the CSPDarknet53, since it\n",
    "significantly increases the receptive field, separates out the\n",
    "most significant context features and causes almost no reduction of the network operation speed. We use PANet as\n",
    "the method of parameter aggregation from different backbone levels for different detector levels, instead of the FPN\n",
    "used in YOLOv3.\n",
    "\n",
    "Finally, we choose CSPDarknet53 backbone, SPP additional module, PANet path-aggregation neck, and YOLOv3\n",
    "(anchor based) head as the architecture of YOLOv4.\n",
    "\n",
    "In the future we plan to expand significantly the content\n",
    "of Bag of Freebies (BoF) for the detector, which theoretically can address some problems and increase the detector\n",
    "accuracy, and sequentially check the influence of each feature in an experimental fashion.\n",
    "\n",
    "We do not use Cross-GPU Batch Normalization (CGBN\n",
    "or SyncBN) or expensive specialized devices. This allows anyone to reproduce our state-of-the-art outcomes on\n",
    "a conventional graphic processor e.g. GTX 1080Ti or RTX\n",
    "2080Ti."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **3.2. Selection of BoF and BoS**\n",
    "\n",
    "For improving the object detection training, a CNN usually uses the following:\n",
    "\n",
    "- **Activations:** ReLU, leaky-ReLU, parametric-ReLU,\n",
    "ReLU6, SELU, Swish, or Mish\n",
    "- **Bounding box regression loss:** MSE, IoU, GIoU,\n",
    "CIoU, DIoU\n",
    "- **Data augmentation:** CutOut, MixUp, CutMix\n",
    "- **Regularization method:** DropOut, DropPath [36],\n",
    "Spatial DropOut [79], or DropBlock\n",
    "- **Normalization of the network activations by their\n",
    "mean and variance:** Batch Normalization (BN) [32],\n",
    "Cross-GPU Batch Normalization (CGBN or SyncBN)\n",
    "[93], Filter Response Normalization (FRN) [70], or\n",
    "Cross-Iteration Batch Normalization (CBN) [89]\n",
    "- **Skip-connections:** Residual connections, Weighted\n",
    "residual connections, Multi-input weighted residual\n",
    "connections, or Cross stage partial connections (CSP)\n",
    "\n",
    "As for training activation function, since PReLU and\n",
    "SELU are more difficult to train, and ReLU6 is specifically\n",
    "designed for quantization network, we therefore remove the\n",
    "above activation functions from the candidate list. In the\n",
    "method of reqularization, the people who published DropBlock have compared their method with other methods in\n",
    "detail, and their regularization method has won a lot. Therefore, we did not hesitate to choose DropBlock as our regularization method. As for the selection of normalization\n",
    "method, since we focus on a training strategy that uses only\n",
    "one GPU, syncBN is not considered.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **3.3. Additional improvements**\n",
    "\n",
    "In order to make the designed detector more suitable for\n",
    "training on single GPU, we made additional design and improvement as follows:\n",
    "\n",
    "- We introduce a new method of data augmentation Mosaic, and Self-Adversarial Training (SAT)\n",
    "- We select optimal hyper-parameters while applying\n",
    "genetic algorithms\n",
    "- We modify some exsiting methods to make our design\n",
    "suitble for efficient training and detection - modified\n",
    "SAM, modified PAN, and Cross mini-Batch Normalization (CmBN)\n",
    "\n",
    "Mosaic represents a new data augmentation method that\n",
    "mixes 4 training images. Thus 4 different contexts are mixed, while CutMix mixes only 2 input images. This allows detection of objects outside their normal context. In\n",
    "addition, batch normalization calculates activation statistics\n",
    "from 4 different images on each layer. This significantly\n",
    "reduces the need for a large mini-batch size.\n",
    "\n",
    "<img src=\"./figure3.png\" width=\"500\" float=\"left\">\n",
    "<img src=\"./figure4.png\" width=\"500\" float=\"right\">\n",
    "\n",
    "Self-Adversarial Training (SAT) also represents a new\n",
    "data augmentation technique that operates in 2 forward\n",
    "backward stages. In the 1st stage the neural network alters\n",
    "the original image instead of the network weights. In this\n",
    "way the neural network executes an adversarial attack on itself, altering the original image to create the deception that\n",
    "there is no desired object on the image. In the 2nd stage, the\n",
    "neural network is trained to detect an object on this modified\n",
    "image in the normal way.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "f8574845d26991fb924b9b73a047d47daa16a02e6e1ac35bb3c12f8621974ea3"
  },
  "kernelspec": {
   "display_name": "Python 3.6.13 64-bit ('buddhalight3.6': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.6.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
