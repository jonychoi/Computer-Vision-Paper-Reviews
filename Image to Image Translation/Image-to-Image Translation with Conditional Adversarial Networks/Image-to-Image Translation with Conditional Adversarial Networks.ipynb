{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Image to Image Translation with Conditional Adversarial Networks (CVPR2017)**\n",
    "\n",
    "**Written By Jonathan Choi**\n",
    "\n",
    "References from\n",
    "\n",
    "https://github.com/eriklindernoren/PyTorch-GAN\n",
    "\n",
    "https://github.com/ndb796/Deep-Learning-Paper-Review-and-Practice/blob/master/code_practices/Pix2Pix_for_Facades.ipynb\n",
    "\n",
    "### **Code Implement**\n",
    "\n",
    "- Here, we use the U-Net that is similar to original paper.\n",
    "- Implementing Pix2Pix model that is representative technique to Image GAN Domain Translation.\n",
    "- Dataset: Facades (3 x 256 x 256)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision\n",
    "\n",
    "import os\n",
    "import glob\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "import torchvision.transforms as transforms\n",
    "from torchvision.utils import save_image\n",
    "\n",
    "import torchvision.datasets as datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import Train Dataset\n",
    "\n",
    "From original Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current working directory: c:\\Users\\John Steve\\Desktop\\Deep-Learning-Paper-Reviews\\Image to Image Translation\\Image-to-Image Translation with Conditional Adversarial Networks\n",
      "Current working directory: c:\\Users\\John Steve\\Desktop\\Deep-Learning-Paper-Reviews\\datasets\n",
      "Directory  facades  already exists\n"
     ]
    }
   ],
   "source": [
    "###### Change Directory ######\n",
    "\n",
    "# Print the current working directory => root/Image to Image Translation/current directory\n",
    "print(\"Current working directory: {0}\".format(os.getcwd()))\n",
    "\n",
    "# Change the current working directory to datasets directory => root/datasets\n",
    "os.chdir('../../datasets')\n",
    "\n",
    "# Print the current working directory\n",
    "print(\"Current working directory: {0}\".format(os.getcwd()))\n",
    "\n",
    "\n",
    "\n",
    "###### Create Directory #######\n",
    "\n",
    "def createDataset():\n",
    "    # Move to created directory\n",
    "    os.chdir('./facades')\n",
    "\n",
    "    ###### Download Datasets #######\n",
    "    dataset_name = \"facades.zip\"\n",
    "\n",
    "    #Get the link\n",
    "    url = 'https://cmp.felk.cvut.cz/~tylecr1/facade/CMP_facade_DB_base.zip'\n",
    "    r = requests.get(url, allow_redirects=True)\n",
    "    #Save the content with name.\n",
    "    open(dataset_name, 'wb').write(r.content)\n",
    "\n",
    "    ##### Unzip the files ######\n",
    "    with ZipFile(dataset_name, 'r') as zipObj:\n",
    "        # Extract all the contents of zip file in current directory\n",
    "        zipObj.extractall()\n",
    "\n",
    "import requests\n",
    "from zipfile import ZipFile\n",
    "\n",
    "# Create directory\n",
    "dirName = 'facades'\n",
    "\n",
    "try:\n",
    "    # Create target Directory\n",
    "    os.mkdir(dirName)\n",
    "    print(\"Directory \" , dirName ,  \" Created \") \n",
    "    createDataset()\n",
    "\n",
    "except FileExistsError:\n",
    "    print(\"Directory \" , dirName ,  \" already exists\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, the dataset facades, would contain the 'base', 'facades.zip', 'label_names.txt', 'readme.txt'.\n",
    "\n",
    "Below, we will gonna create the train valid test dataset from base folder\n",
    "\n",
    "\n",
    "### Data Structure\n",
    "\n",
    "In the base folder, there will be cmp_b000n.jpg and cmp_b0001.png.\n",
    "\n",
    "- cmp_b000n.**jpg** => We will call this \"**A**\" (Real Image)\n",
    "\n",
    "- cmp_b000n.**png** => We will call this \"**B**\". (Condition Image)\n",
    "\n",
    "\n",
    "We will going to create Train A, Train B, Valid A, Valid B, Test A, Test B"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Count the Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "c:\\Users\\John Steve\\Desktop\\Deep-Learning-Paper-Reviews\\datasets\n",
      "1134\n"
     ]
    }
   ],
   "source": [
    "print(os.getcwd())\n",
    "path, dirs, files = next(os.walk(os.getcwd()+'/facades/base'))\n",
    "file_count = len(files)\n",
    "print(file_count)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Split the Dataset to TrainA, TrainB, ValidA, ValidB, TestA, TestB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "StopIteration",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mStopIteration\u001b[0m                             Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-4-074faf4b6565>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Number of train dataset A and B:\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnext\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwalk\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'./datasets/facades/train/'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Number of validation dataset A and B:\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnext\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwalk\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'./datasets/facades/val/'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Number of test dataset A and B:\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnext\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwalk\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'./datasets/facades/test/'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mStopIteration\u001b[0m: "
     ]
    }
   ],
   "source": [
    "print(\"Number of train dataset A and B:\", len(next(os.walk('./datasets/facades/train/'))[2]))\n",
    "print(\"Number of validation dataset A and B:\", len(next(os.walk('./datasets/facades/val/'))[2]))\n",
    "print(\"Number of test dataset A and B:\", len(next(os.walk('./datasets/facades/test/'))[2]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Image Preview\n",
    "\n",
    "Each train image has attached form of two image for each (256 x 256)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image = Image.open('./facades/train/1.jpg')\n",
    "print(\"Image size:\", image.size)\n",
    "\n",
    "plt.imshow(image)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Custom Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ImageDataset(Dataset):\n",
    "    def __init__(self, root, transforms_=None, mode=\"train\"):\n",
    "        self.transform = transforms_\n",
    "\n",
    "        self.files = sorted(glob.glob(os.path.join(root, mode) + \"/*.jpg\"))\n",
    "        #Use Test data at training since the dataset is small\n",
    "        if mode == \"train\":\n",
    "            self.files.extend(sorted(glob.glob(os.path.join(root, \"test\") + \"/*.jpg\")))\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        img = Image.open(self.files[index % len(self.files)])\n",
    "        w, h = img.size\n",
    "        img_A = img.crop((0, 0, w / 2, h)) # Image left half\n",
    "        img_B = img.crop((w / 2, 0, w, h)) # Image right half\n",
    "\n",
    "        #Horizontal Flip to Data Augumentation\n",
    "        if np.random.random() < 0.5:\n",
    "            img_A = Image.fromarray(np.array(img_A)[:, ::-1, :], \"RGB\")\n",
    "            img_B = Image.fromarray(np.array(img_B)[:, ::-1, :], \"RGB\")\n",
    "\n",
    "        img_A = self.transform(img_A)\n",
    "        img_B = self.transform(img_B)\n",
    "\n",
    "        return {\"A\": img_A, \"B\": img_B}\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transforms_ = transforms.Compose([\n",
    "    transforms.Resize((256, 256), Image.BICUBIC),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
    "])\n",
    "\n",
    "train_dataset = ImageDataset(\"facades\", transforms_=transforms_)\n",
    "val_dataset = ImageDataset(\"facades\", transforms_=transforms_)\n",
    "\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=10, shuffle=True, num_workers=4)\n",
    "val_dataloader = DataLoader(val_dataset, batch_size=10, shuffle=True, num_workers=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define Generator and Discriminator Models\n",
    "\n",
    "- Pix2Pix has a form of cGAN, printing the image with the condition of specific image.\n",
    "- To make same dimension of input and output, we use U-Net Architecture.\n",
    "\n",
    "\n",
    "<img src=\"https://blog.kakaocdn.net/dn/MEqfm/btqD2vOm4wM/ojgKsu3uZTG78WQPKgGeXK/img.png\" width=\"1000px\" />\n",
    "\n",
    "- U-Net Architecture uses skip-connection as below.\n",
    "- Many low-level information can be shared at the process of the input and output\n",
    "\n",
    "<img src=\"https://learnopencv.com/wp-content/uploads/2021/07/Pix2Pix-employs-a-UNET-Generator-an-encoder-decoder.jpg\" width=\"1000px\" />"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# U-Net Architecture's Down Sampling Module\n",
    "class UNetDown(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, normalize=True, dropout=0.0):\n",
    "        super(UNetDown, self).__init__()\n",
    "        # Reduce by half at each H, W\n",
    "        layers = [nn.Conv2d(in_channels, out_channels, kernel_size=4, stride=2, padding=1, bias=False)]\n",
    "        if normalize:\n",
    "            layers.append(nn.InstanceNorm2d(out_channels))\n",
    "        layers.append(nn.LeakyReLU(0.2))\n",
    "        if dropout:\n",
    "            layers.append(nn.Dropout(dropout))\n",
    "        self.model = nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "\n",
    "\n",
    "# U-Net Architecture's UpSampling Module: Use Skip Connection\n",
    "class UNetUp(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, dropout=0.0):\n",
    "        super(UNetUp, self).__init__()\n",
    "        # 너비와 높이가 2배씩 증가\n",
    "        layers = [nn.ConvTranspose2d(in_channels, out_channels, kernel_size=4, stride=2, padding=1, bias=False)]\n",
    "        layers.append(nn.InstanceNorm2d(out_channels))\n",
    "        layers.append(nn.ReLU(inplace=True))\n",
    "        if dropout:\n",
    "            layers.append(nn.Dropout(dropout))\n",
    "        self.model = nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x, skip_input):\n",
    "        x = self.model(x)\n",
    "        x = torch.cat((x, skip_input), 1) # Concatenation at Channel Level\n",
    "\n",
    "        return x\n",
    "\n",
    "\n",
    "# U-Net Generator Architecture\n",
    "class GeneratorUNet(nn.Module):\n",
    "    def __init__(self, in_channels=3, out_channels=3):\n",
    "        super(GeneratorUNet, self).__init__()\n",
    "\n",
    "        self.down1 = UNetDown(in_channels, 64, normalize=False) # Output: [64 X 128 X 128]\n",
    "        self.down2 = UNetDown(64, 128) # Output: [128 X 64 X 64]\n",
    "        self.down3 = UNetDown(128, 256) # Output: [256 X 32 X 32]\n",
    "        self.down4 = UNetDown(256, 512, dropout=0.5) # Output: [512 X 16 X 16]\n",
    "        self.down5 = UNetDown(512, 512, dropout=0.5) # Output: [512 X 8 X 8]\n",
    "        self.down6 = UNetDown(512, 512, dropout=0.5) # Output: [512 X 4 X 4]\n",
    "        self.down7 = UNetDown(512, 512, dropout=0.5) # Output: [512 X 2 X 2]\n",
    "        self.down8 = UNetDown(512, 512, normalize=False, dropout=0.5) # Output: [512 X 1 X 1]\n",
    "\n",
    "        # Skip Connection (Output Channel size X 2 == Next input Channel size)\n",
    "        self.up1 = UNetUp(512, 512, dropout=0.5) # Output: [1024 X 2 X 2]\n",
    "        self.up2 = UNetUp(1024, 512, dropout=0.5) # Output: [1024 X 4 X 4]\n",
    "        self.up3 = UNetUp(1024, 512, dropout=0.5) # Output: [1024 X 8 X 8]\n",
    "        self.up4 = UNetUp(1024, 512, dropout=0.5) # Output: [1024 X 16 X 16]\n",
    "        self.up5 = UNetUp(1024, 256) # Output: [512 X 32 X 32]\n",
    "        self.up6 = UNetUp(512, 128) # Output: [256 X 64 X 64]\n",
    "        self.up7 = UNetUp(256, 64) # Output: [128 X 128 X 128]\n",
    "\n",
    "        self.final = nn.Sequential(\n",
    "            nn.Upsample(scale_factor=2), # Output: [128 X 256 X 256]\n",
    "            nn.ZeroPad2d((1, 0, 1, 0)),\n",
    "            nn.Conv2d(128, out_channels, kernel_size=4, padding=1), # Output: [3 X 256 X 256]\n",
    "            nn.Tanh(),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        # U-Net Generator: Feed forwarding from encoder to decoder\n",
    "        d1 = self.down1(x)\n",
    "        d2 = self.down2(d1)\n",
    "        d3 = self.down3(d2)\n",
    "        d4 = self.down4(d3)\n",
    "        d5 = self.down5(d4)\n",
    "        d6 = self.down6(d5)\n",
    "        d7 = self.down7(d6)\n",
    "        d8 = self.down8(d7)\n",
    "        u1 = self.up1(d8, d7)\n",
    "        u2 = self.up2(u1, d6)\n",
    "        u3 = self.up3(u2, d5)\n",
    "        u4 = self.up4(u3, d4)\n",
    "        u5 = self.up5(u4, d3)\n",
    "        u6 = self.up6(u5, d2)\n",
    "        u7 = self.up7(u6, d1)\n",
    "\n",
    "        return self.final(u7)\n",
    "\n",
    "\n",
    "# U-Net Discriminator Architrecture\n",
    "class Discriminator(nn.Module):\n",
    "    def __init__(self, in_channels=3):\n",
    "        super(Discriminator, self).__init__()\n",
    "\n",
    "        def discriminator_block(in_channels, out_channels, normalization=True):\n",
    "            # Reduce by half of each H, W\n",
    "            layers = [nn.Conv2d(in_channels, out_channels, kernel_size=4, stride=2, padding=1)]\n",
    "            if normalization:\n",
    "                layers.append(nn.InstanceNorm2d(out_channels))\n",
    "            layers.append(nn.LeakyReLU(0.2, inplace=True))\n",
    "            return layers\n",
    "\n",
    "        self.model = nn.Sequential(\n",
    "            # Input Channel size doubled since the input is two image (real/ translated Image, Condition Image)\n",
    "            *discriminator_block(in_channels * 2, 64, normalization=False), # Output: [64 X 128 X 128]\n",
    "            *discriminator_block(64, 128), # Output: [128 X 64 X 64]\n",
    "            *discriminator_block(128, 256), # Output: [256 X 32 X 32]\n",
    "            *discriminator_block(256, 512), # Output: [512 X 16 X 16]\n",
    "            nn.ZeroPad2d((1, 0, 1, 0)),\n",
    "            nn.Conv2d(512, 1, kernel_size=4, padding=1, bias=False) # Output: [1 X 16 X 16]\n",
    "        )\n",
    "\n",
    "    # img_A: real / translated image, img_B: condition\n",
    "    def forward(self, img_A, img_B):\n",
    "        # Generating Input Data by concatenating two images at the Channel Level.\n",
    "        img_input = torch.cat((img_A, img_B), 1)\n",
    "        return self.model(img_input)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model training and Sampling\n",
    "\n",
    "- Initialize the Generator and Discriminator Model to train.\n",
    "- Set appropriate hyperparameters.\n",
    "- Set appropriate Loss Function.\n",
    "- Pix2Pix uses L1 loss to make the output image similar to ground-truth."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def weights_init_normal(m):\n",
    "    classname = m.__class__.__name__\n",
    "    if classname.find(\"Conv\") != -1:\n",
    "        torch.nn.init.normal_(m.weight.data, 0.0, 0.02)\n",
    "    elif classname.find(\"BatchNorm2d\") != -1:\n",
    "        torch.nn.init.normal_(m.weight.data, 1.0, 0.02)\n",
    "        torch.nn.init.constant_(m.bias.data, 0.0)\n",
    "\n",
    "\n",
    "# Initialize Generator and Discriminator\n",
    "generator = GeneratorUNet()\n",
    "discriminator = Discriminator()\n",
    "\n",
    "generator.cuda()\n",
    "discriminator.cuda()\n",
    "\n",
    "# Initialize Weights\n",
    "generator.apply(weights_init_normal)\n",
    "discriminator.apply(weights_init_normal)\n",
    "\n",
    "# Loss Function\n",
    "criterion_GAN = torch.nn.MSELoss()\n",
    "criterion_pixelwise = torch.nn.L1Loss()\n",
    "\n",
    "criterion_GAN.cuda()\n",
    "criterion_pixelwise.cuda()\n",
    "\n",
    "# Set Learning Rate\n",
    "lr = 0.0002\n",
    "\n",
    "# Optimize function to Generator and Discriminator\n",
    "optimizer_G = torch.optim.Adam(generator.parameters(), lr=lr, betas=(0.5, 0.999))\n",
    "optimizer_D = torch.optim.Adam(discriminator.parameters(), lr=lr, betas=(0.5, 0.999))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can check the results by sampling periodically while training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "n_epochs = 200 # training epochs\n",
    "sample_interval = 200 # set the interval to print the results\n",
    "\n",
    "# L1 pixel-wise weighted loss parameters between translated image and label image.\n",
    "lambda_pixel = 100\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "    for i, batch in enumerate(train_dataloader):\n",
    "        # import input data of model\n",
    "        real_A = batch[\"B\"].cuda()\n",
    "        real_B = batch[\"A\"].cuda()\n",
    "\n",
    "        # Generate the label of real and fake image (size of width and height divided by 16)\n",
    "        real = torch.cuda.FloatTensor(real_A.size(0), 1, 16, 16).fill_(1.0) # real: 1\n",
    "        fake = torch.cuda.FloatTensor(real_A.size(0), 1, 16, 16).fill_(0.0) # fake: 0\n",
    "\n",
    "        \"\"\" Train the Generator \"\"\"\n",
    "        optimizer_G.zero_grad()\n",
    "\n",
    "        # Generate the Image\n",
    "        fake_B = generator(real_A)\n",
    "\n",
    "        # Generator Loss\n",
    "        loss_GAN = criterion_GAN(discriminator(fake_B, real_A), real)\n",
    "\n",
    "        # Pixel-wise L1 Loss\n",
    "        loss_pixel = criterion_pixelwise(fake_B, real_B) \n",
    "\n",
    "        # Total Loss\n",
    "        loss_G = loss_GAN + lambda_pixel * loss_pixel\n",
    "\n",
    "        # Update Generator\n",
    "        loss_G.backward()\n",
    "        optimizer_G.step()\n",
    "\n",
    "        \"\"\" Train the Discriminator \"\"\"\n",
    "        optimizer_D.zero_grad()\n",
    "\n",
    "        # Discriminator Loss\n",
    "        loss_real = criterion_GAN(discriminator(real_B, real_A), real) # condition: real_A\n",
    "        loss_fake = criterion_GAN(discriminator(fake_B.detach(), real_A), fake)\n",
    "        loss_D = (loss_real + loss_fake) / 2\n",
    "\n",
    "        # Update the Discriminator\n",
    "        loss_D.backward()\n",
    "        optimizer_D.step()\n",
    "\n",
    "        done = epoch * len(train_dataloader) + i\n",
    "        if done % sample_interval == 0:\n",
    "            imgs = next(iter(val_dataloader)) # Generate by sampling 10 images\n",
    "            real_A = imgs[\"B\"].cuda()\n",
    "            real_B = imgs[\"A\"].cuda()\n",
    "            fake_B = generator(real_A)\n",
    "            # real_A: condition, fake_B: translated image, real_B: Label Image\n",
    "            img_sample = torch.cat((real_A.data, fake_B.data, real_B.data), -2) # Connect image by image height\n",
    "            save_image(img_sample, f\"{done}.png\", nrow=5, normalize=True)\n",
    "\n",
    "    # print log after 1 epoch finished.\n",
    "    print(f\"[Epoch {epoch}/{n_epochs}] [D loss: {loss_D.item():.6f}] [G pixel loss: {loss_pixel.item():.6f}, adv loss: {loss_GAN.item()}] [Elapsed time: {time.time() - start_time:.2f}s]\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Print the Generated Images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Image\n",
    "\n",
    "Image('10000.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save the trained Model and Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(generator.state_dict(), \"Pix2Pix_Generator_for_Facades.pt\")\n",
    "torch.save(discriminator.state_dict(), \"Pix2Pix_Discriminator_for_Facades.pt\")\n",
    "print(\"Model saved!\")"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "f8574845d26991fb924b9b73a047d47daa16a02e6e1ac35bb3c12f8621974ea3"
  },
  "kernelspec": {
   "display_name": "Python 3.6.13 64-bit ('buddhalight3.6': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
