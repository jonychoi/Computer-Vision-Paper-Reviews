{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Neighbourhood Consensus Networks**\n",
    "\n",
    "**Authors: Ignacio Rocco†, Mircea Cimpoi‡, Relja Arandjelovic´\n",
    "§, Akihiko Torii∗,\n",
    "Tomas Pajdla‡, Josef Sivic†,‡**\n",
    "\n",
    "**†Inria / ‡CIIRC, CTU in Prague / §DeepMind / ∗Tokyo Institute of Technology**\n",
    "\n",
    "**[Official Github Code](https://github.com/ignacio-rocco/ncnet)** / **[Project Page](https://www.di.ens.fr/willow/research/ncnet/)** / **[Pdf](https://arxiv.org/abs/1810.10510)**\n",
    "\n",
    "---\n",
    "\n",
    "**Edited By Su Hyung Choi (Key Summary & Code Practice)**\n",
    "\n",
    "If you have any issues on this scripts, please PR to the repository below.\n",
    "\n",
    "**[Github: @JonyChoi - Computer Vision Paper Reviews](https://github.com/jonychoi/Computer-Vision-Paper-Reviews)**\n",
    "\n",
    "Edited April 14 2022\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Ncnet - Neighbourhood Consensus Network**\n",
    "\n",
    "This is the code review of NCNET. For more information check out the project [website](https://www.di.ens.fr/willow/research/ncnet/) and the paper on [arXiv](https://arxiv.org/abs/1810.10510).\n",
    "\n",
    "For full sources, please check out the Official Github **[ignacio-rocco/ncnet](https://github.com/ignacio-rocco/ncnet)**\n",
    "\n",
    "### **Index**\n",
    "\n",
    "> **1. Overview of Ncnet**\n",
    "\n",
    "> **2. Configuration**\n",
    "\n",
    "> **3. Implementation**\n",
    "\n",
    ">> **Part 1. Model Implementation**\n",
    "\n",
    ">> **Part 2. Training**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **1. Overview of Ncnet**\n",
    "\n",
    "---\n",
    "\n",
    "A fully convolutional neural network is used to extract dense image descriptors $f_{A}$ and $f_{B}$ for images $I_{A}$ and $I_{B}$, respectively. All pairs of individual feature matches  $f^A_{ij}$ and $f^B_{kl}$ are represented in the 4-D space of matches $(i, j, k, l)$ (here shown as a 3-D perspective for illustration), and their matching scores stored in the 4-D correlation tensor $c$. These matches are further processed by the proposed soft-nearest neighbour filtering and neighbourhood consensus network to produce the final set of output correspondences.\n",
    "\n",
    "![](https://camo.githubusercontent.com/e454c9717acea8b28287cf5b82a7c2a275302ec8634a620742311cdd22db87da/68747470733a2f2f7777772e64692e656e732e66722f77696c6c6f772f72657365617263682f6e636e65742f696d616765732f7465617365722e706e67)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **2. Configuration**\n",
    "\n",
    "---\n",
    "\n",
    "This is the [README.md](https://github.com/ignacio-rocco/ncnet/blob/master/README.md) from the [official github](https://github.com/ignacio-rocco/ncnet).\n",
    "\n",
    "#### **Getting started**\n",
    "\n",
    "---\n",
    "\n",
    "##### **Dependencies**\n",
    "\n",
    "The code is implemented using Python 3 and PyTorch 0.3. All dependencies should be included in the standard Anaconda distribution.\n",
    "\n",
    "##### **Getting the datasets**\n",
    "\n",
    "The PF-Pascal dataset can be downloaded and unzipped by browsing to the `datasets/pf-pascal/` folder and running `download.sh`.\n",
    "\n",
    "The IVD dataset (used for training for the InLoc benchmark) can be downloaded by browsing to the `datasets/ivd/` folder and first running `make_dirs.sh` and then `download.sh`.\n",
    "\n",
    "The InLoc dataset (used for evaluation) an be downloaded by browsing to the `datasets/inloc/` folder and running `download.sh`. \n",
    "\n",
    "##### **Getting the trained models**\n",
    "\n",
    "The trained models trained on PF-Pascal (`ncnet_pfpascal.pth.tar`) and IVD (`ncnet_ivd.pth.tar`) can be dowloaded by browsing to the `trained_models/` folder and running `download.sh`.\n",
    "\n",
    "##### **Keypoint transfer demo**\n",
    "\n",
    "The demo Jupyter notebook file `point_transfer_demo.py` illustrates how to evaluate the model and use it for keypoint transfer on the PF-Pascal dataset. For this, previously download the PF-Pascal dataset and trained model as indicated above.\n",
    "\n",
    "#### **Training**\n",
    "\n",
    "---\n",
    "\n",
    "To train a model, run `train.py` with the desired model architecture and the path to the training dataset.\n",
    "\n",
    "Eg. For PF-Pascal:\n",
    "\n",
    "```bash\n",
    "python train.py --ncons_kernel_sizes 5 5 5 --ncons_channels 16 16 1 --dataset_image_path datasets/pf-pascal --dataset_csv_path datasets/pf-pascal/image_pairs/ \n",
    "```\n",
    "\n",
    "Eg. For InLoc: \n",
    "\n",
    "```bash\n",
    "python train.py --ncons_kernel_sizes 3 3 --ncons_channels 16 1 --dataset_image_path datasets/ivd --dataset_csv_path datasets/ivd/image_pairs/ \n",
    "```\n",
    "\n",
    "#### **Evaluation**\n",
    "\n",
    "---\n",
    "\n",
    "Evaluation for PF-Pascal is implemented in the `eval_pf_pascal.py` file. You can run the evaluation in the following way: \n",
    "\n",
    "```bash\n",
    "python eval_pf_pascal.py --checkpoint trained_models/[checkpoint name]\n",
    "```\n",
    "\n",
    "Evaluation for InLoc is implemented in the `eval_inloc.py` file. You can run the evaluation in the following way: \n",
    "\n",
    "```bash\n",
    "python eval_inloc.py --checkpoint trained_models/[checkpoint name]\n",
    "```\n",
    "\n",
    "This will generate a series of matches files in the `matches/` folder that then need to be fed to the InLoc evaluation Matlab code. \n",
    "In order to run the Matlab evaluation, you first need to clone the [InLoc demo repo](https://github.com/HajimeTaira/InLoc_demo), and download and compile all the required depedencies. Then you can modify the `compute_densePE_NCNet.m` file provided in this repo to indicate the path of the InLoc demo repo, and the name of the experiment (the particular folder name inside `matches/`), and run it to perform the evaluation.\n",
    "\n",
    "\n",
    "#### **BibTeX**\n",
    "\n",
    "---\n",
    "\n",
    "If you use this code in your project, please cite our paper:\n",
    "````\n",
    "@InProceedings{Rocco18b,\n",
    "        author       = \"Rocco, I. and Cimpoi, M. and Arandjelovi\\'c, R. and Torii, A. and Pajdla, T. and Sivic, J.\"\n",
    "        title        = \"Neighbourhood Consensus Networks\",\n",
    "        booktitle    = \"Proceedings of the 32nd Conference on Neural Information Processing Systems\",\n",
    "        year         = \"2018\",\n",
    "        }\n",
    "````\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **3. Implementation**\n",
    "#### **Part 1. Model Implementation**\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **Part 1-1. 4D Convolutional Neural Network**\n",
    "\n",
    "At this section, we are going to review about the Core of the Ncnet, the ***4D Convolutional Neural Network***, which enables the Ncnet injecting the ***Neighbourhood Consensus*** from the correlation volumes which is the aggregation of the 4 dimensional tensor from the pair of images \n",
    "\n",
    "You can find the directory of the original sources of ***conv4d.py*** here ***[ignacio-rocco/ncnet/lib/conv4d.py](https://github.com/ignacio-rocco/ncnet/blob/master/lib/conv4d.py)***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **Overall Implementation of conv4d.py**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import torch\n",
    "from torch.nn.parameter import Parameter\n",
    "import torch.nn.functional as F\n",
    "from torch.nn import Module\n",
    "from torch.nn.modules.conv import _ConvNd\n",
    "from torch.nn.modules.utils import _quadruple\n",
    "from torch.autograd import Variable\n",
    "from torch.nn import Conv2d\n",
    "\n",
    "\n",
    "def conv4d(data, filters, bias=None, permute_filters=True, use_half=False):\n",
    "    b, c, h, w, d, t = data.size()\n",
    "\n",
    "    # permute to avoid making contiguous inside loop\n",
    "    data = data.permute(2, 0, 1, 3, 4, 5).contiguous()\n",
    "\n",
    "    # Same permutation is done with filters, unless already provided with permutation\n",
    "    if permute_filters:\n",
    "        # permute to avoid making contiguous inside loop\n",
    "        filters = filters.permute(2, 0, 1, 3, 4, 5).contiguous()\n",
    "\n",
    "    c_out = filters.size(1)\n",
    "    if use_half:\n",
    "        output = Variable(\n",
    "            torch.HalfTensor(h, b, c_out, w, d, t), requires_grad=data.requires_grad\n",
    "        )\n",
    "    else:\n",
    "        output = Variable(\n",
    "            torch.zeros(h, b, c_out, w, d, t), requires_grad=data.requires_grad\n",
    "        )\n",
    "\n",
    "    padding = filters.size(0) // 2\n",
    "    if use_half:\n",
    "        Z = Variable(torch.zeros(padding, b, c, w, d, t).half())\n",
    "    else:\n",
    "        Z = Variable(torch.zeros(padding, b, c, w, d, t))\n",
    "\n",
    "    if data.is_cuda:\n",
    "        Z = Z.cuda(data.get_device())\n",
    "        output = output.cuda(data.get_device())\n",
    "\n",
    "    data_padded = torch.cat((Z, data, Z), 0)\n",
    "\n",
    "    for i in range(output.size(0)):  # loop on first feature dimension\n",
    "        # convolve with center channel of filter (at position=padding)\n",
    "        output[i, :, :, :, :, :] = F.conv3d(\n",
    "            data_padded[i + padding, :, :, :, :, :],\n",
    "            filters[padding, :, :, :, :, :],\n",
    "            bias=bias,\n",
    "            stride=1,\n",
    "            padding=padding,\n",
    "        )\n",
    "        # convolve with upper/lower channels of filter (at postions [:padding] [padding+1:])\n",
    "        for p in range(1, padding + 1):\n",
    "            output[i, :, :, :, :, :] = output[i, :, :, :, :, :] + F.conv3d(\n",
    "                data_padded[i + padding - p, :, :, :, :, :],\n",
    "                filters[padding - p, :, :, :, :, :],\n",
    "                bias=None,\n",
    "                stride=1,\n",
    "                padding=padding,\n",
    "            )\n",
    "            output[i, :, :, :, :, :] = output[i, :, :, :, :, :] + F.conv3d(\n",
    "                data_padded[i + padding + p, :, :, :, :, :],\n",
    "                filters[padding + p, :, :, :, :, :],\n",
    "                bias=None,\n",
    "                stride=1,\n",
    "                padding=padding,\n",
    "            )\n",
    "\n",
    "    output = output.permute(1, 2, 0, 3, 4, 5).contiguous()\n",
    "    return output\n",
    "\n",
    "\n",
    "class Conv4d(_ConvNd):\n",
    "    \"\"\"\n",
    "    Applies a 4D convolution over an input signal composed of several input planes.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        in_channels,\n",
    "        out_channels,\n",
    "        kernel_size,\n",
    "        bias=True,\n",
    "        pre_permuted_filters=True,\n",
    "    ):\n",
    "        # stride, dilation and groups !=1 functionality not tested\n",
    "        stride = 1\n",
    "        dilation = 1\n",
    "        groups = 1\n",
    "        # zero padding is added automatically in conv4d function to preserve tensor size\n",
    "        padding = 0\n",
    "        kernel_size = _quadruple(kernel_size)\n",
    "        stride = _quadruple(stride)\n",
    "        padding = _quadruple(padding)\n",
    "        dilation = _quadruple(dilation)\n",
    "        super(Conv4d, self).__init__(\n",
    "            in_channels,\n",
    "            out_channels,\n",
    "            kernel_size,\n",
    "            stride,\n",
    "            padding,\n",
    "            dilation,\n",
    "            False,\n",
    "            _quadruple(0),\n",
    "            groups,\n",
    "            bias,\n",
    "            padding_mode=\"zeros\",  # This fixes TypeError: __init__() missing 1 required positional argument: 'padding_mode' in Python 3.7\n",
    "        )\n",
    "        # weights will be sliced along one dimension during convolution loop\n",
    "        # make the looping dimension to be the first one in the tensor,\n",
    "        # so that we don't need to call contiguous() inside the loop\n",
    "        self.pre_permuted_filters = pre_permuted_filters\n",
    "        if self.pre_permuted_filters:\n",
    "            self.weight.data = self.weight.data.permute(2, 0, 1, 3, 4, 5).contiguous()\n",
    "        self.use_half = False\n",
    "\n",
    "    def forward(self, input):\n",
    "        # filters pre-permuted in constructor\n",
    "        return conv4d(\n",
    "            input,\n",
    "            self.weight,\n",
    "            bias=self.bias,\n",
    "            permute_filters=not self.pre_permuted_filters,\n",
    "            use_half=self.use_half,\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Part 1-2. Overall Architecture**\n",
    "\n",
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **Overall Implementation of model.py**\n",
    "\n",
    "\n",
    "You can find the directory of the original sources of ***model.py*** here ***[ignacio-rocco/ncnet/lib/model.py](https://github.com/ignacio-rocco/ncnet/blob/master/lib/model.py)***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<>:211: SyntaxWarning: \"is not\" with a literal. Did you mean \"!=\"?\n",
      "<>:242: SyntaxWarning: \"is not\" with a literal. Did you mean \"!=\"?\n",
      "<>:211: SyntaxWarning: \"is not\" with a literal. Did you mean \"!=\"?\n",
      "<>:242: SyntaxWarning: \"is not\" with a literal. Did you mean \"!=\"?\n",
      "C:\\Users\\JOHNST~1\\AppData\\Local\\Temp/ipykernel_5632/2382861885.py:211: SyntaxWarning: \"is not\" with a literal. Did you mean \"!=\"?\n",
      "  if checkpoint is not None and checkpoint is not '':\n",
      "C:\\Users\\JOHNST~1\\AppData\\Local\\Temp/ipykernel_5632/2382861885.py:242: SyntaxWarning: \"is not\" with a literal. Did you mean \"!=\"?\n",
      "  if checkpoint is not None and checkpoint is not '':\n"
     ]
    }
   ],
   "source": [
    "from __future__ import print_function, division\n",
    "from collections import OrderedDict\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "import torchvision.models as models\n",
    "import numpy as np\n",
    "import numpy.matlib\n",
    "import pickle\n",
    "\n",
    "#from lib.torch_util import Softmax1D\n",
    "#from lib.conv4d import Conv4d\n",
    "\n",
    "def featureL2Norm(feature):\n",
    "    epsilon = 1e-6\n",
    "    norm = torch.pow(torch.sum(torch.pow(feature,2),1)+epsilon,0.5).unsqueeze(1).expand_as(feature)\n",
    "    return torch.div(feature,norm)\n",
    "\n",
    "class FeatureExtraction(torch.nn.Module):\n",
    "    def __init__(self, train_fe=False, feature_extraction_cnn='resnet101', feature_extraction_model_file='', normalization=True, last_layer='', use_cuda=True):\n",
    "        super(FeatureExtraction, self).__init__()\n",
    "        self.normalization = normalization\n",
    "        self.feature_extraction_cnn=feature_extraction_cnn\n",
    "        if feature_extraction_cnn == 'vgg':\n",
    "            self.model = models.vgg16(pretrained=True)\n",
    "            # keep feature extraction network up to indicated layer\n",
    "            vgg_feature_layers=['conv1_1','relu1_1','conv1_2','relu1_2','pool1','conv2_1',\n",
    "                         'relu2_1','conv2_2','relu2_2','pool2','conv3_1','relu3_1',\n",
    "                         'conv3_2','relu3_2','conv3_3','relu3_3','pool3','conv4_1',\n",
    "                         'relu4_1','conv4_2','relu4_2','conv4_3','relu4_3','pool4',\n",
    "                         'conv5_1','relu5_1','conv5_2','relu5_2','conv5_3','relu5_3','pool5']\n",
    "            if last_layer=='':\n",
    "                last_layer = 'pool4'\n",
    "            last_layer_idx = vgg_feature_layers.index(last_layer)\n",
    "            self.model = nn.Sequential(*list(self.model.features.children())[:last_layer_idx+1])\n",
    "        # for resnet below\n",
    "        resnet_feature_layers = ['conv1','bn1','relu','maxpool','layer1','layer2','layer3','layer4']\n",
    "        if feature_extraction_cnn=='resnet101':\n",
    "            self.model = models.resnet101(pretrained=True)            \n",
    "            if last_layer=='':\n",
    "                last_layer = 'layer3'                            \n",
    "            resnet_module_list = [getattr(self.model,l) for l in resnet_feature_layers]\n",
    "            last_layer_idx = resnet_feature_layers.index(last_layer)\n",
    "            self.model = nn.Sequential(*resnet_module_list[:last_layer_idx+1])\n",
    "\n",
    "        if feature_extraction_cnn=='resnet101fpn':\n",
    "            if feature_extraction_model_file!='':\n",
    "                resnet = models.resnet101(pretrained=True) \n",
    "                # swap stride (2,2) and (1,1) in first layers (PyTorch ResNet is slightly different to caffe2 ResNet)\n",
    "                # this is required for compatibility with caffe2 models\n",
    "                resnet.layer2[0].conv1.stride=(2,2)\n",
    "                resnet.layer2[0].conv2.stride=(1,1)\n",
    "                resnet.layer3[0].conv1.stride=(2,2)\n",
    "                resnet.layer3[0].conv2.stride=(1,1)\n",
    "                resnet.layer4[0].conv1.stride=(2,2)\n",
    "                resnet.layer4[0].conv2.stride=(1,1)\n",
    "            else:\n",
    "                resnet = models.resnet101(pretrained=True) \n",
    "            resnet_module_list = [getattr(resnet,l) for l in resnet_feature_layers]\n",
    "            conv_body = nn.Sequential(*resnet_module_list)\n",
    "            self.model = fpn_body(conv_body,\n",
    "                                  resnet_feature_layers,\n",
    "                                  fpn_layers=['layer1','layer2','layer3'],\n",
    "                                  normalize=normalization,\n",
    "                                  hypercols=True)\n",
    "            if feature_extraction_model_file!='':\n",
    "                self.model.load_pretrained_weights(feature_extraction_model_file)\n",
    "\n",
    "        if feature_extraction_cnn == 'densenet201':\n",
    "            self.model = models.densenet201(pretrained=True)\n",
    "            # keep feature extraction network up to denseblock3\n",
    "            # self.model = nn.Sequential(*list(self.model.features.children())[:-3])\n",
    "            # keep feature extraction network up to transitionlayer2\n",
    "            self.model = nn.Sequential(*list(self.model.features.children())[:-4])\n",
    "        if train_fe==False:\n",
    "            # freeze parameters\n",
    "            for param in self.model.parameters():\n",
    "                param.requires_grad = False\n",
    "        # move to GPU\n",
    "        if use_cuda:\n",
    "            self.model = self.model.cuda()\n",
    "        \n",
    "    def forward(self, image_batch):\n",
    "        features = self.model(image_batch)\n",
    "        if self.normalization and not self.feature_extraction_cnn=='resnet101fpn':\n",
    "            features = featureL2Norm(features)\n",
    "        return features\n",
    "    \n",
    "class FeatureCorrelation(torch.nn.Module):\n",
    "    def __init__(self,shape='3D',normalization=True):\n",
    "        super(FeatureCorrelation, self).__init__()\n",
    "        self.normalization = normalization\n",
    "        self.shape=shape\n",
    "        self.ReLU = nn.ReLU()\n",
    "    \n",
    "    def forward(self, feature_A, feature_B):        \n",
    "        if self.shape=='3D':\n",
    "            b,c,h,w = feature_A.size()\n",
    "            # reshape features for matrix multiplication\n",
    "            feature_A = feature_A.transpose(2,3).contiguous().view(b,c,h*w)\n",
    "            feature_B = feature_B.view(b,c,h*w).transpose(1,2)\n",
    "            # perform matrix mult.\n",
    "            feature_mul = torch.bmm(feature_B,feature_A)\n",
    "            # indexed [batch,idx_A=row_A+h*col_A,row_B,col_B]\n",
    "            correlation_tensor = feature_mul.view(b,h,w,h*w).transpose(2,3).transpose(1,2)\n",
    "        elif self.shape=='4D':\n",
    "            b,c,hA,wA = feature_A.size()\n",
    "            b,c,hB,wB = feature_B.size()\n",
    "            # reshape features for matrix multiplication\n",
    "            feature_A = feature_A.view(b,c,hA*wA).transpose(1,2) # size [b,c,h*w]\n",
    "            feature_B = feature_B.view(b,c,hB*wB) # size [b,c,h*w]\n",
    "            # perform matrix mult.\n",
    "            feature_mul = torch.bmm(feature_A,feature_B)\n",
    "            # indexed [batch,row_A,col_A,row_B,col_B]\n",
    "            correlation_tensor = feature_mul.view(b,hA,wA,hB,wB).unsqueeze(1)\n",
    "        \n",
    "        if self.normalization:\n",
    "            correlation_tensor = featureL2Norm(self.ReLU(correlation_tensor))\n",
    "            \n",
    "        return correlation_tensor\n",
    "\n",
    "class NeighConsensus(torch.nn.Module):\n",
    "    def __init__(self, use_cuda=True, kernel_sizes=[3,3,3], channels=[10,10,1], symmetric_mode=True):\n",
    "        super(NeighConsensus, self).__init__()\n",
    "        self.symmetric_mode = symmetric_mode\n",
    "        self.kernel_sizes = kernel_sizes\n",
    "        self.channels = channels\n",
    "        num_layers = len(kernel_sizes)\n",
    "        nn_modules = list()\n",
    "        for i in range(num_layers):\n",
    "            if i==0:\n",
    "                ch_in = 1\n",
    "            else:\n",
    "                ch_in = channels[i-1]\n",
    "            ch_out = channels[i]\n",
    "            k_size = kernel_sizes[i]\n",
    "            nn_modules.append(Conv4d(in_channels=ch_in,out_channels=ch_out,kernel_size=k_size,bias=True))\n",
    "            nn_modules.append(nn.ReLU(inplace=True))\n",
    "        self.conv = nn.Sequential(*nn_modules)        \n",
    "        if use_cuda:\n",
    "            self.conv.cuda()\n",
    "\n",
    "    def forward(self, x):\n",
    "        if self.symmetric_mode:\n",
    "            # apply network on the input and its \"transpose\" (swapping A-B to B-A ordering of the correlation tensor),\n",
    "            # this second result is \"transposed back\" to the A-B ordering to match the first result and be able to add together\n",
    "            x = self.conv(x)+self.conv(x.permute(0,1,4,5,2,3)).permute(0,1,4,5,2,3)\n",
    "            # because of the ReLU layers in between linear layers, \n",
    "            # this operation is different than convolving a single time with the filters+filters^T\n",
    "            # and therefore it makes sense to do this.\n",
    "        else:\n",
    "            x = self.conv(x)\n",
    "        return x\n",
    "\n",
    "def MutualMatching(corr4d):\n",
    "    # mutual matching\n",
    "    batch_size,ch,fs1,fs2,fs3,fs4 = corr4d.size()\n",
    "\n",
    "    corr4d_B=corr4d.view(batch_size,fs1*fs2,fs3,fs4) # [batch_idx,k_A,i_B,j_B]\n",
    "    corr4d_A=corr4d.view(batch_size,fs1,fs2,fs3*fs4)\n",
    "\n",
    "    # get max\n",
    "    corr4d_B_max,_=torch.max(corr4d_B,dim=1,keepdim=True)\n",
    "    corr4d_A_max,_=torch.max(corr4d_A,dim=3,keepdim=True)\n",
    "\n",
    "    eps = 1e-5\n",
    "    corr4d_B=corr4d_B/(corr4d_B_max+eps)\n",
    "    corr4d_A=corr4d_A/(corr4d_A_max+eps)\n",
    "\n",
    "    corr4d_B=corr4d_B.view(batch_size,1,fs1,fs2,fs3,fs4)\n",
    "    corr4d_A=corr4d_A.view(batch_size,1,fs1,fs2,fs3,fs4)\n",
    "\n",
    "    corr4d=corr4d*(corr4d_A*corr4d_B) # parenthesis are important for symmetric output \n",
    "        \n",
    "    return corr4d\n",
    "\n",
    "def maxpool4d(corr4d_hres,k_size=4):\n",
    "    slices=[]\n",
    "    for i in range(k_size):\n",
    "        for j in range(k_size):\n",
    "            for k in range(k_size):\n",
    "                for l in range(k_size):\n",
    "                    slices.append(corr4d_hres[:,0,i::k_size,j::k_size,k::k_size,l::k_size].unsqueeze(0))\n",
    "    slices=torch.cat(tuple(slices),dim=1)\n",
    "    corr4d,max_idx=torch.max(slices,dim=1,keepdim=True)\n",
    "    max_l=torch.fmod(max_idx,k_size)\n",
    "    max_k=torch.fmod(max_idx.sub(max_l).div(k_size),k_size)\n",
    "    max_j=torch.fmod(max_idx.sub(max_l).div(k_size).sub(max_k).div(k_size),k_size)\n",
    "    max_i=max_idx.sub(max_l).div(k_size).sub(max_k).div(k_size).sub(max_j).div(k_size)\n",
    "    # i,j,k,l represent the *relative* coords of the max point in the box of size k_size*k_size*k_size*k_size\n",
    "    return (corr4d,max_i,max_j,max_k,max_l)\n",
    "\n",
    "class ImMatchNet(nn.Module):\n",
    "    def __init__(self, \n",
    "                 feature_extraction_cnn='resnet101', \n",
    "                 feature_extraction_last_layer='',\n",
    "                 feature_extraction_model_file=None,\n",
    "                 return_correlation=False,  \n",
    "                 ncons_kernel_sizes=[3,3,3],\n",
    "                 ncons_channels=[10,10,1],\n",
    "                 normalize_features=True,\n",
    "                 train_fe=False,\n",
    "                 use_cuda=True,\n",
    "                 relocalization_k_size=0,\n",
    "                 half_precision=False,\n",
    "                 checkpoint=None,\n",
    "                 ):\n",
    "        \n",
    "        super(ImMatchNet, self).__init__()\n",
    "        # Load checkpoint\n",
    "        if checkpoint is not None and checkpoint is not '':\n",
    "            print('Loading checkpoint...')\n",
    "            checkpoint = torch.load(checkpoint, map_location=lambda storage, loc: storage)\n",
    "            checkpoint['state_dict'] = OrderedDict([(k.replace('vgg', 'model'), v) for k, v in checkpoint['state_dict'].items()])\n",
    "            # override relevant parameters\n",
    "            print('Using checkpoint parameters: ')\n",
    "            ncons_channels=checkpoint['args'].ncons_channels\n",
    "            print('  ncons_channels: '+str(ncons_channels))\n",
    "            ncons_kernel_sizes=checkpoint['args'].ncons_kernel_sizes\n",
    "            print('  ncons_kernel_sizes: '+str(ncons_kernel_sizes))            \n",
    "\n",
    "        self.use_cuda = use_cuda\n",
    "        self.normalize_features = normalize_features\n",
    "        self.return_correlation = return_correlation\n",
    "        self.relocalization_k_size = relocalization_k_size\n",
    "        self.half_precision = half_precision\n",
    "        \n",
    "        self.FeatureExtraction = FeatureExtraction(train_fe=train_fe,\n",
    "                                                   feature_extraction_cnn=feature_extraction_cnn,\n",
    "                                                   feature_extraction_model_file=feature_extraction_model_file,\n",
    "                                                   last_layer=feature_extraction_last_layer,\n",
    "                                                   normalization=normalize_features,\n",
    "                                                   use_cuda=self.use_cuda)\n",
    "        \n",
    "        self.FeatureCorrelation = FeatureCorrelation(shape='4D',normalization=False)\n",
    "\n",
    "        self.NeighConsensus = NeighConsensus(use_cuda=self.use_cuda,\n",
    "                                             kernel_sizes=ncons_kernel_sizes,\n",
    "                                             channels=ncons_channels)\n",
    "\n",
    "        # Load weights\n",
    "        if checkpoint is not None and checkpoint is not '':\n",
    "            print('Copying weights...')\n",
    "            for name, param in self.FeatureExtraction.state_dict().items():\n",
    "                if 'num_batches_tracked' not in name:\n",
    "                    self.FeatureExtraction.state_dict()[name].copy_(checkpoint['state_dict']['FeatureExtraction.' + name])    \n",
    "            for name, param in self.NeighConsensus.state_dict().items():\n",
    "                self.NeighConsensus.state_dict()[name].copy_(checkpoint['state_dict']['NeighConsensus.' + name])\n",
    "            print('Done!')\n",
    "        \n",
    "        self.FeatureExtraction.eval()\n",
    "\n",
    "        if self.half_precision:\n",
    "            for p in self.NeighConsensus.parameters():\n",
    "                p.data=p.data.half()\n",
    "            for l in self.NeighConsensus.conv:\n",
    "                if isinstance(l,Conv4d):\n",
    "                    l.use_half=True\n",
    "                    \n",
    "    # used only for foward pass at eval and for training with strong supervision\n",
    "    def forward(self, tnf_batch): \n",
    "        # feature extraction\n",
    "        feature_A = self.FeatureExtraction(tnf_batch['source_image'])\n",
    "        feature_B = self.FeatureExtraction(tnf_batch['target_image'])\n",
    "        if self.half_precision:\n",
    "            feature_A=feature_A.half()\n",
    "            feature_B=feature_B.half()\n",
    "        # feature correlation\n",
    "        corr4d = self.FeatureCorrelation(feature_A,feature_B)\n",
    "        # do 4d maxpooling for relocalization\n",
    "        if self.relocalization_k_size>1:\n",
    "            corr4d,max_i,max_j,max_k,max_l=maxpool4d(corr4d,k_size=self.relocalization_k_size)\n",
    "        # run match processing model \n",
    "        corr4d = MutualMatching(corr4d)\n",
    "        corr4d = self.NeighConsensus(corr4d)            \n",
    "        corr4d = MutualMatching(corr4d)\n",
    "        \n",
    "        if self.relocalization_k_size>1:\n",
    "            delta4d=(max_i,max_j,max_k,max_l)\n",
    "            return (corr4d,delta4d)\n",
    "        else:\n",
    "            return corr4d"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Part 3. Training**"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "3efb3a3e09a38f93b1d9bfa4b9b8ce66a167f6ada77b64bc003d7d6d298d81d5"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 ('buddhalight')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
