{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **End-to-end weakly-supervised semantic alignment**\n",
    "\n",
    "**Authors: Ignacio Rocco1,2, Relja Arandjelovic´3, Josef Sivic1,2,4**\n",
    "\n",
    "**1 DI ENS / 2 Inria / 3 DeepMind / 4 CIIRC, CTU in Prague**\n",
    "\n",
    "**[Official Github Code](https://github.com/ignacio-rocco/weakalign)** / **[Project Page](https://www.di.ens.fr/willow/research/weakalign/)** / **[Pdf](https://arxiv.org/abs/1712.06861)**\n",
    "\n",
    "---\n",
    "\n",
    "**Edited By Su Hyung Choi (Key Summary & Code Practice)**\n",
    "\n",
    "If you have any issues on this scripts, please PR to the repository below.\n",
    "\n",
    "**[Github: @JonyChoi - Computer Vision Paper Reviews](https://github.com/jonychoi/Computer-Vision-Paper-Reviews)**\n",
    "\n",
    "Edited March 21 2022\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Abstract**\n",
    "\n",
    "***\"We tackle the task of semantic alignment where the goal\n",
    "is to compute dense semantic correspondence aligning two\n",
    "images depicting objects of the same category.\"***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Contribution**\n",
    "\n",
    "---\n",
    "\n",
    "***First***, *we develop a convolutional neural network architecture for semantic alignment\n",
    "that is trainable in an end-to-end manner from weak imagelevel supervision in the form of matching image pairs.*\n",
    "\n",
    "> ***Semi-Local Constraints*를 이용하여 애매모한 Feature를 매칭시키는 클래식한 아이디어에서 영감을 얻은바, Global Gemoetric Model의 필요 없이, 4D Space에서 가능한 모든 neighbourhood consensus 패턴들을 분석해서 spatially matching하는, End-to-End로 학습가능한 CNN Architecture를 개발.**\n",
    "\n",
    "---\n",
    "\n",
    "***Second***, *the main component of this architecture is a differentiable soft inlier scoring module, inspired by the RANSAC inlier scoring procedure, that computes the quality of the alignment based on\n",
    "only geometrically consistent correspondences thereby reducing the effect of background clutter.*\n",
    "\n",
    "> **Manual annotation, 즉 완전 하드 annotated 방식이 아닌 matching과 non-matchinig image pairs 형태를 갖춘 weak supervision을 통해 효율적 학습**\n",
    "\n",
    "---\n",
    "\n",
    "***Third***, *we demonstrate that the proposed approach achieves state-of-the-art\n",
    "performance on multiple standard benchmarks for semantic\n",
    "alignment.*\n",
    "\n",
    "> **여기서는 정성적 평가. Neighbourhood Consensus Network는 Category, Instance-level Matching을 포함한 넓은 matching task에서 PF PASCAL 데이터셋과 InLoc Indoor Visual Localization 벤치마크에서 소타달성.**\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Introduction**\n",
    "\n",
    "![](https://www.di.ens.fr/willow/research/weakalign/images/teaser.jpg)\n",
    "\n",
    "> **기존의 Visual Correspondence는 *Viewpoint*의 변화나, *illumination* 등 다양한 variation에 꽤 괜찮은 결과를 낳았으나, 여전히 hand crafted 모델에 비해서 trainable한 모델들이 근소하게 좋은 성능을 보이고 있음. (OD나 Classification과 다르게)**\n",
    "\n",
    "*\"One of the reasons for this plateauing performance could be the currently dominant approach for\n",
    "finding image correspondence based on matching individual image features. While we have now better\n",
    "local patch descriptors, the matching is still performed by variants of the nearest neighbour assignment\n",
    "in a feature space followed by separate disambiguation stages based on geometric constraints.\"*\n",
    "\n",
    "> **애매모하고 variate한 feature들 분리하고, invariate한 부분들 위주로 가장 가까운 neighbour을 갖다가 assign해버리는 individual image feature 기반의 image correspondence 가 dominant한 approach.**\n",
    "\n",
    "*\"This\n",
    "approach has, however, fundamental limitations. Imagine a scene with textureless regions or repetitive\n",
    "patterns, such as a corridor with almost textureless walls and only few distinguishing features. A\n",
    "small patch of an image, depicting a repetitive pattern or a textureless area, is indistinguishable from other portions of the image depicting the same repetitive or textureless pattern. Such matches will be\n",
    "either discarded [23] or incorrect. As a result, matching individual patch descriptors will often fail in\n",
    "such challenging situations.\"*\n",
    "\n",
    "> **그러나 이건 근본적인 문제점. Textureless한 region 또는 반복되는 패턴의 회랑 같은 것들은 조금의 분류가능한 feature들을 가지고 있음. 때문에 이러한 Individual한 patch descriptors는 이러한 challenging한 상황에서 실패**\n",
    "\n",
    "\n",
    "*\"In this work we take a different direction and develop a trainable neural network architecture that\n",
    "disambiguates such challenging situations by analyzing local neighbourhood patterns in a full set of\n",
    "dense correspondences. **The intuition is the following: in order to disambiguate a match on a repetitive\n",
    "pattern, it is necessary to analyze a larger context of the scene that contains a unique non-repetitive\n",
    "feature.** The information from this unique match can then be propagated to the neighbouring uncertain\n",
    "matches. **In other words, the certain unique matches will support the close-by uncertain ambiguous\n",
    "matches in the image.\"***\n",
    "\n",
    "> **그래서 큰 Context에서 Unique한 것들이 가까운 애매모한 것들을 Matching하는 걸 support함으로써 uncertain한 것들을 처리하자!! + trainable CNN은 당연하고**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Related Works**\n",
    "\n",
    "#### **Matching with hand-crafted image descriptors**\n",
    "\n",
    "- **What is it**: *\"Traditionally, correspondences between images\n",
    "have been obtained by hand crafted local invariant feature detectors and descriptors [23, 25, 42]\n",
    "that were extracted from the image with a controlled degree of invariance to local geometric and\n",
    "photometric transformations. Candidate (tentative) correspondences were then obtained by variants\n",
    "of nearest neighbour matching. Strategies for removing ambiguous and non-distinctive matches\n",
    "include the widely used second nearest neighbour ratio test [23], or enforcing matches to be mutual\n",
    "nearest neighbours.\"*\n",
    "\n",
    "- **Limitation**: \"*Both approaches work well for many applications, but have the disadvantage\n",
    "of discarding many correct matches, which can be problematic for challenging scenes, such as\n",
    "indoor spaces considered in this work that include repetitive and textureless areas. While successful,\n",
    "handcrafted descriptors have only limited tolerance to large appearance changes beyond the built-in\n",
    "invariance.*\"\n",
    "\n",
    "> **전통적으로, Correspondence는 local invariant feature (상대적으로 안변하는 local feautre들) 중심으로 Candidate correspondences (애매모한 애들)을 nearest neighbour참고하여 처리. 즉 nearest neighbour을 information삼는 방식은 예전에도 hand crafted된 방식으로 존재.**\n",
    "\n",
    "\n",
    "> **하지만 Indoor나 반복적이고 textureless한 scene들은 잘 처리하지 못함. 즉, INVARIANCE가 뚜렷하게 탑재된 큰 변화에 있어서 한계적으로 작동**\n",
    "\n",
    "---\n",
    "\n",
    "#### **Matching with trainable descriptors.**\n",
    "\n",
    "- **What is it**: *\"The majority of trainable image descriptors are based on\n",
    "convolutional neural networks (CNNs) and typically operate on patches extracted using a feature\n",
    "detector such as DoG [23], yielding a sparse set of descriptors [3, 4, 10, 17, 36, 37] or use a pre-trained\n",
    "image-level CNN feature extractor [26, 32]. Others have recently developed trainable methods that\n",
    "comprise both feature detection and description [7, 26, 43]. The extracted descriptors are typically\n",
    "compared using the Euclidean distance,\"*\n",
    "\n",
    "- **Limitation**: *\"but an appropriate similarity score can be also learnt in\n",
    "a discriminative manner [13, 44], where a trainable model is used to both extract descriptors and produce a similarity score. Finding matches consistent with a geometric model is typically performed\n",
    "in a separate post-processing stage [3, 4, 7, 10, 17, 22, 26, 36, 37, 43].\"*\n",
    "\n",
    "> **trainable한 친구들은 CNN기반. patch 단위로 feature extracting 하거나 (DoG) / sprase set descriptors 적용하거나 / pre-trained된 image-level CNN feautre extractor 적용. feature detection과 description에 있어서 둘다 trainable한 methods 적용한 애들도 있음. 걔네들 descriptors는 유클라디언 distance를 주로 사용.**\n",
    "\n",
    "> **그러나 trainable model 같은 경우 extract descriptors하고 유사도 도출하는데 둘다쓰임. matching은 별도의 다른 stage로 구분되어 사용해야 했음**\n",
    "\n",
    "---\n",
    "\n",
    "#### **Trainable image alignment.**\n",
    "\n",
    "- **What is it**: *\"Recently, end-to-end trainable methods have been developed to\n",
    "produce correspondences between images according to a parametric geometric model, such as an\n",
    "affine, perspective or thin-plate spline transformation [28, 29]. In these works, all pairwise feature\n",
    "matches are computed and used to estimate the geometric transformation parameters using a CNN.*\n",
    "***Unlike previous methods that capture only a sparse set of correspondences, this geometric estimation\n",
    "CNN captures interactions between a full set of dense correspondences.\"***\n",
    "\n",
    "- **Limitation**:  \"*However, these methods\n",
    "currently only estimate a low complexity parametric transformation, and therefore their application\n",
    "is limited to only coarse image alignment tasks.*\"\n",
    "\n",
    "- **Solution**: *\"In contrast, we target a more general problem of\n",
    "identifying reliable correspondences between images of a general 3D scene. Our approach is not\n",
    "limited to a low dimensional parametric model, but outputs a generic set of locally consistent image\n",
    "correspondences, applicable to a wide range of computer vision problems ranging from category-level\n",
    "image alignment to camera pose estimation. The proposed method builds on the classical ideas of\n",
    "neighbourhood consensus, which we review next.\"*\n",
    "\n",
    "> **최근에 나온 End to End trainable한 Correspondence한 Methods들은 parametic geometric model로 이전 모델들이 sparse한 correspondences를 검출하는 반면에 전체의 dense correspondences를 검출. 그러나 아직은 low complexity parametic transformation에 그쳐서 coarse한 이미지 alignment task에 머물고 있음.**\n",
    "\n",
    "> **반면에 해당 논문의 method는 low dimensional parametric model에 그치지 않고, 3D scene의 일반적인 이미지의 안정적인 correspondeces를 통해 넓은 비젼분야에 걸쳐서 적용가능한 (category-level에서 부터 camera pose estimation까지) method임. 이 기본 아이디어는 향후 설명할 Neighbourhood consensus임.**\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "#### **Match filtering by neighbourhood consensus**\n",
    "\n",
    "- **What is it**: *\"Several strategies have been introduced to decide\n",
    "whether a match is correct or not, given the supporting evidence from the neighbouring matches. The\n",
    "early examples analyzed the patterns of* ***distances [46] or angles [34] between neighbouring matches.***\n",
    "*Later work simply counts the number of consistent matches in a certain image neighbourhood [33, 38],\n",
    "which can be built in a scale invariant manner [30] or using a regular image grid [5]. While\n",
    "simple, these techniques have been remarkably effective in removing random incorrect matches and\n",
    "disambiguating local repetitive patterns [30].\"*\n",
    "\n",
    "- **Solution**: *\"Inspired by this simple yet powerful idea we develop a\n",
    "neighbourhood consensus network – a convolutional neural architecture that (i) analyzes the full set of\n",
    "dense matches between a pair of images and (ii) learns patterns of locally consistent correspondences\n",
    "directly from data.\"*\n",
    "\n",
    "> **Neigbouring을 heurestic으로 matching하는 여러 방법들이 존재하였음. 몇몇 방법들은 neigbouring 매치들간에 거리나 각도의 패턴을 분석함으로써 구현하였고, 나중에 몇몇 작업들은 확실한 neigbourhood 이미지에서 consistent matches들을 세는 방법으로 구현. 이는 잘못된 matches와 애매모한 반복적인 local 패턴들을 제거하는데 효과적.**\n",
    "\n",
    "> **여기에 영감을 받아서 CNN기반의 neighbourhood consensus network 개발. \n",
    " (i) pair 이미지 전체 set에 대한 dense matches (ii) 데이터로부터 directly하게 locally consistent correpondeces 패턴까지 학습**\n",
    "\n",
    "---\n",
    "\n",
    "#### **Flow and disparity estimation**\n",
    "\n",
    "- **What is it**: *\"Related are also methods that estimate optical flow or stereo\n",
    "disparity such as [6, 15, 16, 24, 39], or their trainable counterparts [8, 19, 40]. These works also aim\n",
    "at establishing reliable point to point correspondences between images.\"*\n",
    "\n",
    "- **Limitation**: *\"This(our method) is different from optical flow where\n",
    "image pairs are usually consecutive video frames with small viewpoint or appearance changes, and\n",
    "stereo where matching is often reduced to a local search around epipolar lines. The optical flow\n",
    "and stereo problems are well addressed by specialized methods that explicitly exploit the problem\n",
    "constraints (such as epipolar line constraint, small motion, smoothness, etc.).\"*\n",
    "\n",
    "- **Solution**: *\"However, we address a more\n",
    "general matching problem where images can have large viewpoint changes (indoor localization) or\n",
    "major changes in appearance (category-level matching).*\n",
    "\n",
    "> **optical flow나 stereo disparity(치우침) 측정을 위한 방법들과 trainable한 counterparts들에 대한 methods들도 존재. 그러나 optical flow는 small viewpoint 나 appearance changes에 대해서 consecutive한 video frames들을 갖음. 또한 local search를 위해서 epipolar liines들 주변에서 매칭이 줄어드는 경향. 이러한 optical flow는 epipolar line constraint나 small motion, smoothness들을 가지고 처리하는데 특화되있음.**\n",
    "\n",
    "> **그러나 해당 논문에서는 좀 더 general한 matching을 위해서 더 large한 viewpoint changes (indoor localization) 과 major changes in appearance (category-level matching)을 해결하고자 함."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Proposed approach**\n",
    "\n",
    "***\"In this work, we combine the robustness of neighbourhood consensus filtering with the power of\n",
    "trainable neural architectures.\"***\n",
    "\n",
    ">\n",
    "\n",
    "- *We design a model which learns to discriminate a reliable match by\n",
    "recognizing patterns of supporting matches in its neighbourhood.*\n",
    "\n",
    ">\n",
    "\n",
    "- *Furthermore, we do this in a fully\n",
    "differentiable way, such that this trainable matching module can be directly combined with strong\n",
    "CNN image descriptors. The resulting pipeline can then be trained in an end-to-end manner for the\n",
    "task of feature matching. An overview of our proposed approach is presented in Fig. 1.*\n",
    "\n",
    ">\n",
    "\n",
    "- *There are five main components: (i) dense feature extraction and matching, (ii) the neighbourhood consensus\n",
    "network, (iii) a soft mutual nearest neighbour filtering, (iv) extraction of correspondences from the\n",
    "output 4D filtered match tensor, and (v) weakly supervised training loss. These components are\n",
    "described next.*\n",
    "\n",
    ">\n",
    "\n",
    "#### **Dense feature extraction and matching**\n",
    "\n",
    "![](https://www.di.ens.fr/willow/research/ncnet/images/teaser.png)\n",
    "\n",
    "---\n",
    "\n",
    "#### **Neighbourhood consensus network**\n",
    "---\n",
    "\n",
    "#### **Soft mutual nearest neighbour filtering**\n",
    "---\n",
    "\n",
    "#### **Extracting correspondences from the correlation map**\n",
    "---\n",
    "\n",
    "#### **Weakly-supervised training**\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Experimental results**\n",
    "\n",
    "#### **Implementation details**\n",
    "\n",
    "---\n",
    "\n",
    "#### **Category-level matching**\n",
    "\n",
    "---\n",
    "\n",
    "#### **Instance-level matching**\n",
    "\n",
    "---\n",
    "\n",
    "#### **Limitations**\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Conclusion**"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "3efb3a3e09a38f93b1d9bfa4b9b8ce66a167f6ada77b64bc003d7d6d298d81d5"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 ('buddhalight')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
