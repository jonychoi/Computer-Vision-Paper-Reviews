{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **About INR and Style Transfer**\n",
    "\n",
    "### **The ulitmate purpose of style transfer via INR, with introducing its background and papers**\n",
    "\n",
    "**Edited By Su Hyung Choi (Key Summary & Code Practice)**\n",
    "\n",
    "If you have any issues on this scripts, please PR to the repository below.\n",
    "\n",
    "**[Github: @JonyChoi - Computer Vision Paper Reviews]** https://github.com/jonychoi/Computer-Vision-Paper-Reviews\n",
    "\n",
    "Edited Jan 16 2022\n",
    "\n",
    "---\n",
    "\n",
    "Here, I wrote the writing contains about the *\"what are Implicit Neural Representations'*, the basic concept of INR, and its potential usage and applications. With the understanding of INR, I introduced the its representative papers within few couples of years to apprehend its trend and evolution history."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Overview**\n",
    "\n",
    "**1. The Goal of the Writing**\n",
    "\n",
    "**2. What are Implicit Neural Representations**\n",
    "\n",
    "**3. The applications and potentials of INR**\n",
    "\n",
    "**4. About Style Transfer**\n",
    "\n",
    "**5. What Style Transfer ultimately purpose**\n",
    "\n",
    "**6. Paper review for basic understanding for Style Transfer via INR**\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **1. The Goal of the Writing**\n",
    "\n",
    "Here, as a beginner of *Style Transfer* within *INR(Implicit Neural Representation)* field, I'm going to introduce what are implict neural representations and style transfer with its *basic concept* and *the usages with the potential applicablity*, by *reviewing the core papers* that enhance understanding of Style Transfer within INR field.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **2. What are Implicit Neural Representations?**\n",
    "\n",
    "---\n",
    "\n",
    "#### **Implicit neural representations**\n",
    "\n",
    "<img src=\"./imgs/example7.png\" width=\"500\" />\n",
    "\n",
    "Picture by [Implicit Neural Scene Representations](https://www.youtube.com/watch?v=Or9J-DCDGko)\n",
    "\n",
    "***\"Implicit neural representations is about parameterizing a continuous differentiable signal with a neural network. The signal is encoded within the neural network, providing a possibly more compact representation.*** *This is a type of [regression](https://hugocisneros.com/notes/machine_learning/) problem. Applications of these learned representations range from simple [compression](https://hugocisneros.com/notes/compression/), to 3D scene reconstruction from 2D images, semantic information inference, etc.[CPPN](https://hugocisneros.com/notes/cppn/) is an early example of a implicit neural representation implementation mainly used for pattern generation. It uses a neural network to generate patterns parameterized by two (or more) coordinates.\"*\n",
    "\n",
    "***by [Hugo Cisneros](https://hugocisneros.com/notes/implicit_neural_representations/#org36a6837)***\n",
    "\n",
    "***\"Implicit Neural Representations (sometimes also referred to as coordinate-based representations) are a novel way to parameterize signals of all kinds.*** Conventional signal representations are usually discrete - *for instance, images are discrete grids of pixels, audio signals are discrete samples of amplitudes, and 3D shapes are usually parameterized as grids of voxels, point clouds, or meshes. In contrast, Implicit Neural Representations parameterize a signal as a ***continuous function*** that maps the domain of the signal (i.e., a coordinate, such as a pixel coordinate for an image) to whatever is at that coordinate (for an image, an R,G,B color). Of course, these functions are usually not analytically tractable - it is impossible to \"write down\" the function that parameterizes a natural image as a mathematical formula. Implicit Neural Representations thus approximate that function via a neural network.\"*\n",
    "\n",
    "***by [Vincent Sitzman](https://github.com/vsitzmann/awesome-implicit-representations#what-are-implicit-neural-representations)***\n",
    "\n",
    "---\n",
    "\n",
    "#### **Why are they interesting?**\n",
    "\n",
    "Implicit Neural Representations have several benefits: First, they are not coupled to spatial resolution anymore, the way, for instance, an image is coupled to the number of pixels. This is because they are continuous functions! Thus, the memory required to parameterize the signal is independent of spatial resolution, and only scales with the complexity of the underyling signal. Another corollary of this is that implicit representations have \"infinite resolution\" - they can be sampled at arbitrary spatial resolutions.\n",
    "\n",
    "This is immediately useful for a number of applications, such as super-resolution, or in parameterizing signals in 3D and higher dimensions, where memory requirements grow intractably fast with spatial resolution. Further, generalizing across neural implicit representations amounts to learning a prior over a space of functions, implemented via learning a prior over the weights of neural networks - this is commonly referred to as meta-learning and is an extremely exciting intersection of two very active research areas! Another exciting overlap is between neural implicit representations and the study of symmetries in neural network architectures - for intance, creating a neural network architecture that is 3D rotation-equivariant immediately yields a viable path to rotation-equivariant generative models via neural implicit representations.\n",
    "\n",
    "Another key promise of implicit neural representations lie in algorithms that directly operate in the space of these representations. In other words: What's the \"convolutional neural network\" equivalent of a neural network operating on images represented by implicit representations?\n",
    "\n",
    "*Reference from [Vincent Sitzman](https://github.com/vsitzmann/awesome-implicit-representations#what-are-implicit-neural-representations)*\n",
    "\n",
    "---\n",
    "\n",
    "#### **Similar Fields related to INR**\n",
    "\n",
    "***Implicit neural representations for high frequency data***\n",
    "\n",
    "*To encode potentially high frequency data such as sound or images, it is much more efficient to start from periodic feature transformations. This can be achieved with periodic activation functions [(Sitzmann et al. 2020)](https://arxiv.org/abs/2006.09661) or by using a Fourier feature mapping [(Tancik et al. 2020)](https://arxiv.org/abs/2006.10739).*\n",
    "\n",
    "***Neural radiance fields***\n",
    "\n",
    "[(Mildenhall et al. 2020)](https://arxiv.org/abs/2003.08934)\n",
    "\n",
    "*Reference From [Hugo Cisneros](https://hugocisneros.com/notes/implicit_neural_representations/#org36a6837)*\n",
    "\n",
    "---\n",
    "\n",
    "#### **Here's my thought.**\n",
    "\n",
    "***The Implicit Neural Representation*** *is a full kinds of* ***mapping function***, *that identifying a specific pattern from existing phenomena and maps to an unspecified object.*\n",
    "\n",
    "*It contains various models of Deep Learning fields, especially the Generative Adversarial Networks are frequently used to construct the function nowadays, previously many approaches suggested such as statistical methods since its purpose is encoding the potential frequency as much as possible from the original phenomenon that can appear.*\n",
    "\n",
    "*One of the difference between* ***Traditional Explicit Representations*** *and* ***Implicit Neural Representation*** *is whether* ***discrete*** *or* ***contionus***, *which its property of 'continous' better represents the kind of mapping function, a distribution sampled from the existing distribution.*\n",
    "\n",
    "*The Deep Learning, which contains the huge amount of* ***techiniques to map the specific function*** *including MLP, CNN, Encoder-Decoder architecture are* ***can be seen in the same context.***\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **3. The applications and potentials of INR**\n",
    "\n",
    "There are huge usages and applicable stuffs with the Implicit Neural Representation. \n",
    "**Due to the property of Implicit Encoding that mapping the meaningful new distribution from the actual space**, It can be applicable to various domains through implicit encoding and has the advantage of being able to largerly expandable from a specific domain to various scales.\n",
    "\n",
    "One of the most applicable field with INR is **Image to Image Translation**. Image-to-image translation is called 'I2I' as a abbreviation, the I2I is a class of vision and graphics problems where the goal is to learn the mapping between an input image and an output image. It can be applied to a wide range of applications, such as collection style transfer, object transfiguration, season transfer and photo enhancement. Briefly, Image to Image Translation takes a task of taking images from one domain and transforming them so they have the style (or characteristics) of images from another domain. Here's some example.\n",
    "\n",
    "---\n",
    "\n",
    "### **Image to Image Translation**\n",
    "\n",
    "Numerous image processing problems are already being solved using convolutional neural nets (CNNs). However, it is still difficult to design effective losses when learning CNNs. For example, simply learning CNN in the direction of reducing the distance between the target and the prediction result, the model outputs a blurred image.\n",
    "\n",
    "To solve this problem, General Adversarial Networks (GANs) are used a lot in image processing problems. GANs competitively learn generative models and discriminant models. The generated model is trained so that the discriminant model cannot distinguish between the generated image and the actual image, and the discriminant model is trained to distinguish the generated image from the actual image. Through this learning process, the generated model can create a similar image to the real thing.\n",
    "\n",
    "##### **Conditional GAN (Pix2Pix)**\n",
    "\n",
    "<table>\n",
    "    <tbody>\n",
    "        <tr>\n",
    "            <td width=\"300\">\n",
    "                <img src=\"https://www.secmem.org/assets/images/pix2pix/pix2pix_images.jpg\" width=\"300\" />\n",
    "            </td>\n",
    "            <td>\n",
    "                <strong>Image-to-Image Translation with Conditional Adversarial Nets (CVPR 2017):</strong>\n",
    "                 This paper first defined image to image translation, a problem dealing with interdomain changes in images. This transformation includes coloring black and white photos and drawing objects when outlined. Previously, different models were used for each conversion problem, but the paper showed that their Pix2Pix model solves most conversion problems well.\n",
    "            </td>\n",
    "        </tr>\n",
    "        <tr>\n",
    "            <td width=\"300\">\n",
    "                <img src=\"https://learnopencv.com/wp-content/uploads/2021/07/pix-2-pix-GAN-a-type-of-CGAN.jpg\" width=\"300\" />\n",
    "            </td>\n",
    "            <td>\n",
    "                In the paper, conditional GANs (cGANs) that can include conditions were used. When converting an image, if you learn by putting an image in the generation model and the discrimination model as a condition, the generation model can generate a result image according to the input image.\n",
    "            </td>\n",
    "        </tr>\n",
    "    </tbody>\n",
    "</table>\n",
    "\n",
    "##### **Goal**\n",
    "\n",
    "<table>\n",
    "    <tbody>\n",
    "        <tr>\n",
    "            <td width=\"300\">\n",
    "                <img src=\"imgs/equation1.png\" width=\"300\" />\n",
    "            </td>\n",
    "            <td>\n",
    "                Let's call G, D the generation model and the discrimination model. \n",
    "                Let's call x, y, z for each input image, output image, and noise. The goal of cGAN is to minimize the cGAN loss defined as follows. Through this process, G learns mapping from input image x and noise z to output image y.\n",
    "            </td>\n",
    "        </tr>\n",
    "        <tr>\n",
    "            <td width=\"300\">\n",
    "                <img src=\"imgs/equation2.png\" width=\"300\" />\n",
    "            </td>\n",
    "            <td>\n",
    "                In addition to these GAN losses, the paper added traditional losses such as L1 and L2 distance. At this time, the objective function of the discriminant model remains unchanged, but the generated model further narrows the distance between the output value and the target image. Here, Pix2Pix used L1 distance instead of L2 to prevent blurring.\n",
    "            </td>\n",
    "        </tr>\n",
    "        <tr>\n",
    "            <td width=\"300\">\n",
    "                <img src=\"imgs/equation3.png\" width=\"300\" />\n",
    "            </td>\n",
    "            <td>\n",
    "                Then, the final goal is to find the generation model G as follows. Even without z in the generative model, mapping from x to y can be learned. However, the generating model will then output only one resulting image for a given input image. Therefore, past cGANs gave Gaussian distribution noise z as input of the generation model with x. In the author's experiment, the phenomenon of learning occurred when the given noise was ignored through this method, but it was not possible to find a way to solve it effectively. So Pix2Pix was noisy only through dropout and used for both learning and testing.\n",
    "            </td>\n",
    "        </tr>\n",
    "    </tbody>\n",
    "</table>\n",
    "\n",
    "\n",
    "Reference: [Pix2Pix Image to Image translation](https://www.secmem.org/blog/2020/07/19/pix2pix/)\n",
    "\n",
    "##### **CycleGAN**\n",
    "\n",
    "<table>\n",
    "    <tbody>\n",
    "        <tr>\n",
    "            <td width=\"300\">\n",
    "                <img src=\"https://miro.medium.com/max/1050/0*Udvw6tGu40iDEkuH.jpg\" width=\"300\" />\n",
    "            </td>\n",
    "            <td>\n",
    "                <i><strong>Unpaired Image-to-Image Translation Using Cycle-Consistent Adversarial Networks(ICCV 2017)</strong></i>\n",
    "                Author presents an approach for learning to translate an image from a source domain X to a target domain Y in the absence of paired examples. The goal is to learn a mapping G : X → Y such that the distribution of images from G(X) is indistinguishable from the distribution Y using an adversarial loss. Because this mapping is highly under-constrained, we couple it with an inverse mapping F : Y → X and introduce a cycle consistency loss to enforce F(G(X)) ≈ X (and vice versa).\n",
    "            </td>\n",
    "        </tr>\n",
    "        <tr>\n",
    "            <td width=\"300\">\n",
    "                <img src=\"https://miro.medium.com/max/975/0*P-46iNsLcF2edVfn.png\" width=\"300\" />\n",
    "            </td>\n",
    "            <td>\n",
    "               Paired training data (left) consists of training examples have one to one correspondence. Unpaired training set has no such correspondence.\n",
    "            </td>\n",
    "        </tr>\n",
    "        <tr>\n",
    "            <td width=\"300\">\n",
    "                <img src=\"https://sp-ao.shortpixel.ai/client/to_webp,q_glossy,ret_img/https://godatadriven.com/wp-content/images/how-to-style-transfer/ccan.png\" width=\"300\" />\n",
    "            </td>\n",
    "            <td>\n",
    "                The model contains two mapping functions G : X → Y and F : Y → X , and associated adversarial discriminators DY and DX . DY encourages G to translate X into outputs indistinguishable from domain Y , and vice versa for DX , F , and X . To further regularize the mappings, they introduce two “cycle consistency losses” that capture the intuition that if we translate from one domain to the other and back again we should arrive where we started.\n",
    "            </td>\n",
    "        </tr>\n",
    "    </tbody>\n",
    "</table>\n",
    "\n",
    "##### **StarGAN**\n",
    "\n",
    "<table>\n",
    "    <tbody>\n",
    "        <tr>\n",
    "            <td width=\"300\">\n",
    "                <img src=\"https://miro.medium.com/max/1050/0*bPadlkT94xTpp2Om.jpg\" width=\"300\" />\n",
    "            </td>\n",
    "            <td>\n",
    "                <strong>Unified Generative Adversarial Networks for Multi-Domain Image-to-Image Translation(CVPR 2018):</strong>\n",
    "                Existing image to image translation approaches have limited scalability and robustness in handling more than two domains, since different models should be built independently for every pair of image domains. StarGAN is a novel and scalable approach that can perform image-to-image translations for multiple domains using only a single model.\n",
    "            </td>\n",
    "        </tr>\n",
    "        <tr>\n",
    "            <td width=\"300\">\n",
    "                <img src=\"https://miro.medium.com/max/972/0*S7N84-uT_6zqrhxl.png\" width=\"300\" />\n",
    "            </td>\n",
    "            <td>\n",
    "                Comparison between cross-domain models and our proposed model, StarGAN. (a) To handle multiple domains, crossdomain models should be built for every pair of image domains. (b) StarGAN is capable of learning mappings among multiple domains using a single generator. The figure represents a star topology connecting multi-domains.\n",
    "            </td>\n",
    "        </tr>\n",
    "        <tr>\n",
    "            <td width=\"300\">\n",
    "                <img src=\"https://miro.medium.com/max/1050/0*9ICw3yzQ9qWnPBLK.png\" width=\"300\" />\n",
    "            </td>\n",
    "            <td>\n",
    "                Overview of StarGAN, consisting of two modules, a discriminator D and a generator G. (a) D learns to distinguish between real and fake images and classify the real images to its corresponding domain. (b) G takes in as input both the image and target domain label and generates an fake image. The target domain label is spatially replicated and concatenated with the input image. (c) G tries to reconstruct the original image from the fake image given the original domain label. (d) G tries to generate images indistinguishable from real images and classifiable as target domain by D.\n",
    "            </td>\n",
    "        </tr>\n",
    "    </tbody>\n",
    "</table>\n",
    "\n",
    "Reference: [Image-to-Image Translation](https://towardsdatascience.com/image-to-image-translation-69c10c18f6ff)\n",
    "\n",
    "---\n",
    "\n",
    "### **Image generation**\n",
    "\n",
    "##### **Style GAN**\n",
    "\n",
    "<table>\n",
    "    <tbody>\n",
    "        <tr>\n",
    "            <td width=\"300\">\n",
    "                <img src=\"https://raw.githubusercontent.com/happy-jihye/happy-jihye.github.io/master/_posts/images/gan/stylegan-1.gif\" width=\"300\" />\n",
    "            </td>\n",
    "            <td>\n",
    "                <strong>A Style-Based Generator Architecture for Generative Adversarial Networks (StyleGAN) [CVPR 2019]:</strong>\n",
    "                This model is a model that develops the architecture of the generator to better learn the style without touching the descriptor or loss function. Image synthesis was developed to style scale-specific control by modifying the existing PGGAN (ProGAN) model. As in the picture on the left, it is style scale-specific control that can scale a specific style.\n",
    "            </td>\n",
    "        </tr>\n",
    "        <tr>\n",
    "            <td width=\"300\">\n",
    "                <img src=\"https://user-images.githubusercontent.com/37301677/84801592-55112480-b03a-11ea-96ab-7df7d933430b.png\" width=\"300\" />\n",
    "            </td>\n",
    "            <td>\n",
    "                Learned by covering each layer with style information in the Generator network. The style of the image (gender, pose). You can change your hair color, skin tone, etc.). Latent vectors tend to follow the distribution of specific datasets as they are, and to compensate for this, they do not use them as they are, but use the wvector created by creating the Mapping network to change their styles more diversely.\n",
    "            </td>\n",
    "        </tr>\n",
    "        <tr>\n",
    "            <td width=\"300\">\n",
    "                <img src=\"https://raw.githubusercontent.com/happy-jihye/happy-jihye.github.io/master/_posts/images/gan/stylegan-9.PNG\" width=\"300\" />\n",
    "            </td>\n",
    "            <td>\n",
    "                Style mixing is mixing the style as the word suggests. After sampling (z1,z2), two style codes (w1,w2) are created through a mapping network. After setting a certain reference point, two styles are added to mix the styles. When mixing regularization is performed in this way, the styles are not correlated.\n",
    "            </td>\n",
    "        </tr>\n",
    "    </tbody>\n",
    "</table>\n",
    "\n",
    "Reference: [Style Gan](https://happy-jihye.github.io/gan/gan-6/), [Gan Guide](https://ysbsb.github.io/gan/2020/06/17/GAN-newbie-guide.html)\n",
    "\n",
    "---\n",
    "\n",
    "### **Neural Rendering**\n",
    "\n",
    "To start with some definitions, the larger field of Neural rendering is defined by the excellent review paper by [Tewari et al](https://www.neuralrender.com/assets/downloads/TewariFriedThiesSitzmannEtAl_EG2020STAR.pdf). as\n",
    "\n",
    "> *\"deep image or video generation approaches that enable explicit or implicit control of scene properties such as illumination, camera parameters, pose, geometry, appearance, and semantic structure.\"*\n",
    "\n",
    "It is a novel, data-driven solution to the long-standing problem in computer graphics of the realistic rendering of virtual worlds.\n",
    "\n",
    "Neural volume rendering refers to methods that generate images or video by tracing a ray into the scene and taking an integral of some sort over the length of the ray. Typically a neural network like a multi-layer perceptron encodes a function from the 3D coordinates on the ray to quantities like density and color, which are integrated to yield an image.\n",
    "\n",
    "<table>\n",
    "    <tbody>\n",
    "        <tr>\n",
    "            <td width=\"300\">\n",
    "                <img src=\"./imgs/nerf.gif\" width=\"300\" />\n",
    "            </td>\n",
    "            <td>\n",
    "                <strong>NeRF: Representing Scenes as Neural Radiance Fields for View Synthesis [ECCV 2020].</strong>\n",
    "                The paper that got everyone talking was the Neural Radiance Fields or NeRF paper, with three first authors from Berkeley. In essence, they take the DeepSDF architecture but regress not a signed distance function, but density and color. They then use an (easily differentiable) numerical integration method to approximate a true volumetric rendering step. One of the reasons NeRF is able to render with great detail is because it encodes a 3D point and associated view direction on a ray using periodic activation functions, i.e., Fourier Features. This innovation was later generalized to multi-layer networks with periodic activations, aka SIREN (SInusoidal REpresentation Networks). Both were published later at NeurIPS 2020.\n",
    "            </td>\n",
    "        </tr>\n",
    "        <tr>\n",
    "            <td width=\"300\">\n",
    "                <img src=\"https://uploads-ssl.webflow.com/51e0d73d83d06baa7a00000f/5e700a025ff238947d682a1f_pipeline_website-03.svg\" width=\"300\" />\n",
    "            </td>\n",
    "            <td>\n",
    "                presents a method that achieves state-of-the-art results for synthesizing novel views of complex scenes by optimizing an underlying continuous volumetric scene function using a sparse set of input views. Their algorithm represents a scene using a fully-connected (non-convolutional) deep network, whose input is a single continuous 5D coordinate (spatial location (x, y, z) and viewing direction (θ, φ)) and whose output is the volume density and view-dependent emitted radiance at that spatial location.\n",
    "            </td>\n",
    "        </tr>\n",
    "        <tr>\n",
    "            <td width=\"300\">\n",
    "                <img src=\"https://uploads-ssl.webflow.com/51e0d73d83d06baa7a00000f/5e700ef6067b43821ed52768_pipeline_website-01-p-800.png\" width=\"300\" />\n",
    "            </td>\n",
    "            <td>\n",
    "                They synthesize views by querying 5D coordinates along camera rays and use classic volume rendering techniques to project the output colors and densities into an image. Because volume rendering is naturally differentiable, the only input required to optimize our representation is a set of images with known camera poses. We describe how to effectively optimize neural radiance fields to render photorealistic novel views of scenes with complicated geometry and appearance, and demonstrate results that outperform prior work on neural rendering and view synthesis.\n",
    "            </td>\n",
    "        </tr>\n",
    "    </tbody>\n",
    "</table>\n",
    "\n",
    "Reference: [NeRF](https://www.matthewtancik.com/nerf), [NeRF Explosion 2020](https://dellaert.github.io/NeRF/)\n",
    "\n",
    "---\n",
    "\n",
    "### **Style Transfer**\n",
    "\n",
    "The term \"style transfer\" is used to describe the operation of recomposing one image in the style of another (group of) image(s). In general, this requires two inputs: content image(s) and style image(s). The technique applied should then produce output whose “content” mirrors the content image and whose \"style\" resembles that of the style image.\n",
    "\n",
    "<table>\n",
    "    <tbody>\n",
    "        <tr>\n",
    "            <td width=\"300\">\n",
    "                <img src=\"https://sp-ao.shortpixel.ai/client/to_webp,q_glossy,ret_img/https://godatadriven.com/wp-content/images/how-to-style-transfer/style-transfer-example.jpg\" width=\"300\" />\n",
    "            </td>\n",
    "            <td>\n",
    "                Left an example with an original cat photo on the left (the content image), which is \"styled\" in three different ways using the style images in the middle. The different styles lead to different generated output images, as can be seen on the right.\n",
    "            </td>\n",
    "        </tr>\n",
    "        <tr>\n",
    "            <td width=\"300\">\n",
    "                <img src=\"https://sp-ao.shortpixel.ai/client/to_webp,q_glossy,ret_img/https://godatadriven.com/wp-content/images/how-to-style-transfer/ns-result.png\" width=\"300\" />\n",
    "            </td>\n",
    "            <td>\n",
    "                <strong>Neural style transfer</strong> was first demonstrated in August 2015 in a paper published by Gatys, Ecker, and Bethge at the University of Tübingen. The algorithm is described well on ml4a, a website by Gene Kogan that provides free educational resources about machine learning for artists.\n",
    "                In short, the objective of the style transfer algorithm is to generate an output image which minimizes a loss function that is the sum of two separate terms, a “content loss” and a “style loss.” The content loss represents the dissimilarity between the content image and the output image, whereas the style loss represents dissimilarity between the style image and the output image.\n",
    "                The algorithm trains a single convolutional neural network, wherein at each iteration the output image’s pixels are slightly adjusted so as to decrease the overall loss. We do this repeatedly until the loss converges, or until we are satisfied with our result.\n",
    "            </td>\n",
    "        </tr>\n",
    "        <tr>\n",
    "            <td width=\"300\">\n",
    "                <img src=\"https://sp-ao.shortpixel.ai/client/to_webp,q_glossy,ret_img/https://godatadriven.com/wp-content/images/how-to-style-transfer/style-transfer-explained.jpg\" width=\"300\" />\n",
    "            </td>\n",
    "            <td>\n",
    "                The content and style loss are defined differently:\n",
    "                <ul>\n",
    "                    <li>\n",
    "                        For the content loss the output image and content image are run through the convnet, giving us a set of feature maps for both. The loss at a single layer is then the euclidean (L2) distance between the activations of the content image and the activations of the output image.\n",
    "                    </li>\n",
    "                    <li>\n",
    "                        For the style loss we also use the convnet’s activations, but instead of comparing the raw activations directly, we take the <a href=\"https://towardsdatascience.com/neural-networks-intuitions-2-dot-product-gram-matrix-and-neural-style-transfer-5d39653e7916\">Gram matrix</a> of the activations at each layer in the network. This is a way to capture correlations between features in different parts of the image, which turns out to be a very good representation of our perception of style within images. The style loss at a single layer is then defined as the euclidean (L2) distance between the Gram matrices of the style and output images.\n",
    "                    </li>\n",
    "                </ul>\n",
    "            </td>\n",
    "        </tr>\n",
    "    </tbody>\n",
    "</table>\n",
    "\n",
    "Reference: [How to style transfer your own images](https://godatadriven.com/blog/how-to-style-transfer-your-own-images/)\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **4. About Style Transfer**\n",
    "\n",
    "\"Image Style Transfer\" is the problem is to take two images, extract content from one image, style(texture) from the other and seamlessly merge them together into one final image that looks realistic. Here, I'm going to explain the first attempt of Image Style Transfer, the paper called ['A Neural Algorithm of Artistic Style' by Gatys et. al'](https://arxiv.org/abs/1508.06576)\n",
    "\n",
    "<table>\n",
    "    <tbody>\n",
    "        <tr>\n",
    "            <td width=\"400\">\n",
    "                <img src=\"https://miro.medium.com/max/1034/1*-DKzmMajUn45Hu0pe_PUyQ.jpeg\" width=\"400\" />\n",
    "            </td>\n",
    "            <td>\n",
    "                <strong>Overview</strong>: Let me give a brief overview of the solution\n",
    "                <ol>\n",
    "                    <li>\n",
    "                    Create a random input image\n",
    "                    </li>\n",
    "                    <li>\n",
    "                    Pass the input through a pretrained backbone architecture say VGG, ResNet(note that this backbone will not be trained during backpropagation).\n",
    "                    </li>\n",
    "                    <li>\n",
    "                    Calculate loss and compute the gradients w.r.t input image pixels. Hence only the input pixels are adjusted whereas the weights remain constant.\n",
    "                    </li>\n",
    "                </ol>\n",
    "                The objective is to change the input image so that it represents the content and the style from the respective images.\n",
    "            </td>\n",
    "        </tr>\n",
    "    </tbody>\n",
    "</table>\n",
    "\n",
    "This problem consists of two sub problems: \n",
    "\n",
    "1. to generate the content\n",
    "\n",
    "2. to generate the style.\n",
    "\n",
    "**Problem — 1. Generate Content**\n",
    "\n",
    "The problem is to produce an image that contains a content as in the content image.\n",
    "\n",
    "> One point to note here is that the image should only contain the content(as in a rough sketch of the content image and not the texture from it, since the output should contain a style as that of the style image)\n",
    "\n",
    "**Solution**: The answer should be pretty straightforward. Use MSE loss(or any similarity measure such as SSIM, PSNR) between input and the target. But what is the target here? Well, if there were no constraints on the style of the output, then simply MSE between input and content image would be sufficient. So how to get an image’s content without copying its style?\n",
    "\n",
    "***\"Use feature maps.\"***\n",
    "\n",
    "Convolutional feature maps are generally a very good representation of input image’s features. They capture spatial information of an image without containing the style information(if a feature map is used as it is), which is what we need. And this is the reason we keep the backbone weights fixed during backpropagation.\n",
    "\n",
    "> Therefore, MSE loss between the input image’s features and content image’s features will work!\n",
    "\n",
    "<table>\n",
    "    <tbody>\n",
    "        <tr>\n",
    "            <td>\n",
    "                <img src=\"https://miro.medium.com/max/702/1*PKnjB3bxzgg6yy0uOsljqw.png\" width=\"400\" />\n",
    "            </td>\n",
    "        </tr>\n",
    "        <tr>\n",
    "            <td>\n",
    "                Content Loss\n",
    "            </td>\n",
    "        </tr>\n",
    "    </tbody>\n",
    "</table>\n",
    "\n",
    "\n",
    "**Problem — 2. Generate Style**\n",
    "\n",
    "The problem is to produce an image which contains the style as in the style image.\n",
    "\n",
    "**Solution**: To extract the style of an image(or more specifically to compute the style loss), we need something called as Gram matrix. Wait, what is a Gram matrix?\n",
    "Before talking about how to compute the style loss, let me talk about some math basics.\n",
    "\n",
    "\"Dot Product.\"\n",
    "\n",
    "A dot product of two vectors is the sum of products of respective coordinates.\n",
    "\n",
    "> So what does this mean?\n",
    "\n",
    "In a more intuitive way, dot product can be seen as how similar two vectors actually are. The more similar they are, the lesser the angle between them as in fig (a) or more closer the respective coordinates as in fig(b). In both the cases, the result is large. So the more similar they are, the larger the dot product gets.\n",
    "\n",
    "But what does this have to do with a neural network?\n",
    "\n",
    "> Consider two vectors(more specifically 2 flattened feature vectors from a convolutional feature map of depth C) representing features of the input space, and their dot product give us the information about the relation between them. The lesser the product the more different the learned features are and greater the product, the more correlated the features are. In other words, the lesser the product, the lesser the two features co-occur and the greater it is, the more they occur together. This in a sense gives information about an image’s style(texture) and zero information about its spatial structure, since we already flatten the feature and perform dot product on top of it.\n",
    "\n",
    "Now take all C feature vectors(flattened) from a convolutional feature map of depth C and compute the dot product with every one of them(including with a feature vector itself). The result is the Gram Matrix(of size CxC).\n",
    "\n",
    "<table>\n",
    "    <tbody>\n",
    "        <tr>\n",
    "            <td>\n",
    "                <img src=\"https://miro.medium.com/max/389/1*C3fkQanKHMwOi_rf0q0OQQ.png\" width=\"400\" />\n",
    "            </td>\n",
    "        </tr>\n",
    "        <tr>\n",
    "            <td>\n",
    "                Gram Matrix\n",
    "            </td>\n",
    "        </tr>\n",
    "    </tbody>\n",
    "</table>\n",
    "\n",
    "There you go!\n",
    "\n",
    "> Compute MSE loss between gram matrix of input and the style image and you’re good to generate an input image with the required style.\n",
    "\n",
    "<table>\n",
    "    <tbody>\n",
    "        <tr>\n",
    "            <td>\n",
    "                <img src=\"https://miro.medium.com/max/560/1*0nASi-BjZI3I9J0RWo7wtw.png\" width=\"400\" />\n",
    "            </td>\n",
    "        </tr>\n",
    "        <tr>\n",
    "            <td>\n",
    "                Summing MSE of Gram matrices for all layers, normalizing and computing a weighted sum in the end\n",
    "            </td>\n",
    "        </tr>\n",
    "    </tbody>\n",
    "</table>\n",
    "\n",
    "Finally, <strong>add both the content and style loss</strong> before backpropagating to get an output image which has the content from a content image and the style from a style image.\n",
    "\n",
    "A normal sum of both the losses may not work when the losses are in a different scale. So a weighted sum of content and style loss should work.\n",
    "\n",
    "\n",
    "<table>\n",
    "    <tbody>\n",
    "        <tr>\n",
    "            <td>\n",
    "                <img src=\"https://miro.medium.com/max/1262/1*39DOPiFLq8TcncxuLKro7Q.png\" width=\"400\" />\n",
    "            </td>\n",
    "        </tr>\n",
    "        <tr>\n",
    "            <td>\n",
    "                Total loss\n",
    "            </td>\n",
    "        </tr>\n",
    "    </tbody>\n",
    "</table>\n",
    "\n",
    "Note that the input image can be any random tensor <strong>which has the values in the same range</strong> as the content and style image.\n",
    "\n",
    "Few of the results from my implementation:\n",
    "\n",
    "<img src=\"https://miro.medium.com/max/1400/1*xa9V7EEgW-iwwsDeZhnymw.jpeg\" width=\"400\" />\n",
    "<img src=\"https://miro.medium.com/max/1400/1*cMRLk8roY5N88XqLYx0SOQ.jpeg\" width=\"400\" />\n",
    "<img src=\"https://miro.medium.com/max/1400/1*AL9gi_6AEQntjZVn7uR4AA.jpeg\" width=\"400\" />\n",
    "\n",
    "Reference from: [A Neural Algorithm of Artistic Style](https://arxiv.org/abs/1508.06576), [Neural Networks Intuitions: 2. Dot product, Gram Matrix and Neural Style Transfer](https://towardsdatascience.com/neural-networks-intuitions-2-dot-product-gram-matrix-and-neural-style-transfer-5d39653e7916)\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **5. What Style Transfer ultimately purpose**\n",
    "\n",
    "Style transfer is actively studied and there are still enormous papers pouring out in a wide variety of fields. However, through applications currently applicable, we will be able to grasp its potential. We don't know in what direction style transfer will eventually evolve, but we can get hints from the current situation.\n",
    "\n",
    "Here, I will provide an overview of real-world use cases for style transfer. I’ll dive a bit deeper and explore the impact this computer vision technique can have across industries.\n",
    "\n",
    "Specifically, I’ll examine how style transfer can be used in the following areas:\n",
    "\n",
    "- Photo and video editors\n",
    "- Commercial art\n",
    "- Gaming\n",
    "- Virtual Reality\n",
    "\n",
    "#### **Photo and video editors**\n",
    "\n",
    "One of the most clear applications of style transfer is in photo and video editing software. From [sharing stylized selfies](https://heartbeat.fritz.ai/community-spotlight-looq-cb52060947fa) to [augmenting user-generated music videos](https://heartbeat.fritz.ai/video-star-creating-custom-artistic-filters-for-music-videos-with-mobile-machine-learning-3be0ed5a3029), and beyond, the ability to add famous art styles to images and video clips promises to add unprecedented power to these kinds of creativity tools.\n",
    "\n",
    "And given the flexibility and performance of current deep learning approaches, style transfer models can easily be embedded on edge devices—for instance, mobile phones—allowing for applications that can process and transform images and video in real-time. This means that professional-quality photo and video editing tools can become more widely accessible and easier to use than ever before.\n",
    "\n",
    "There are already a number of incredible tools that employ style transfer as part of their toolkits. Here are a few fun projects, apps, and demos to play with:\n",
    "\n",
    "- [Video Star](https://apps.apple.com/us/app/video-star/id438596432) (Art Studio, iOS)\n",
    "- [Painter’s Lens](https://apps.apple.com/us/app/painters-lens/id1458827474) (iOS)\n",
    "- [Looq](https://apps.apple.com/us/app/looq-ai-powered-filters/id1159704664) (iOS)\n",
    "- [Instapainting’s AI Painter](https://www.instapainting.com/ai-painter) (Desktop)\n",
    "- [Arbitrary Style Transfer in the Browser](https://reiinakano.com/arbitrary-image-stylization-tfjs/) (Desktop)\n",
    "\n",
    "<img src=\"./imgs/st%202.png\" width=\"600\" />\n",
    "\n",
    "#### **Commercial art**\n",
    "\n",
    "From poetry to artwork to music, we’ve only just entered the era of AI creations. And style transfer is one of the computer vision techniques making this wave of - AI-powered artwork possible.\n",
    "\n",
    "Whether it’s artwork sold at a [high-end auction](https://www.theverge.com/2018/10/25/18023266/ai-art-portrait-christies-obvious-sold) or [up-and-coming artists](https://apps.apple.com/us/app/painters-lens/id1458827474) finding new ways to share their aesthetic with the world, style transfer promises to change the ways we think about art, what originality means, and how we present art in the real world.\n",
    "\n",
    "We could also imagine style transfer being used to create reproducible, high-quality prints for office buildings, or for large-scale advertising campaigns. These are just a few possible ways in which style transfer could change how we view the commercial impacts of art.\n",
    "\n",
    "#### **Gaming**\n",
    "\n",
    "At a press conference during the 2019 Game Developers Conference, [Google introduced Stadia](https://venturebeat.com/2019/03/19/googles-stadia-uses-style-transfer-ml-to-manipulate-video-game-environments/), it’s cloud-powered video game streaming service. And one of the primary features included in that demo was an in-game style transfer feature that automatically recomposes the virtual world with textures and color palettes from a potentially limitless range of art styles.\n",
    "\n",
    "Google noted that efforts like these are intended to “empower the artist inside of every developer.” This implies that gaming, much like other applications of style transfer, will broaden the accessibility of artistic creation to those who we might not traditionally see as artists.\n",
    "\n",
    "#### **Virtual reality**\n",
    "\n",
    "Much like gaming, where immersive virtual worlds represent the anchor of the user experience, virtual reality has also had its share of interest in exploring what’s possible with style transfer.\n",
    "\n",
    "While the applications with style transfer in VR are still largely in the research phase, the possibilities are exciting and promising. [Facebook touts the potential of style transfer](https://engineering.fb.com/2017/07/26/virtual-reality/using-ai-for-new-visual-storytelling-techniques-in-vr/) to radically alter the ways VR developers tell visual stories through their applications, games, films, and more.\n",
    "\n",
    "And there are some early demos that show how style transfer could help augment the immersiveness of worlds created for virtual reality. Here’s one example\n",
    "\n",
    "<img src=\"./imgs/st.png\" width=\"500\" />\n",
    "\n",
    "[Style transfer for a 360° VR video](https://www.youtube.com/watch?v=pkgMUfNeUCQ&t=20s)\n",
    "\n",
    "Reference: [Style transfer for a 360° VR video](https://www.youtube.com/watch?v=pkgMUfNeUCQ&t=20s), [AI Painter](https://www.instapainting.com/ai-painter), [Augmented User Generated Music Videos](https://medium.com/@austin_32493/video-star-creating-custom-artistic-filters-for-music-videos-with-mobile-machine-learning-3be0ed5a3029), \n",
    "[Style Transfer Guide](https://www.fritz.ai/style-transfer/)\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **6. Paper review for basic understanding for Style Transfer via INR**\n",
    "\n",
    "While various techniques for style transfer have emerged and are developing them, there is a technique called normalization. Normalization helps to approximate mapping functions for a particular distribution, which helps to estimate well from the late space in style transfer that does implicit encoding. Therefore, in this section, we will review normalization and style transfer according to its development.\n",
    "\n",
    "#### **1. Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift**\n",
    "> **Authors: Sergey Ioffe, Christian Szegedy - Google Inc., {sioffe, szegedy} @google.com**\n",
    "\n",
    "> **Original Paper**: https://arxiv.org/pdf/1502.03167.pdf\n",
    "\n",
    "> **Un official Github**: https://github.com/shuuki4/Batch-Normalization\n",
    "\n",
    "#### **2. Instance Normalization: The Missing Ingredient for Fast Stylization**\n",
    "\n",
    "> **Authors: Dmitry Ulyanov [dmitry.ulyanov@skoltech.ru], Andrea Vedaldi [vedaldi@robots.ox.ac.uk], Victor Lempitsky [lempitsky@skoltech.ru]**\n",
    "\n",
    "> **Original Paper**: https://arxiv.org/pdf/1607.08022.pdf\n",
    "\n",
    "> **Official Github**: https://github.com/DmitryUlyanov/texture_nets\n",
    "\n",
    "#### 3. **Arbitrary Style Transfer in Real-time with Adaptive Instance Normalization**\n",
    "\n",
    "> **Authors: Xun Huang, Serge Belongie - Department of Computer Science & Cornell Tech, Cornell University, {xh258,sjb344}@cornell.edu**\n",
    "\n",
    "> **Official Github**: https://github.com/xunhuang1995/AdaIN-style\n",
    "\n",
    "> **Original Paper**: https://arxiv.org/pdf/1703.06868.pdf\n",
    "\n",
    "#### **4. Semantic Image Synthesis with Spatially-Adaptive Normalization**\n",
    "\n",
    "> **Authors: Taesung Park, Ming-Yu Liu, Ting-Chun Wang, Jun-Yan ZhuL**\n",
    "\n",
    "> **Original Paper**: https://arxiv.org/pdf/1903.07291.pdf\n",
    "\n",
    "> **Official Github**: https://github.com/NVlabs/SPADE\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Why Internal Covariance Shift Matters?**\n",
    "\n",
    "At the first when I encounter the *'Normalization'* term in INR (Implicit Neural Representation) Field, I thought it is just a one of the common technique(at least the current point of Jan, 2022) to enhance the deep learning models' performance.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **1. Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift**\n",
    "\n",
    "### **Overview**\n",
    "\n",
    "The paper suggest the three main things that enhance the neural networks' performance by using the one of the technique called 'Batch Normalization', and the suggested benefits are below.\n",
    "\n",
    "1. Allow to use higher learning rate\n",
    "2. Allow to be less careful about initializations\n",
    "3. Acting as a regularizer, in some cases eliminating the needs of Dropout\n",
    "4. When using a nonlinear function (such as sigmoid), it prevents falling into the saturated region (where the slope is zero). Therefore, it prevents slope loss and saturation problem.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Presented Previous Problem**\n",
    "\n",
    "With using SGD(Mini-Batch Stochastic Gradient Descent), it is good algorithm to train the network, however there are several problems.\n",
    "\n",
    "1. It requires careful tuning of the model **hyper-parameters**, specifically the ***learning rate*** used in optimization, as well as the inital values for the ***model parameters***.\n",
    "\n",
    "2. The change in the distributions of layers' input presents a problem because the layers need to continuosly adapt to the new distribution.\n",
    "\n",
    "*When the input distribution to a learning system changes, it is is said to experience* ***Covariance Shift***.\n",
    "\n",
    "> This is typically handled via domain adaption. However, the notion of covariate shift can be extended beyond the learning system as a whole, to apply its parts, such as a sub-network or a layer.\n",
    "\n",
    "#### **Internal Covariate Shift**\n",
    "\n",
    "***The authors explained that batch normalization can speed up learning because it reduces internal covariates.*** Then let's find out what the internal covariate shift is.\n",
    "\n",
    "> For reference, in a follow-up paper on batch normalization in 2017, the reason why batch normalization shows good performance is that it has nothing to do with changes in internal covariates and has a smoothing effect on optimization. If you are interested, I recommend you to read \"SANTURKAR, Shibani, et al. How Does Batch Normalization Help Optimization?\"\n",
    "\n",
    "In the paper, they defined ***Internal Covariant Shift*** as a change in the distribution of neural network activation due to changes in neural network parameters during learning. And they say that learning can be improved only by reducing this Internal Covariate Sheet. As the neural network deepens as shown in the figure below, the distribution of activation changes.\n",
    "\n",
    "<img src=\"./imgs/bn.png\" width=\"400\" />\n",
    "\n",
    "It can also be thought that the distribution of input values changes for each layer in the internal cavity shift.\n",
    "\n",
    "#### **Disadvantages of Internal Covariate Shift**\n",
    "\n",
    "Internal Covariate Shift falls into the saturated regime (the part with zero slope in the activation function) and loses and explodes.\n",
    "\n",
    "The above drawbacks can be solved by using the ReLU function, carefully selecting the initial value, or applying a low learning rate. However, it is difficult to use the appropriate initial values, and applying a small learning rate slows the learning speed. If you solve the problem of internal covariate change, you can reduce your dependence on initial values, speed up learning by applying a high learning rate, and solve the problem of slope loss and explosion.\n",
    "\n",
    "Then, how can we solve the problem of internal covariate change?\n",
    "\n",
    "#### **The Method to reduce Internal Covariate Shifts**\n",
    "\n",
    "The paper explains the ***whitening technique***. It is said that applying whitening to the input value of every hidden layer has a fixed distribution, reducing the influence of the internal cavity shift.\n",
    " \n",
    "\n",
    "However, if the whitening technique is applied, the amount of computation is too large and backpropagation causes problems. That's why they say two simplifications should be applied.\n",
    " \n",
    "\n",
    "First of all, let's find out what the whitening technique is.\n",
    "\n",
    "##### **Whitening**\n",
    "\n",
    "Whitening means linear transformation that means 0, variance 1, and decorlated. It is said that applying whitening to the input value of each layer will reduce the internal coverage shift. But there was a problem.\n",
    " \n",
    "**The problem with the whitening.**\n",
    "\n",
    "- There is a large amount of computation because it requires a cooperation matrix calculation and inverse calculation.\n",
    "- Whitening reduces the gradient descent effect by ignoring the effects of some parameters.\n",
    "- Applying the whitening technique to every layer input reduces the effectiveness of gradient descent due to problems in the optimization stage.\n",
    "\n",
    "Why does it reduce the effectiveness of gradient descent?\n",
    "\n",
    "\n",
    "Let x be the addition of deflection b to the input u of the layer (x = u + b). Subtract the mean here to make the normal size (x - E[x]). In the process of subtracting the average value, b also falls out, and eventually the effect of b on the output disappears. If you divide it into standard deviations or even do the scaling process, this problem will weaken further.\n",
    "\n",
    "\n",
    "It is said that whitening of each layer's input requires a lot of computation, and because differentiation is not possible everywhere, two simplifications are needed. Let's find out how we simplified and solved the above problems."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Method**\n",
    "\n",
    "#### **Two simplifications to solve problems**\n",
    "\n",
    "There were two problems with reducing the internal coverage shift by whitening the input of each layer. We use two methods to solve this problem.\n",
    "\n",
    " \n",
    "**The first method**\n",
    "\n",
    "Instead of simultaneously whitening the input and output of the layer, it independently normalizes each scalar feature to have an average of 0 and a variance of 1.\n",
    "\n",
    " \n",
    "<img src=\"https://img1.daumcdn.net/thumb/R1280x0/?scode=mtistory2&fname=https%3A%2F%2Fblog.kakaocdn.net%2Fdn%2Fc5tc6q%2FbtqUM8WFEEP%2Ff0JUdwRtu5XYjd4KimXO91%2Fimg.png\" width=\"200\" />\n",
    "\n",
    "This normalization is said to speed up convergence even though the feature is not decorated.\n",
    "\n",
    "However, the normalization above had a disadvantage. Normalizing sigmoid input as above limits the linear time of non-linearity. Therefore, you get nonlinearity. \n",
    "\n",
    " \n",
    "Input exists only in the indicated area above, resulting in loss of nonlinearity.\n",
    "\n",
    "To solve this problem, add a learnable parameter gamma and beta that scales and shifts the normalized values.\n",
    "\n",
    "<img src=\"https://img1.daumcdn.net/thumb/R1280x0/?scode=mtistory2&fname=https%3A%2F%2Fblog.kakaocdn.net%2Fdn%2FQObV6%2FbtqUzuN0p0O%2FXUWxjJcQz7jwAzU35Ckww1%2Fimg.png\" width=\"200\" />\n",
    "\n",
    "\n",
    "If you scale and shift the normalized values with an average of 0, a variance of 1, with gamma, beta, you will be able to get out of the linear zone of sigmoid.\n",
    "\n",
    "\n",
    "**Second method**\n",
    "\n",
    "Normalize processes the entire dataset, but SGD processes data in batch units. Therefore, when SGD is used, it becomes impractical to normalize the entire dataset. Therefore, we also do normal size in batch units.\n",
    "\n",
    "<img src=\"https://www.licor.com/bio/guide/westerns/images/linear-range-graph.png\" width=\"400\" />\n",
    "\n",
    "Normalize the input of the layer with each mini-batch data. In this way, all statistical values used for normalization can participate in backpropagation. Processing normalize in mini-batch units has the advantage of reducing the amount of computation processed at once."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Solution**\n",
    "\n",
    "Finally, with using the two designed methods, the batch normalization formula is as follows.\n",
    "\n",
    "<img src=\"https://img1.daumcdn.net/thumb/R1280x0/?scode=mtistory2&fname=https%3A%2F%2Fblog.kakaocdn.net%2Fdn%2FbEJm3G%2FbtqUwxEkcBm%2FPOGFVhPqVMqeN29yoEl0LK%2Fimg.png\" width=\"400\" />\n",
    "\n",
    "Through this process, the activation values of each layer will have a stable distribution during learning."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Contribution**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Batch normalization is a proposed method in 2015. Batch normalization is used by many people. For the conclusion, the reasons for the attention are as follows.\n",
    "\n",
    "- You can proceed with learning quickly. (High learning rate is applicable)\n",
    "- It doesn't depend much on the initial value. (We improved the initial value selection failure)\n",
    "- It suppresses overfitting. (reduced the need for dropout.)\n",
    "- When using a nonlinear function (such as sigmoid), it prevents falling into the saturated region (where the slope is zero). Therefore, it prevents slope loss and saturation problem."
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "3efb3a3e09a38f93b1d9bfa4b9b8ce66a167f6ada77b64bc003d7d6d298d81d5"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 64-bit ('buddhalight': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
