{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **About INR and Style Transfer: the ulitmate purpose of style transfer within INR, by introducing its background and papers**\n",
    "\n",
    "**Edited By Su Hyung Choi (Key Summary & Code Practice)**\n",
    "\n",
    "If you have any issues on this scripts, please PR to the repository below.\n",
    "\n",
    "**[Github: @JonyChoi - Computer Vision Paper Reviews]** https://github.com/jonychoi/Computer-Vision-Paper-Reviews\n",
    "\n",
    "Edited Jan 16 2022\n",
    "\n",
    "---\n",
    "\n",
    "Here, I wrote the writing contains about the *\"what are Implicit Neural Representations'*, the basic concept of INR, and its potential usage and applications. With the understanding of INR, I introduced the its representative papers within few couples of years to apprehend its trend and evolution history."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Overview**\n",
    "\n",
    "**1. The Goal of the Writing**\n",
    "\n",
    "**2. What are Implicit Neural Representations**\n",
    "\n",
    "**3. The applications and potentials of INR**\n",
    "\n",
    "**4. About Style Transfer**\n",
    "\n",
    "**5. What Style Transfer ultimately purpose**\n",
    "\n",
    "**6. Paper review for basic understanding for Style Transfer via INR**\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **1. The Goal of the Writing**\n",
    "\n",
    "Here, as a beginner of *Style Transfer* within *INR(Implicit Neural Representation)* field, I'm going to introduce what are implict neural representations and style transfer with its *basic concept* and *the usages with the potential applicablity*, by *reviewing the core papers* that enhance understanding of Style Transfer within INR field.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **2. What are Implicit Neural Representations?**\n",
    "\n",
    "---\n",
    "\n",
    "#### **Implicit neural representations**\n",
    "\n",
    "***\"Implicit neural representations is about parameterizing a continuous differentiable signal with a neural network. The signal is encoded within the neural network, providing a possibly more compact representation.*** *This is a type of [regression](https://hugocisneros.com/notes/machine_learning/) problem. Applications of these learned representations range from simple [compression](https://hugocisneros.com/notes/compression/), to 3D scene reconstruction from 2D images, semantic information inference, etc.[CPPN](https://hugocisneros.com/notes/cppn/) is an early example of a implicit neural representation implementation mainly used for pattern generation. It uses a neural network to generate patterns parameterized by two (or more) coordinates.\"*\n",
    "\n",
    "***by [Hugo Cisneros](https://hugocisneros.com/notes/implicit_neural_representations/#org36a6837)***\n",
    "\n",
    "***\"Implicit Neural Representations (sometimes also referred to as coordinate-based representations) are a novel way to parameterize signals of all kinds.*** Conventional signal representations are usually discrete - *for instance, images are discrete grids of pixels, audio signals are discrete samples of amplitudes, and 3D shapes are usually parameterized as grids of voxels, point clouds, or meshes. In contrast, Implicit Neural Representations parameterize a signal as a ***continuous function*** that maps the domain of the signal (i.e., a coordinate, such as a pixel coordinate for an image) to whatever is at that coordinate (for an image, an R,G,B color). Of course, these functions are usually not analytically tractable - it is impossible to \"write down\" the function that parameterizes a natural image as a mathematical formula. Implicit Neural Representations thus approximate that function via a neural network.\"*\n",
    "\n",
    "***by [Vincent Sitzman](https://github.com/vsitzmann/awesome-implicit-representations#what-are-implicit-neural-representations)***\n",
    "\n",
    "---\n",
    "\n",
    "#### **Why are they interesting?**\n",
    "\n",
    "Implicit Neural Representations have several benefits: First, they are not coupled to spatial resolution anymore, the way, for instance, an image is coupled to the number of pixels. This is because they are continuous functions! Thus, the memory required to parameterize the signal is independent of spatial resolution, and only scales with the complexity of the underyling signal. Another corollary of this is that implicit representations have \"infinite resolution\" - they can be sampled at arbitrary spatial resolutions.\n",
    "\n",
    "This is immediately useful for a number of applications, such as super-resolution, or in parameterizing signals in 3D and higher dimensions, where memory requirements grow intractably fast with spatial resolution. Further, generalizing across neural implicit representations amounts to learning a prior over a space of functions, implemented via learning a prior over the weights of neural networks - this is commonly referred to as meta-learning and is an extremely exciting intersection of two very active research areas! Another exciting overlap is between neural implicit representations and the study of symmetries in neural network architectures - for intance, creating a neural network architecture that is 3D rotation-equivariant immediately yields a viable path to rotation-equivariant generative models via neural implicit representations.\n",
    "\n",
    "Another key promise of implicit neural representations lie in algorithms that directly operate in the space of these representations. In other words: What's the \"convolutional neural network\" equivalent of a neural network operating on images represented by implicit representations?\n",
    "\n",
    "*Reference from [Vincent Sitzman](https://github.com/vsitzmann/awesome-implicit-representations#what-are-implicit-neural-representations)*\n",
    "\n",
    "---\n",
    "\n",
    "#### **Similar Fields related to INR**\n",
    "\n",
    "***Implicit neural representations for high frequency data***\n",
    "\n",
    "*To encode potentially high frequency data such as sound or images, it is much more efficient to start from periodic feature transformations. This can be achieved with periodic activation functions [(Sitzmann et al. 2020)](https://arxiv.org/abs/2006.09661) or by using a Fourier feature mapping [(Tancik et al. 2020)](https://arxiv.org/abs/2006.10739).*\n",
    "\n",
    "***Neural radiance fields***\n",
    "\n",
    "[(Mildenhall et al. 2020)](https://arxiv.org/abs/2003.08934)\n",
    "\n",
    "*Reference From [Hugo Cisneros](https://hugocisneros.com/notes/implicit_neural_representations/#org36a6837)*\n",
    "\n",
    "---\n",
    "\n",
    "#### **Here's my thought.**\n",
    "\n",
    "***The Implicit Neural Representation*** *is a full kinds of* ***mapping function***, *that identifying a specific pattern from existing phenomena and maps to an unspecified object.*\n",
    "\n",
    "*It contains various models of Deep Learning fields, especially the Generative Adversarial Networks are frequently used to construct the function nowadays, previously many approaches suggested such as statistical methods since its purpose is encoding the potential frequency as much as possible from the original phenomenon that can appear.*\n",
    "\n",
    "*One of the difference between* ***Traditional Explicit Representations*** *and* ***Implicit Neural Representation*** *is whether* ***discrete*** *or* ***contionus***, *which its property of 'continous' better represents the kind of mapping function, a distribution sampled from the existing distribution.*\n",
    "\n",
    "*The Deep Learning, which contains the huge amount of* ***techiniques to map the specific function*** *including MLP, CNN, Encoder-Decoder architecture are* ***can be seen in the same context.***\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **3. The applications and potentials of INR**\n",
    "\n",
    "There are huge usages and applicable stuffs with the Implicit Neural Representation. \n",
    "**Due to the property of Implicit Encoding that mapping the meaningful new distribution from the actual space**, It can be applicable to various domains through implicit encoding and has the advantage of being able to largerly expandable from a specific domain to various scales.\n",
    "\n",
    "One of the most applicable field with INR is **Image to Image Translation**. Image-to-image translation is called 'I2I' as a abbreviation, the I2I is a class of vision and graphics problems where the goal is to learn the mapping between an input image and an output image. It can be applied to a wide range of applications, such as collection style transfer, object transfiguration, season transfer and photo enhancement. Briefly, Image to Image Translation takes a task of taking images from one domain and transforming them so they have the style (or characteristics) of images from another domain. Here's some example.\n",
    "\n",
    "#### **Image to Image Translation**\n",
    "\n",
    "Numerous image processing problems are already being solved using convolutional neural nets (CNNs). However, it is still difficult to design effective losses when learning CNNs. For example, simply learning CNN in the direction of reducing the distance between the target and the prediction result, the model outputs a blurred image.\n",
    "\n",
    "To solve this problem, General Adversarial Networks (GANs) are used a lot in image processing problems. GANs competitively learn generative models and discriminant models. The generated model is trained so that the discriminant model cannot distinguish between the generated image and the actual image, and the discriminant model is trained to distinguish the generated image from the actual image. Through this learning process, the generated model can create a similar image to the real thing.\n",
    "\n",
    "##### **Conditional GAN (Pix2Pix)**\n",
    "\n",
    "<table>\n",
    "    <tbody>\n",
    "        <tr>\n",
    "            <td width=\"300\">\n",
    "                <img src=\"https://www.secmem.org/assets/images/pix2pix/pix2pix_images.jpg\" width=\"300\" />\n",
    "            </td>\n",
    "            <td>\n",
    "                <strong>Image-to-Image Translation with Conditional Adversarial Nets (CVPR 2017):</strong>\n",
    "                 This paper first defined image to image translation, a problem dealing with interdomain changes in images. This transformation includes coloring black and white photos and drawing objects when outlined. Previously, different models were used for each conversion problem, but the paper showed that their Pix2Pix model solves most conversion problems well.\n",
    "            </td>\n",
    "        </tr>\n",
    "        <tr>\n",
    "            <td width=\"300\">\n",
    "                <img src=\"https://learnopencv.com/wp-content/uploads/2021/07/pix-2-pix-GAN-a-type-of-CGAN.jpg\" width=\"300\" />\n",
    "            </td>\n",
    "            <td>\n",
    "                In the paper, conditional GANs (cGANs) that can include conditions were used. When converting an image, if you learn by putting an image in the generation model and the discrimination model as a condition, the generation model can generate a result image according to the input image.\n",
    "            </td>\n",
    "        </tr>\n",
    "    </tbody>\n",
    "</table>\n",
    "\n",
    "##### **Goal**\n",
    "\n",
    "<table>\n",
    "    <tbody>\n",
    "        <tr>\n",
    "            <td width=\"300\">\n",
    "                <img src=\"imgs/equation1.png\" width=\"300\" />\n",
    "            </td>\n",
    "            <td>\n",
    "                Let's call G, D the generation model and the discrimination model. \n",
    "                Let's call x, y, z for each input image, output image, and noise. The goal of cGAN is to minimize the cGAN loss defined as follows. Through this process, G learns mapping from input image x and noise z to output image y.\n",
    "            </td>\n",
    "        </tr>\n",
    "        <tr>\n",
    "            <td width=\"300\">\n",
    "                <img src=\"imgs/equation2.png\" width=\"300\" />\n",
    "            </td>\n",
    "            <td>\n",
    "                In addition to these GAN losses, the paper added traditional losses such as L1 and L2 distance. At this time, the objective function of the discriminant model remains unchanged, but the generated model further narrows the distance between the output value and the target image. Here, Pix2Pix used L1 distance instead of L2 to prevent blurring.\n",
    "            </td>\n",
    "        </tr>\n",
    "        <tr>\n",
    "            <td width=\"300\">\n",
    "                <img src=\"imgs/equation3.png\" width=\"300\" />\n",
    "            </td>\n",
    "            <td>\n",
    "                Then, the final goal is to find the generation model G as follows. Even without z in the generative model, mapping from x to y can be learned. However, the generating model will then output only one resulting image for a given input image. Therefore, past cGANs gave Gaussian distribution noise z as input of the generation model with x. In the author's experiment, the phenomenon of learning occurred when the given noise was ignored through this method, but it was not possible to find a way to solve it effectively. So Pix2Pix was noisy only through dropout and used for both learning and testing.\n",
    "            </td>\n",
    "        </tr>\n",
    "    </tbody>\n",
    "</table>\n",
    "\n",
    "\n",
    "Reference: [Pix2Pix Image to Image translation](https://www.secmem.org/blog/2020/07/19/pix2pix/)\n",
    "\n",
    "##### **CycleGAN**\n",
    "\n",
    "<table>\n",
    "    <tbody>\n",
    "        <tr>\n",
    "            <td width=\"300\">\n",
    "                <img src=\"https://miro.medium.com/max/1050/0*Udvw6tGu40iDEkuH.jpg\" width=\"300\" />\n",
    "            </td>\n",
    "            <td>\n",
    "                <i><strong>Unpaired Image-to-Image Translation Using Cycle-Consistent Adversarial Networks(ICCV 2017)</strong></i>\n",
    "                Author presents an approach for learning to translate an image from a source domain X to a target domain Y in the absence of paired examples. The goal is to learn a mapping G : X → Y such that the distribution of images from G(X) is indistinguishable from the distribution Y using an adversarial loss. Because this mapping is highly under-constrained, we couple it with an inverse mapping F : Y → X and introduce a cycle consistency loss to enforce F(G(X)) ≈ X (and vice versa).\n",
    "            </td>\n",
    "        </tr>\n",
    "        <tr>\n",
    "            <td width=\"300\">\n",
    "                <img src=\"https://miro.medium.com/max/975/0*P-46iNsLcF2edVfn.png\" width=\"300\" />\n",
    "            </td>\n",
    "            <td>\n",
    "               Paired training data (left) consists of training examples have one to one correspondence. Unpaired training set has no such correspondence.\n",
    "            </td>\n",
    "        </tr>\n",
    "        <tr>\n",
    "            <td width=\"300\">\n",
    "                <img src=\"https://miro.medium.com/max/1050/0*KXiC6nIcowYS5GtA.png\" width=\"300\" />\n",
    "            </td>\n",
    "            <td>\n",
    "                The model contains two mapping functions G : X → Y and F : Y → X , and associated adversarial discriminators DY and DX . DY encourages G to translate X into outputs indistinguishable from domain Y , and vice versa for DX , F , and X . To further regularize the mappings, they introduce two “cycle consistency losses” that capture the intuition that if we translate from one domain to the other and back again we should arrive where we started.\n",
    "            </td>\n",
    "        </tr>\n",
    "    </tbody>\n",
    "</table>\n",
    "\n",
    "##### **StarGAN**\n",
    "\n",
    "<table>\n",
    "    <tbody>\n",
    "        <tr>\n",
    "            <td width=\"300\">\n",
    "                <img src=\"https://miro.medium.com/max/1050/0*bPadlkT94xTpp2Om.jpg\" width=\"300\" />\n",
    "            </td>\n",
    "            <td>\n",
    "                <strong>Unified Generative Adversarial Networks for Multi-Domain Image-to-Image Translation(CVPR 2018):</strong>\n",
    "                Existing image to image translation approaches have limited scalability and robustness in handling more than two domains, since different models should be built independently for every pair of image domains. StarGAN is a novel and scalable approach that can perform image-to-image translations for multiple domains using only a single model.\n",
    "            </td>\n",
    "        </tr>\n",
    "        <tr>\n",
    "            <td width=\"300\">\n",
    "                <img src=\"https://miro.medium.com/max/972/0*S7N84-uT_6zqrhxl.png\" width=\"300\" />\n",
    "            </td>\n",
    "            <td>\n",
    "                Comparison between cross-domain models and our proposed model, StarGAN. (a) To handle multiple domains, crossdomain models should be built for every pair of image domains. (b) StarGAN is capable of learning mappings among multiple domains using a single generator. The figure represents a star topology connecting multi-domains.\n",
    "            </td>\n",
    "        </tr>\n",
    "        <tr>\n",
    "            <td width=\"300\">\n",
    "                <img src=\"https://miro.medium.com/max/1050/0*9ICw3yzQ9qWnPBLK.png\" width=\"300\" />\n",
    "            </td>\n",
    "            <td>\n",
    "                Overview of StarGAN, consisting of two modules, a discriminator D and a generator G. (a) D learns to distinguish between real and fake images and classify the real images to its corresponding domain. (b) G takes in as input both the image and target domain label and generates an fake image. The target domain label is spatially replicated and concatenated with the input image. (c) G tries to reconstruct the original image from the fake image given the original domain label. (d) G tries to generate images indistinguishable from real images and classifiable as target domain by D.\n",
    "            </td>\n",
    "        </tr>\n",
    "    </tbody>\n",
    "</table>\n",
    "\n",
    "Reference: [Image-to-Image Translation](https://towardsdatascience.com/image-to-image-translation-69c10c18f6ff)\n",
    "\n",
    "#### **Image generation**\n",
    "\n",
    "##### **Style GAN**\n",
    "\n",
    "<table>\n",
    "    <tbody>\n",
    "        <tr>\n",
    "            <td width=\"300\">\n",
    "                <img src=\"https://raw.githubusercontent.com/happy-jihye/happy-jihye.github.io/master/_posts/images/gan/stylegan-1.gif\" width=\"300\" />\n",
    "            </td>\n",
    "            <td>\n",
    "                <strong>A Style-Based Generator Architecture for Generative Adversarial Networks (StyleGAN) [CVPR 2019]:</strong>\n",
    "                This model is a model that develops the architecture of the generator to better learn the style without touching the descriptor or loss function. Image synthesis was developed to style scale-specific control by modifying the existing PGGAN (ProGAN) model. As in the picture on the left, it is style scale-specific control that can scale a specific style.\n",
    "            </td>\n",
    "        </tr>\n",
    "        <tr>\n",
    "            <td width=\"300\">\n",
    "                <img src=\"https://user-images.githubusercontent.com/37301677/84801592-55112480-b03a-11ea-96ab-7df7d933430b.png\" width=\"300\" />\n",
    "            </td>\n",
    "            <td>\n",
    "                Learned by covering each layer with style information in the Generator network. The style of the image (gender, pose). You can change your hair color, skin tone, etc.). Latent vectors tend to follow the distribution of specific datasets as they are, and to compensate for this, they do not use them as they are, but use the wvector created by creating the Mapping network to change their styles more diversely.\n",
    "            </td>\n",
    "        </tr>\n",
    "        <tr>\n",
    "            <td width=\"300\">\n",
    "                <img src=\"https://raw.githubusercontent.com/happy-jihye/happy-jihye.github.io/master/_posts/images/gan/stylegan-9.PNG\" width=\"300\" />\n",
    "            </td>\n",
    "            <td>\n",
    "                Style mixing is mixing the style as the word suggests. After sampling (z1,z2), two style codes (w1,w2) are created through a mapping network. After setting a certain reference point, two styles are added to mix the styles. When mixing regularization is performed in this way, the styles are not correlated.\n",
    "            </td>\n",
    "        </tr>\n",
    "    </tbody>\n",
    "</table>\n",
    "\n",
    "Reference: [Style Gan](https://happy-jihye.github.io/gan/gan-6/), [Gan Guide](https://ysbsb.github.io/gan/2020/06/17/GAN-newbie-guide.html)\n",
    "\n",
    "#### **3D reconstruction**\n",
    "\n",
    "#### **Neural Radiance Field**\n",
    "\n",
    "nerf\n",
    "\n",
    "https://miro.medium.com/max/960/1*Wg-kFTJ8FEQGqO5uIudEJg.gif\n",
    "\n",
    "here, style transfer\n",
    "\n",
    "#### **Style Transfer**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **4. About Style Transfer**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **5. What Style Transfer ultimately purpose**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **6. Paper review for basic understanding for Style Transfer via INR**\n",
    "\n",
    "##### **1. Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift**\n",
    "> **Authors: Sergey Ioffe, Christian Szegedy - Google Inc., {sioffe, szegedy} @google.com**\n",
    "\n",
    "> **Original Paper**: https://arxiv.org/pdf/1502.03167.pdf\n",
    "\n",
    "> **Un official Github**: https://github.com/shuuki4/Batch-Normalization\n",
    "\n",
    "##### **2. Instance Normalization: The Missing Ingredient for Fast Stylization**\n",
    "\n",
    "> **Authors: Dmitry Ulyanov [dmitry.ulyanov@skoltech.ru], Andrea Vedaldi [vedaldi@robots.ox.ac.uk], Victor Lempitsky [lempitsky@skoltech.ru]**\n",
    "\n",
    "> **Original Paper**: https://arxiv.org/pdf/1607.08022.pdf\n",
    "\n",
    "> **Official Github**: https://github.com/DmitryUlyanov/texture_nets\n",
    "\n",
    "##### 3. **Arbitrary Style Transfer in Real-time with Adaptive Instance Normalization**\n",
    "\n",
    "> **Authors: Xun Huang, Serge Belongie - Department of Computer Science & Cornell Tech, Cornell University, {xh258,sjb344}@cornell.edu**\n",
    "\n",
    "> **Official Github**: https://github.com/xunhuang1995/AdaIN-style\n",
    "\n",
    "> **Original Paper**: https://arxiv.org/pdf/1703.06868.pdf\n",
    "\n",
    "##### **4. Semantic Image Synthesis with Spatially-Adaptive Normalization**\n",
    "\n",
    "> **Authors: Taesung Park, Ming-Yu Liu, Ting-Chun Wang, Jun-Yan ZhuL**\n",
    "\n",
    "> **Original Paper**: https://arxiv.org/pdf/1903.07291.pdf\n",
    "\n",
    "> **Official Github**: https://github.com/NVlabs/SPADE\n",
    "\n",
    "##### **5. Universal Style Transfer via Feature Transforms**\n",
    "\n",
    "> **Authors: Yijun Li\n",
    "(UC Merced -\n",
    "yli62@ucmerced.edu),\n",
    "Chen Fang\n",
    "(Adobe Research - \n",
    "cfang@adobe.com)\n",
    "Jimei Yang\n",
    "(Adobe Research -\n",
    "jimyang@adobe.com)\n",
    "Zhaowen Wang\n",
    "(Adobe Research - \n",
    "zhawang@adobe.com)\n",
    "Xin Lu\n",
    "(Adobe Research - \n",
    "xinl@adobe.com)\n",
    "Ming-Hsuan Yang\n",
    "(UC Merced, NVIDIA Research - \n",
    "mhyang@ucmerced.edu)**\n",
    "\n",
    "> **Original Paper**: https://arxiv.org/pdf/1705.08086.pdf\n",
    "\n",
    "> **Official Github**: https://github.com/Yijunmaverick/UniversalStyleTransfer\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Why Internal Covariance Shift Matters?**\n",
    "\n",
    "At the first when I encounter the *'Normalization'* term in INR (Implicit Neural Representation) Field, I thought it is just a one of the common technique(at least the current point of Jan, 2022) to enhance the deep learning models' performance.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **1. Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift**\n",
    "\n",
    "**Overview**\n",
    "\n",
    "The paper suggest the three main things that enhance the neural networks' performance by using the one of the technique called 'Batch Normalization', and the suggested benefits are below.\n",
    "\n",
    "1. Allow to use higher learning rate\n",
    "2. Allow to be less careful about initializations\n",
    "3. Acting as a regularizer, in some cases eliminating the needs of Dropout\n",
    "4. When using a nonlinear function (such as sigmoid), it prevents falling into the saturated region (where the slope is zero). Therefore, it prevents slope loss and saturation problem.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Presented Previous Problem**\n",
    "\n",
    "With using SGD(Mini-Batch Stochastic Gradient Descent), it is good algorithm to train the network, however there are several problems.\n",
    "\n",
    "1. It requires careful tuning of the model **hyper-parameters**, specifically the ***learning rate*** used in optimization, as well as the inital values for the ***model parameters***.\n",
    "\n",
    "2. The change in the distributions of layers' input presents a problem because the layers need to continuosly adapt to the new distribution.\n",
    "\n",
    "*When the input distribution to a learning system changes, it is is said to experience* ***Covariance Shift***.\n",
    "\n",
    "> This is typically handled via domain adaption. However, the notion of covariate shift can be extended beyond the learning system as a whole, to apply its parts, such as a sub-network or a layer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Solution**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Method**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Contribution**"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "3efb3a3e09a38f93b1d9bfa4b9b8ce66a167f6ada77b64bc003d7d6d298d81d5"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 64-bit ('buddhalight': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
