{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **About INR and Style Transfer**\n",
    "\n",
    "### **The ulitmate purpose of style transfer via INR, with introducing its background and papers**\n",
    "\n",
    "**Edited By Su Hyung Choi (Key Summary & Code Practice)**\n",
    "\n",
    "If you have any issues on this scripts, please PR to the repository below.\n",
    "\n",
    "**[Github: @JonyChoi - Computer Vision Paper Reviews]** https://github.com/jonychoi/Computer-Vision-Paper-Reviews\n",
    "\n",
    "Edited Jan 16 2022\n",
    "\n",
    "---\n",
    "\n",
    "Here, I wrote the writing contains about the *\"what are Implicit Neural Representations'*, the basic concept of INR, and its potential usage and applications. With the understanding of INR, I introduced the its representative papers within few couples of years to apprehend its trend and evolution history."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Overview**\n",
    "\n",
    "**1. The Goal of the Writing**\n",
    "\n",
    "**2. What are Implicit Neural Representations**\n",
    "\n",
    "**3. The applications and potentials of INR**\n",
    "\n",
    "**4. About Style Transfer**\n",
    "\n",
    "**5. What Style Transfer ultimately purpose**\n",
    "\n",
    "**6. Paper review for basic understanding for Style Transfer via INR**\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **1. The Goal of the Writing**\n",
    "\n",
    "Here, as a beginner of *Style Transfer* within *INR(Implicit Neural Representation)* field, I'm going to introduce what are implict neural representations and style transfer with its *basic concept* and *the usages with the potential applicablity*, by *reviewing the core papers* that enhance understanding of Style Transfer within INR field.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **2. What are Implicit Neural Representations?**\n",
    "\n",
    "---\n",
    "\n",
    "#### **Implicit neural representations**\n",
    "\n",
    "<img src=\"./imgs/example7.png\" width=\"500\" />\n",
    "\n",
    "Picture by [Implicit Neural Scene Representations](https://www.youtube.com/watch?v=Or9J-DCDGko)\n",
    "\n",
    "***\"Implicit neural representations is about parameterizing a continuous differentiable signal with a neural network. The signal is encoded within the neural network, providing a possibly more compact representation.*** *This is a type of [regression](https://hugocisneros.com/notes/machine_learning/) problem. Applications of these learned representations range from simple [compression](https://hugocisneros.com/notes/compression/), to 3D scene reconstruction from 2D images, semantic information inference, etc.[CPPN](https://hugocisneros.com/notes/cppn/) is an early example of a implicit neural representation implementation mainly used for pattern generation. It uses a neural network to generate patterns parameterized by two (or more) coordinates.\"*\n",
    "\n",
    "***by [Hugo Cisneros](https://hugocisneros.com/notes/implicit_neural_representations/#org36a6837)***\n",
    "\n",
    "***\"Implicit Neural Representations (sometimes also referred to as coordinate-based representations) are a novel way to parameterize signals of all kinds.*** Conventional signal representations are usually discrete - *for instance, images are discrete grids of pixels, audio signals are discrete samples of amplitudes, and 3D shapes are usually parameterized as grids of voxels, point clouds, or meshes. In contrast, Implicit Neural Representations parameterize a signal as a ***continuous function*** that maps the domain of the signal (i.e., a coordinate, such as a pixel coordinate for an image) to whatever is at that coordinate (for an image, an R,G,B color). Of course, these functions are usually not analytically tractable - it is impossible to \"write down\" the function that parameterizes a natural image as a mathematical formula. Implicit Neural Representations thus approximate that function via a neural network.\"*\n",
    "\n",
    "***by [Vincent Sitzman](https://github.com/vsitzmann/awesome-implicit-representations#what-are-implicit-neural-representations)***\n",
    "\n",
    "---\n",
    "\n",
    "#### **Why are they interesting?**\n",
    "\n",
    "Implicit Neural Representations have several benefits: First, they are not coupled to spatial resolution anymore, the way, for instance, an image is coupled to the number of pixels. This is because they are continuous functions! Thus, the memory required to parameterize the signal is independent of spatial resolution, and only scales with the complexity of the underyling signal. Another corollary of this is that implicit representations have \"infinite resolution\" - they can be sampled at arbitrary spatial resolutions.\n",
    "\n",
    "This is immediately useful for a number of applications, such as super-resolution, or in parameterizing signals in 3D and higher dimensions, where memory requirements grow intractably fast with spatial resolution. Further, generalizing across neural implicit representations amounts to learning a prior over a space of functions, implemented via learning a prior over the weights of neural networks - this is commonly referred to as meta-learning and is an extremely exciting intersection of two very active research areas! Another exciting overlap is between neural implicit representations and the study of symmetries in neural network architectures - for intance, creating a neural network architecture that is 3D rotation-equivariant immediately yields a viable path to rotation-equivariant generative models via neural implicit representations.\n",
    "\n",
    "Another key promise of implicit neural representations lie in algorithms that directly operate in the space of these representations. In other words: What's the \"convolutional neural network\" equivalent of a neural network operating on images represented by implicit representations?\n",
    "\n",
    "*Reference from [Vincent Sitzman](https://github.com/vsitzmann/awesome-implicit-representations#what-are-implicit-neural-representations)*\n",
    "\n",
    "---\n",
    "\n",
    "#### **Similar Fields related to INR**\n",
    "\n",
    "***Implicit neural representations for high frequency data***\n",
    "\n",
    "*To encode potentially high frequency data such as sound or images, it is much more efficient to start from periodic feature transformations. This can be achieved with periodic activation functions [(Sitzmann et al. 2020)](https://arxiv.org/abs/2006.09661) or by using a Fourier feature mapping [(Tancik et al. 2020)](https://arxiv.org/abs/2006.10739).*\n",
    "\n",
    "***Neural radiance fields***\n",
    "\n",
    "[(Mildenhall et al. 2020)](https://arxiv.org/abs/2003.08934)\n",
    "\n",
    "*Reference From [Hugo Cisneros](https://hugocisneros.com/notes/implicit_neural_representations/#org36a6837)*\n",
    "\n",
    "---\n",
    "\n",
    "#### **Here's my thought.**\n",
    "\n",
    "***The Implicit Neural Representation*** *is a full kinds of* ***mapping function***, *that identifying a specific pattern from existing phenomena and maps to an unspecified object.*\n",
    "\n",
    "*It contains various models of Deep Learning fields, especially the Generative Adversarial Networks are frequently used to construct the function nowadays, previously many approaches suggested such as statistical methods since its purpose is encoding the potential frequency as much as possible from the original phenomenon that can appear.*\n",
    "\n",
    "*One of the difference between* ***Traditional Explicit Representations*** *and* ***Implicit Neural Representation*** *is whether* ***discrete*** *or* ***contionus***, *which its property of 'continous' better represents the kind of mapping function, a distribution sampled from the existing distribution.*\n",
    "\n",
    "*The Deep Learning, which contains the huge amount of* ***techiniques to map the specific function*** *including MLP, CNN, Encoder-Decoder architecture are* ***can be seen in the same context.***\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **3. The applications and potentials of INR**\n",
    "\n",
    "There are huge usages and applicable stuffs with the Implicit Neural Representation. \n",
    "**Due to the property of Implicit Encoding that mapping the meaningful new distribution from the actual space**, It can be applicable to various domains through implicit encoding and has the advantage of being able to largerly expandable from a specific domain to various scales.\n",
    "\n",
    "One of the most applicable field with INR is **Image to Image Translation**. Image-to-image translation is called 'I2I' as a abbreviation, the I2I is a class of vision and graphics problems where the goal is to learn the mapping between an input image and an output image. It can be applied to a wide range of applications, such as collection style transfer, object transfiguration, season transfer and photo enhancement. Briefly, Image to Image Translation takes a task of taking images from one domain and transforming them so they have the style (or characteristics) of images from another domain. Here's some example.\n",
    "\n",
    "---\n",
    "\n",
    "### **Image to Image Translation**\n",
    "\n",
    "Numerous image processing problems are already being solved using convolutional neural nets (CNNs). However, it is still difficult to design effective losses when learning CNNs. For example, simply learning CNN in the direction of reducing the distance between the target and the prediction result, the model outputs a blurred image.\n",
    "\n",
    "To solve this problem, General Adversarial Networks (GANs) are used a lot in image processing problems. GANs competitively learn generative models and discriminant models. The generated model is trained so that the discriminant model cannot distinguish between the generated image and the actual image, and the discriminant model is trained to distinguish the generated image from the actual image. Through this learning process, the generated model can create a similar image to the real thing.\n",
    "\n",
    "##### **Conditional GAN (Pix2Pix)**\n",
    "\n",
    "<table>\n",
    "    <tbody>\n",
    "        <tr>\n",
    "            <td width=\"300\">\n",
    "                <img src=\"https://www.secmem.org/assets/images/pix2pix/pix2pix_images.jpg\" width=\"300\" />\n",
    "            </td>\n",
    "            <td>\n",
    "                <strong>Image-to-Image Translation with Conditional Adversarial Nets (CVPR 2017):</strong>\n",
    "                 This paper first defined image to image translation, a problem dealing with interdomain changes in images. This transformation includes coloring black and white photos and drawing objects when outlined. Previously, different models were used for each conversion problem, but the paper showed that their Pix2Pix model solves most conversion problems well.\n",
    "            </td>\n",
    "        </tr>\n",
    "        <tr>\n",
    "            <td width=\"300\">\n",
    "                <img src=\"https://learnopencv.com/wp-content/uploads/2021/07/pix-2-pix-GAN-a-type-of-CGAN.jpg\" width=\"300\" />\n",
    "            </td>\n",
    "            <td>\n",
    "                In the paper, conditional GANs (cGANs) that can include conditions were used. When converting an image, if you learn by putting an image in the generation model and the discrimination model as a condition, the generation model can generate a result image according to the input image.\n",
    "            </td>\n",
    "        </tr>\n",
    "    </tbody>\n",
    "</table>\n",
    "\n",
    "##### **Goal**\n",
    "\n",
    "<table>\n",
    "    <tbody>\n",
    "        <tr>\n",
    "            <td width=\"300\">\n",
    "                <img src=\"imgs/equation1.png\" width=\"300\" />\n",
    "            </td>\n",
    "            <td>\n",
    "                Let's call G, D the generation model and the discrimination model. \n",
    "                Let's call x, y, z for each input image, output image, and noise. The goal of cGAN is to minimize the cGAN loss defined as follows. Through this process, G learns mapping from input image x and noise z to output image y.\n",
    "            </td>\n",
    "        </tr>\n",
    "        <tr>\n",
    "            <td width=\"300\">\n",
    "                <img src=\"imgs/equation2.png\" width=\"300\" />\n",
    "            </td>\n",
    "            <td>\n",
    "                In addition to these GAN losses, the paper added traditional losses such as L1 and L2 distance. At this time, the objective function of the discriminant model remains unchanged, but the generated model further narrows the distance between the output value and the target image. Here, Pix2Pix used L1 distance instead of L2 to prevent blurring.\n",
    "            </td>\n",
    "        </tr>\n",
    "        <tr>\n",
    "            <td width=\"300\">\n",
    "                <img src=\"imgs/equation3.png\" width=\"300\" />\n",
    "            </td>\n",
    "            <td>\n",
    "                Then, the final goal is to find the generation model G as follows. Even without z in the generative model, mapping from x to y can be learned. However, the generating model will then output only one resulting image for a given input image. Therefore, past cGANs gave Gaussian distribution noise z as input of the generation model with x. In the author's experiment, the phenomenon of learning occurred when the given noise was ignored through this method, but it was not possible to find a way to solve it effectively. So Pix2Pix was noisy only through dropout and used for both learning and testing.\n",
    "            </td>\n",
    "        </tr>\n",
    "    </tbody>\n",
    "</table>\n",
    "\n",
    "\n",
    "Reference: [Pix2Pix Image to Image translation](https://www.secmem.org/blog/2020/07/19/pix2pix/)\n",
    "\n",
    "##### **CycleGAN**\n",
    "\n",
    "<table>\n",
    "    <tbody>\n",
    "        <tr>\n",
    "            <td width=\"300\">\n",
    "                <img src=\"https://miro.medium.com/max/1050/0*Udvw6tGu40iDEkuH.jpg\" width=\"300\" />\n",
    "            </td>\n",
    "            <td>\n",
    "                <i><strong>Unpaired Image-to-Image Translation Using Cycle-Consistent Adversarial Networks(ICCV 2017)</strong></i>\n",
    "                Author presents an approach for learning to translate an image from a source domain X to a target domain Y in the absence of paired examples. The goal is to learn a mapping G : X → Y such that the distribution of images from G(X) is indistinguishable from the distribution Y using an adversarial loss. Because this mapping is highly under-constrained, we couple it with an inverse mapping F : Y → X and introduce a cycle consistency loss to enforce F(G(X)) ≈ X (and vice versa).\n",
    "            </td>\n",
    "        </tr>\n",
    "        <tr>\n",
    "            <td width=\"300\">\n",
    "                <img src=\"https://miro.medium.com/max/975/0*P-46iNsLcF2edVfn.png\" width=\"300\" />\n",
    "            </td>\n",
    "            <td>\n",
    "               Paired training data (left) consists of training examples have one to one correspondence. Unpaired training set has no such correspondence.\n",
    "            </td>\n",
    "        </tr>\n",
    "        <tr>\n",
    "            <td width=\"300\">\n",
    "                <img src=\"https://sp-ao.shortpixel.ai/client/to_webp,q_glossy,ret_img/https://godatadriven.com/wp-content/images/how-to-style-transfer/ccan.png\" width=\"300\" />\n",
    "            </td>\n",
    "            <td>\n",
    "                The model contains two mapping functions G : X → Y and F : Y → X , and associated adversarial discriminators DY and DX . DY encourages G to translate X into outputs indistinguishable from domain Y , and vice versa for DX , F , and X . To further regularize the mappings, they introduce two “cycle consistency losses” that capture the intuition that if we translate from one domain to the other and back again we should arrive where we started.\n",
    "            </td>\n",
    "        </tr>\n",
    "    </tbody>\n",
    "</table>\n",
    "\n",
    "##### **StarGAN**\n",
    "\n",
    "<table>\n",
    "    <tbody>\n",
    "        <tr>\n",
    "            <td width=\"300\">\n",
    "                <img src=\"https://miro.medium.com/max/1050/0*bPadlkT94xTpp2Om.jpg\" width=\"300\" />\n",
    "            </td>\n",
    "            <td>\n",
    "                <strong>Unified Generative Adversarial Networks for Multi-Domain Image-to-Image Translation(CVPR 2018):</strong>\n",
    "                Existing image to image translation approaches have limited scalability and robustness in handling more than two domains, since different models should be built independently for every pair of image domains. StarGAN is a novel and scalable approach that can perform image-to-image translations for multiple domains using only a single model.\n",
    "            </td>\n",
    "        </tr>\n",
    "        <tr>\n",
    "            <td width=\"300\">\n",
    "                <img src=\"https://miro.medium.com/max/972/0*S7N84-uT_6zqrhxl.png\" width=\"300\" />\n",
    "            </td>\n",
    "            <td>\n",
    "                Comparison between cross-domain models and our proposed model, StarGAN. (a) To handle multiple domains, crossdomain models should be built for every pair of image domains. (b) StarGAN is capable of learning mappings among multiple domains using a single generator. The figure represents a star topology connecting multi-domains.\n",
    "            </td>\n",
    "        </tr>\n",
    "        <tr>\n",
    "            <td width=\"300\">\n",
    "                <img src=\"https://miro.medium.com/max/1050/0*9ICw3yzQ9qWnPBLK.png\" width=\"300\" />\n",
    "            </td>\n",
    "            <td>\n",
    "                Overview of StarGAN, consisting of two modules, a discriminator D and a generator G. (a) D learns to distinguish between real and fake images and classify the real images to its corresponding domain. (b) G takes in as input both the image and target domain label and generates an fake image. The target domain label is spatially replicated and concatenated with the input image. (c) G tries to reconstruct the original image from the fake image given the original domain label. (d) G tries to generate images indistinguishable from real images and classifiable as target domain by D.\n",
    "            </td>\n",
    "        </tr>\n",
    "    </tbody>\n",
    "</table>\n",
    "\n",
    "Reference: [Image-to-Image Translation](https://towardsdatascience.com/image-to-image-translation-69c10c18f6ff)\n",
    "\n",
    "---\n",
    "\n",
    "### **Image generation**\n",
    "\n",
    "##### **Style GAN**\n",
    "\n",
    "<table>\n",
    "    <tbody>\n",
    "        <tr>\n",
    "            <td width=\"300\">\n",
    "                <img src=\"https://raw.githubusercontent.com/happy-jihye/happy-jihye.github.io/master/_posts/images/gan/stylegan-1.gif\" width=\"300\" />\n",
    "            </td>\n",
    "            <td>\n",
    "                <strong>A Style-Based Generator Architecture for Generative Adversarial Networks (StyleGAN) [CVPR 2019]:</strong>\n",
    "                This model is a model that develops the architecture of the generator to better learn the style without touching the descriptor or loss function. Image synthesis was developed to style scale-specific control by modifying the existing PGGAN (ProGAN) model. As in the picture on the left, it is style scale-specific control that can scale a specific style.\n",
    "            </td>\n",
    "        </tr>\n",
    "        <tr>\n",
    "            <td width=\"300\">\n",
    "                <img src=\"https://user-images.githubusercontent.com/37301677/84801592-55112480-b03a-11ea-96ab-7df7d933430b.png\" width=\"300\" />\n",
    "            </td>\n",
    "            <td>\n",
    "                Learned by covering each layer with style information in the Generator network. The style of the image (gender, pose). You can change your hair color, skin tone, etc.). Latent vectors tend to follow the distribution of specific datasets as they are, and to compensate for this, they do not use them as they are, but use the wvector created by creating the Mapping network to change their styles more diversely.\n",
    "            </td>\n",
    "        </tr>\n",
    "        <tr>\n",
    "            <td width=\"300\">\n",
    "                <img src=\"https://raw.githubusercontent.com/happy-jihye/happy-jihye.github.io/master/_posts/images/gan/stylegan-9.PNG\" width=\"300\" />\n",
    "            </td>\n",
    "            <td>\n",
    "                Style mixing is mixing the style as the word suggests. After sampling (z1,z2), two style codes (w1,w2) are created through a mapping network. After setting a certain reference point, two styles are added to mix the styles. When mixing regularization is performed in this way, the styles are not correlated.\n",
    "            </td>\n",
    "        </tr>\n",
    "    </tbody>\n",
    "</table>\n",
    "\n",
    "Reference: [Style Gan](https://happy-jihye.github.io/gan/gan-6/), [Gan Guide](https://ysbsb.github.io/gan/2020/06/17/GAN-newbie-guide.html)\n",
    "\n",
    "---\n",
    "\n",
    "### **Neural Rendering**\n",
    "\n",
    "To start with some definitions, the larger field of Neural rendering is defined by the excellent review paper by [Tewari et al](https://www.neuralrender.com/assets/downloads/TewariFriedThiesSitzmannEtAl_EG2020STAR.pdf). as\n",
    "\n",
    "> *\"deep image or video generation approaches that enable explicit or implicit control of scene properties such as illumination, camera parameters, pose, geometry, appearance, and semantic structure.\"*\n",
    "\n",
    "It is a novel, data-driven solution to the long-standing problem in computer graphics of the realistic rendering of virtual worlds.\n",
    "\n",
    "Neural volume rendering refers to methods that generate images or video by tracing a ray into the scene and taking an integral of some sort over the length of the ray. Typically a neural network like a multi-layer perceptron encodes a function from the 3D coordinates on the ray to quantities like density and color, which are integrated to yield an image.\n",
    "\n",
    "<table>\n",
    "    <tbody>\n",
    "        <tr>\n",
    "            <td width=\"300\">\n",
    "                <img src=\"./imgs/nerf.gif\" width=\"300\" />\n",
    "            </td>\n",
    "            <td>\n",
    "                <strong>NeRF: Representing Scenes as Neural Radiance Fields for View Synthesis [ECCV 2020].</strong>\n",
    "                The paper that got everyone talking was the Neural Radiance Fields or NeRF paper, with three first authors from Berkeley. In essence, they take the DeepSDF architecture but regress not a signed distance function, but density and color. They then use an (easily differentiable) numerical integration method to approximate a true volumetric rendering step. One of the reasons NeRF is able to render with great detail is because it encodes a 3D point and associated view direction on a ray using periodic activation functions, i.e., Fourier Features. This innovation was later generalized to multi-layer networks with periodic activations, aka SIREN (SInusoidal REpresentation Networks). Both were published later at NeurIPS 2020.\n",
    "            </td>\n",
    "        </tr>\n",
    "        <tr>\n",
    "            <td width=\"300\">\n",
    "                <img src=\"https://uploads-ssl.webflow.com/51e0d73d83d06baa7a00000f/5e700a025ff238947d682a1f_pipeline_website-03.svg\" width=\"300\" />\n",
    "            </td>\n",
    "            <td>\n",
    "                presents a method that achieves state-of-the-art results for synthesizing novel views of complex scenes by optimizing an underlying continuous volumetric scene function using a sparse set of input views. Their algorithm represents a scene using a fully-connected (non-convolutional) deep network, whose input is a single continuous 5D coordinate (spatial location (x, y, z) and viewing direction (θ, φ)) and whose output is the volume density and view-dependent emitted radiance at that spatial location.\n",
    "            </td>\n",
    "        </tr>\n",
    "        <tr>\n",
    "            <td width=\"300\">\n",
    "                <img src=\"https://uploads-ssl.webflow.com/51e0d73d83d06baa7a00000f/5e700ef6067b43821ed52768_pipeline_website-01-p-800.png\" width=\"300\" />\n",
    "            </td>\n",
    "            <td>\n",
    "                They synthesize views by querying 5D coordinates along camera rays and use classic volume rendering techniques to project the output colors and densities into an image. Because volume rendering is naturally differentiable, the only input required to optimize our representation is a set of images with known camera poses. We describe how to effectively optimize neural radiance fields to render photorealistic novel views of scenes with complicated geometry and appearance, and demonstrate results that outperform prior work on neural rendering and view synthesis.\n",
    "            </td>\n",
    "        </tr>\n",
    "    </tbody>\n",
    "</table>\n",
    "\n",
    "Reference: [NeRF](https://www.matthewtancik.com/nerf), [NeRF Explosion 2020](https://dellaert.github.io/NeRF/)\n",
    "\n",
    "---\n",
    "\n",
    "### **Style Transfer**\n",
    "\n",
    "The term \"style transfer\" is used to describe the operation of recomposing one image in the style of another (group of) image(s). In general, this requires two inputs: content image(s) and style image(s). The technique applied should then produce output whose “content” mirrors the content image and whose \"style\" resembles that of the style image.\n",
    "\n",
    "<table>\n",
    "    <tbody>\n",
    "        <tr>\n",
    "            <td width=\"300\">\n",
    "                <img src=\"https://sp-ao.shortpixel.ai/client/to_webp,q_glossy,ret_img/https://godatadriven.com/wp-content/images/how-to-style-transfer/style-transfer-example.jpg\" width=\"300\" />\n",
    "            </td>\n",
    "            <td>\n",
    "                Left an example with an original cat photo on the left (the content image), which is \"styled\" in three different ways using the style images in the middle. The different styles lead to different generated output images, as can be seen on the right.\n",
    "            </td>\n",
    "        </tr>\n",
    "        <tr>\n",
    "            <td width=\"300\">\n",
    "                <img src=\"https://sp-ao.shortpixel.ai/client/to_webp,q_glossy,ret_img/https://godatadriven.com/wp-content/images/how-to-style-transfer/ns-result.png\" width=\"300\" />\n",
    "            </td>\n",
    "            <td>\n",
    "                <strong>Neural style transfer</strong> was first demonstrated in August 2015 in a paper published by Gatys, Ecker, and Bethge at the University of Tübingen. The algorithm is described well on ml4a, a website by Gene Kogan that provides free educational resources about machine learning for artists.\n",
    "                In short, the objective of the style transfer algorithm is to generate an output image which minimizes a loss function that is the sum of two separate terms, a “content loss” and a “style loss.” The content loss represents the dissimilarity between the content image and the output image, whereas the style loss represents dissimilarity between the style image and the output image.\n",
    "                The algorithm trains a single convolutional neural network, wherein at each iteration the output image’s pixels are slightly adjusted so as to decrease the overall loss. We do this repeatedly until the loss converges, or until we are satisfied with our result.\n",
    "            </td>\n",
    "        </tr>\n",
    "        <tr>\n",
    "            <td width=\"300\">\n",
    "                <img src=\"https://sp-ao.shortpixel.ai/client/to_webp,q_glossy,ret_img/https://godatadriven.com/wp-content/images/how-to-style-transfer/style-transfer-explained.jpg\" width=\"300\" />\n",
    "            </td>\n",
    "            <td>\n",
    "                The content and style loss are defined differently:\n",
    "                <ul>\n",
    "                    <li>\n",
    "                        For the content loss the output image and content image are run through the convnet, giving us a set of feature maps for both. The loss at a single layer is then the euclidean (L2) distance between the activations of the content image and the activations of the output image.\n",
    "                    </li>\n",
    "                    <li>\n",
    "                        For the style loss we also use the convnet’s activations, but instead of comparing the raw activations directly, we take the <a href=\"https://towardsdatascience.com/neural-networks-intuitions-2-dot-product-gram-matrix-and-neural-style-transfer-5d39653e7916\">Gram matrix</a> of the activations at each layer in the network. This is a way to capture correlations between features in different parts of the image, which turns out to be a very good representation of our perception of style within images. The style loss at a single layer is then defined as the euclidean (L2) distance between the Gram matrices of the style and output images.\n",
    "                    </li>\n",
    "                </ul>\n",
    "            </td>\n",
    "        </tr>\n",
    "    </tbody>\n",
    "</table>\n",
    "\n",
    "Reference: [How to style transfer your own images](https://godatadriven.com/blog/how-to-style-transfer-your-own-images/)\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **4. About Style Transfer**\n",
    "\n",
    "\"Image Style Transfer\" is the problem is to take two images, extract content from one image, style(texture) from the other and seamlessly merge them together into one final image that looks realistic. Here, I'm going to explain the first attempt of Image Style Transfer, the paper called ['A Neural Algorithm of Artistic Style' by Gatys et. al'](https://arxiv.org/abs/1508.06576)\n",
    "\n",
    "<table>\n",
    "    <tbody>\n",
    "        <tr>\n",
    "            <td width=\"400\">\n",
    "                <img src=\"https://miro.medium.com/max/1034/1*-DKzmMajUn45Hu0pe_PUyQ.jpeg\" width=\"400\" />\n",
    "            </td>\n",
    "            <td>\n",
    "                <strong>Overview</strong>: Let me give a brief overview of the solution\n",
    "                <ol>\n",
    "                    <li>\n",
    "                    Create a random input image\n",
    "                    </li>\n",
    "                    <li>\n",
    "                    Pass the input through a pretrained backbone architecture say VGG, ResNet(note that this backbone will not be trained during backpropagation).\n",
    "                    </li>\n",
    "                    <li>\n",
    "                    Calculate loss and compute the gradients w.r.t input image pixels. Hence only the input pixels are adjusted whereas the weights remain constant.\n",
    "                    </li>\n",
    "                </ol>\n",
    "                The objective is to change the input image so that it represents the content and the style from the respective images.\n",
    "            </td>\n",
    "        </tr>\n",
    "    </tbody>\n",
    "</table>\n",
    "\n",
    "This problem consists of two sub problems: \n",
    "\n",
    "1. to generate the content\n",
    "\n",
    "2. to generate the style.\n",
    "\n",
    "**Problem — 1. Generate Content**\n",
    "\n",
    "The problem is to produce an image that contains a content as in the content image.\n",
    "\n",
    "> One point to note here is that the image should only contain the content(as in a rough sketch of the content image and not the texture from it, since the output should contain a style as that of the style image)\n",
    "\n",
    "**Solution**: The answer should be pretty straightforward. Use MSE loss(or any similarity measure such as SSIM, PSNR) between input and the target. But what is the target here? Well, if there were no constraints on the style of the output, then simply MSE between input and content image would be sufficient. So how to get an image’s content without copying its style?\n",
    "\n",
    "***\"Use feature maps.\"***\n",
    "\n",
    "Convolutional feature maps are generally a very good representation of input image’s features. They capture spatial information of an image without containing the style information(if a feature map is used as it is), which is what we need. And this is the reason we keep the backbone weights fixed during backpropagation.\n",
    "\n",
    "> Therefore, MSE loss between the input image’s features and content image’s features will work!\n",
    "\n",
    "<table>\n",
    "    <tbody>\n",
    "        <tr>\n",
    "            <td>\n",
    "                <img src=\"https://miro.medium.com/max/702/1*PKnjB3bxzgg6yy0uOsljqw.png\" width=\"400\" />\n",
    "            </td>\n",
    "        </tr>\n",
    "        <tr>\n",
    "            <td>\n",
    "                Content Loss\n",
    "            </td>\n",
    "        </tr>\n",
    "    </tbody>\n",
    "</table>\n",
    "\n",
    "\n",
    "**Problem — 2. Generate Style**\n",
    "\n",
    "The problem is to produce an image which contains the style as in the style image.\n",
    "\n",
    "**Solution**: To extract the style of an image(or more specifically to compute the style loss), we need something called as Gram matrix. Wait, what is a Gram matrix?\n",
    "Before talking about how to compute the style loss, let me talk about some math basics.\n",
    "\n",
    "\"Dot Product.\"\n",
    "\n",
    "A dot product of two vectors is the sum of products of respective coordinates.\n",
    "\n",
    "> So what does this mean?\n",
    "\n",
    "In a more intuitive way, dot product can be seen as how similar two vectors actually are. The more similar they are, the lesser the angle between them as in fig (a) or more closer the respective coordinates as in fig(b). In both the cases, the result is large. So the more similar they are, the larger the dot product gets.\n",
    "\n",
    "But what does this have to do with a neural network?\n",
    "\n",
    "> Consider two vectors(more specifically 2 flattened feature vectors from a convolutional feature map of depth C) representing features of the input space, and their dot product give us the information about the relation between them. The lesser the product the more different the learned features are and greater the product, the more correlated the features are. In other words, the lesser the product, the lesser the two features co-occur and the greater it is, the more they occur together. This in a sense gives information about an image’s style(texture) and zero information about its spatial structure, since we already flatten the feature and perform dot product on top of it.\n",
    "\n",
    "Now take all C feature vectors(flattened) from a convolutional feature map of depth C and compute the dot product with every one of them(including with a feature vector itself). The result is the Gram Matrix(of size CxC).\n",
    "\n",
    "<table>\n",
    "    <tbody>\n",
    "        <tr>\n",
    "            <td>\n",
    "                <img src=\"https://miro.medium.com/max/389/1*C3fkQanKHMwOi_rf0q0OQQ.png\" width=\"400\" />\n",
    "            </td>\n",
    "        </tr>\n",
    "        <tr>\n",
    "            <td>\n",
    "                Gram Matrix\n",
    "            </td>\n",
    "        </tr>\n",
    "    </tbody>\n",
    "</table>\n",
    "\n",
    "There you go!\n",
    "\n",
    "> Compute MSE loss between gram matrix of input and the style image and you’re good to generate an input image with the required style.\n",
    "\n",
    "<table>\n",
    "    <tbody>\n",
    "        <tr>\n",
    "            <td>\n",
    "                <img src=\"https://miro.medium.com/max/560/1*0nASi-BjZI3I9J0RWo7wtw.png\" width=\"400\" />\n",
    "            </td>\n",
    "        </tr>\n",
    "        <tr>\n",
    "            <td>\n",
    "                Summing MSE of Gram matrices for all layers, normalizing and computing a weighted sum in the end\n",
    "            </td>\n",
    "        </tr>\n",
    "    </tbody>\n",
    "</table>\n",
    "\n",
    "Finally, <strong>add both the content and style loss</strong> before backpropagating to get an output image which has the content from a content image and the style from a style image.\n",
    "\n",
    "A normal sum of both the losses may not work when the losses are in a different scale. So a weighted sum of content and style loss should work.\n",
    "\n",
    "\n",
    "<table>\n",
    "    <tbody>\n",
    "        <tr>\n",
    "            <td>\n",
    "                <img src=\"https://miro.medium.com/max/1262/1*39DOPiFLq8TcncxuLKro7Q.png\" width=\"400\" />\n",
    "            </td>\n",
    "        </tr>\n",
    "        <tr>\n",
    "            <td>\n",
    "                Total loss\n",
    "            </td>\n",
    "        </tr>\n",
    "    </tbody>\n",
    "</table>\n",
    "\n",
    "Note that the input image can be any random tensor <strong>which has the values in the same range</strong> as the content and style image.\n",
    "\n",
    "Few of the results from my implementation:\n",
    "\n",
    "<img src=\"https://miro.medium.com/max/1400/1*xa9V7EEgW-iwwsDeZhnymw.jpeg\" width=\"400\" />\n",
    "<img src=\"https://miro.medium.com/max/1400/1*cMRLk8roY5N88XqLYx0SOQ.jpeg\" width=\"400\" />\n",
    "<img src=\"https://miro.medium.com/max/1400/1*AL9gi_6AEQntjZVn7uR4AA.jpeg\" width=\"400\" />\n",
    "\n",
    "Reference from: [Neural Networks Intuitions: 2. Dot product, Gram Matrix and Neural Style Transfer](https://towardsdatascience.com/neural-networks-intuitions-2-dot-product-gram-matrix-and-neural-style-transfer-5d39653e7916)\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **5. What Style Transfer ultimately purpose**\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **6. Paper review for basic understanding for Style Transfer via INR**\n",
    "\n",
    "#### **1. Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift**\n",
    "> **Authors: Sergey Ioffe, Christian Szegedy - Google Inc., {sioffe, szegedy} @google.com**\n",
    "\n",
    "> **Original Paper**: https://arxiv.org/pdf/1502.03167.pdf\n",
    "\n",
    "> **Un official Github**: https://github.com/shuuki4/Batch-Normalization\n",
    "\n",
    "#### **2. Instance Normalization: The Missing Ingredient for Fast Stylization**\n",
    "\n",
    "> **Authors: Dmitry Ulyanov [dmitry.ulyanov@skoltech.ru], Andrea Vedaldi [vedaldi@robots.ox.ac.uk], Victor Lempitsky [lempitsky@skoltech.ru]**\n",
    "\n",
    "> **Original Paper**: https://arxiv.org/pdf/1607.08022.pdf\n",
    "\n",
    "> **Official Github**: https://github.com/DmitryUlyanov/texture_nets\n",
    "\n",
    "#### 3. **Arbitrary Style Transfer in Real-time with Adaptive Instance Normalization**\n",
    "\n",
    "> **Authors: Xun Huang, Serge Belongie - Department of Computer Science & Cornell Tech, Cornell University, {xh258,sjb344}@cornell.edu**\n",
    "\n",
    "> **Official Github**: https://github.com/xunhuang1995/AdaIN-style\n",
    "\n",
    "> **Original Paper**: https://arxiv.org/pdf/1703.06868.pdf\n",
    "\n",
    "#### **4. Semantic Image Synthesis with Spatially-Adaptive Normalization**\n",
    "\n",
    "> **Authors: Taesung Park, Ming-Yu Liu, Ting-Chun Wang, Jun-Yan ZhuL**\n",
    "\n",
    "> **Original Paper**: https://arxiv.org/pdf/1903.07291.pdf\n",
    "\n",
    "> **Official Github**: https://github.com/NVlabs/SPADE\n",
    "\n",
    "#### **5. Universal Style Transfer via Feature Transforms**\n",
    "\n",
    "> **Authors: Yijun Li\n",
    "(UC Merced -\n",
    "yli62@ucmerced.edu),\n",
    "Chen Fang\n",
    "(Adobe Research - \n",
    "cfang@adobe.com)\n",
    "Jimei Yang\n",
    "(Adobe Research -\n",
    "jimyang@adobe.com)\n",
    "Zhaowen Wang\n",
    "(Adobe Research - \n",
    "zhawang@adobe.com)\n",
    "Xin Lu\n",
    "(Adobe Research - \n",
    "xinl@adobe.com)\n",
    "Ming-Hsuan Yang\n",
    "(UC Merced, NVIDIA Research - \n",
    "mhyang@ucmerced.edu)**\n",
    "\n",
    "> **Original Paper**: https://arxiv.org/pdf/1705.08086.pdf\n",
    "\n",
    "> **Official Github**: https://github.com/Yijunmaverick/UniversalStyleTransfer\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Why Internal Covariance Shift Matters?**\n",
    "\n",
    "At the first when I encounter the *'Normalization'* term in INR (Implicit Neural Representation) Field, I thought it is just a one of the common technique(at least the current point of Jan, 2022) to enhance the deep learning models' performance.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **1. Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift**\n",
    "\n",
    "**Overview**\n",
    "\n",
    "The paper suggest the three main things that enhance the neural networks' performance by using the one of the technique called 'Batch Normalization', and the suggested benefits are below.\n",
    "\n",
    "1. Allow to use higher learning rate\n",
    "2. Allow to be less careful about initializations\n",
    "3. Acting as a regularizer, in some cases eliminating the needs of Dropout\n",
    "4. When using a nonlinear function (such as sigmoid), it prevents falling into the saturated region (where the slope is zero). Therefore, it prevents slope loss and saturation problem.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Presented Previous Problem**\n",
    "\n",
    "With using SGD(Mini-Batch Stochastic Gradient Descent), it is good algorithm to train the network, however there are several problems.\n",
    "\n",
    "1. It requires careful tuning of the model **hyper-parameters**, specifically the ***learning rate*** used in optimization, as well as the inital values for the ***model parameters***.\n",
    "\n",
    "2. The change in the distributions of layers' input presents a problem because the layers need to continuosly adapt to the new distribution.\n",
    "\n",
    "*When the input distribution to a learning system changes, it is is said to experience* ***Covariance Shift***.\n",
    "\n",
    "> This is typically handled via domain adaption. However, the notion of covariate shift can be extended beyond the learning system as a whole, to apply its parts, such as a sub-network or a layer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Solution**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Method**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Contribution**"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "3efb3a3e09a38f93b1d9bfa4b9b8ce66a167f6ada77b64bc003d7d6d298d81d5"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 64-bit ('buddhalight': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
