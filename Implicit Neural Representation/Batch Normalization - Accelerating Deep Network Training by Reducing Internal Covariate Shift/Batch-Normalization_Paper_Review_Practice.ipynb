{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Batch Normalization: Accerlerating Deep Network Training by Reducing Internal Covariate Shift**\n",
    "\n",
    "**Authors: Sergey Ioffe, Christian Szegedy - Google Inc., {sioffe, szegedy} @google.com**\n",
    "\n",
    "**Un official Github**: https://github.com/shuuki4/Batch-Normalization\n",
    "\n",
    "---\n",
    "\n",
    "**Edited By Su Hyung Choi (Key Summary & Code Practice)**\n",
    "\n",
    "If you have any issues on this scripts, please PR to the repository below.\n",
    "\n",
    "**[Github: @JonyChoi - Computer Vision Paper Reviews]** https://github.com/jonychoi/Computer-Vision-Paper-Reviews\n",
    "\n",
    "Edited Jan 15 2022\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Abstract**\n",
    "\n",
    "\n",
    "<table>\n",
    "    <thead>\n",
    "        <tr>\n",
    "            <th>\n",
    "                Abstract\n",
    "            </th>\n",
    "            <th>\n",
    "                Key Summary\n",
    "            </th>\n",
    "        </tr>\n",
    "    </thead>\n",
    "    <tbody>\n",
    "        <tr>\n",
    "            <td>\n",
    "                <p>\n",
    "                    Training Deep Neural Networks is complicated by the fact\n",
    "                    that the distribution of each layer’s inputs changes during\n",
    "                    training, as the parameters of the previous layers change.\n",
    "                    This slows down the training by requiring lower learning\n",
    "                    rates and careful parameter initialization, and makes it notoriously hard to train models with saturating nonlinearities. We refer to this phenomenon as internal covariate\n",
    "                    shift, and address the problem by normalizing layer inputs. Our method draws its strength from making normalization a part of the model architecture and performing the\n",
    "                    normalization for each training mini-batch. Batch Normalization allows us to use much higher learning rates and\n",
    "                    be less careful about initialization. It also acts as a regularizer, in some cases eliminating the need for Dropout.\n",
    "                    Applied to a state-of-the-art image classification model,\n",
    "                    Batch Normalization achieves the same accuracy with 14\n",
    "                    times fewer training steps, and beats the original model\n",
    "                    by a significant margin. Using an ensemble of batchnormalized networks, we improve upon the best published\n",
    "                    result on ImageNet classification: reaching 4.9% top-5\n",
    "                    validation error (and 4.8% test error), exceeding the accuracy of human raters.\n",
    "                </p>\n",
    "                <p>\n",
    "                </p>\n",
    "            </td>\n",
    "        </tr>\n",
    "    </tbody>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **1 Introduction**\n",
    "\n",
    "<table>\n",
    "    <thead>\n",
    "        <tr>\n",
    "            <th>\n",
    "                1. Introduction\n",
    "            </th>\n",
    "            <th>\n",
    "                Key Summary\n",
    "            </th>\n",
    "        </tr>\n",
    "    </thead>\n",
    "    <tbody>\n",
    "        <tr>\n",
    "            <td>\n",
    "                <p>\n",
    "                    Deep learning has dramatically advanced the state of the\n",
    "                    art in vision, speech, and many other areas. Stochastic gradient descent (SGD) has proved to be an effective way of training deep networks, and SGD variants\n",
    "                    such as momentum (Sutskever et al., 2013) and Adagrad\n",
    "                    (Duchi et al., 2011) have been used to achieve state of the\n",
    "                    art performance. SGD optimizes the parameters\n",
    "                    Θ of the\n",
    "                    network, so as to minimize the loss\n",
    "                </p>\n",
    "                <img src=\"./imgs/equation1.png\" />\n",
    "                <p>\n",
    "                where x 1...N is the training data set. With SGD, the training proceeds in steps, and at each step we consider a minibatch x1...m of size m. The mini-batch is used to approximate the gradient of the loss function with respect to the parameters, by computing\n",
    "                </p>\n",
    "                <img src=\"./imgs/equation2.png\" />\n",
    "                <p>\n",
    "                    Using mini-batches of examples, as opposed to one example at a time, is helpful in several ways. First, the gradient\n",
    "                    of the loss over a mini-batch is an estimate of the gradient\n",
    "                    over the training set, whose quality improves as the batch\n",
    "                    size increases. Second, computation over a batch can be\n",
    "                    much more efficient than\n",
    "                    m computations for individual\n",
    "                    examples, due to the parallelism afforded by the modern\n",
    "                    computing platforms.\n",
    "                </p>\n",
    "                <p>\n",
    "                    While stochastic gradient is simple and effective, it\n",
    "                    requires careful tuning of the model hyper-parameters,\n",
    "                    specifically the learning rate used in optimization, as well\n",
    "                    as the initial values for the model parameters. The training is complicated by the fact that the inputs to each layer\n",
    "                    are affected by the parameters of all preceding layers – so\n",
    "                    that small changes to the network parameters amplify as\n",
    "                    the network becomes deeper.\n",
    "                </p>\n",
    "                <p>\n",
    "                    The change in the distributions of layers’ inputs\n",
    "                    presents a problem because the layers need to continuously adapt to the new distribution. When the input distribution to a learning system changes, it is said to experience covariate shift (Shimodaira, 2000). This is typically\n",
    "                    handled via domain adaptation (Jiang, 2008). However,\n",
    "                    the notion of covariate shift can be extended beyond the\n",
    "                    learning system as a whole, to apply to its parts, such as a\n",
    "                    sub-network or a layer. Consider a network computing\n",
    "                </p>\n",
    "                <img src=\"./imgs/equation3.png\" />\n",
    "                <p>\n",
    "                    where F1 and F2 are arbitrary transformations, and the parameters Θ1 , Θ2 are to be learned so as to minimize the loss ℓ. Learning Θ2 can be viewed as if the inputs x = F1(u , Θ 1 ) are fed into the sub-network\n",
    "                </p>\n",
    "                <img src=\"./imgs/equation4.png\" />\n",
    "                <p>\n",
    "                    For example, a gradient descent step\n",
    "                </p>\n",
    "                <img src=\"./imgs/equation5.png\" />\n",
    "                <p>\n",
    "                    (for batch size m and learning rate\n",
    "                    α) is exactly equivalent\n",
    "                    to that for a stand-alone network\n",
    "                    F2 with input\n",
    "                    x. Therefore, the input distribution properties that make training\n",
    "                    more efficient – such as having the same distribution between the training and test data – apply to training the\n",
    "                    sub-network as well. As such it is advantageous for the\n",
    "                    distribution of\n",
    "                    x to remain fixed over time. Then,\n",
    "                    Θ\n",
    "                    2 does not have to readjust to compensate for the change in the\n",
    "                    distribution of x.\n",
    "                </p>\n",
    "                <p>\n",
    "                    Fixed distribution of inputs to a sub-network would\n",
    "                    have positive consequences for the layers outside the subnetwork, as well. Consider a layer with a sigmoid activation function z = g(Wu + b) where u is the layer input,\n",
    "                    the weight matrix W and bias vector b are the layer parameters to be learned, and g(x) = 1\n",
    "                    1+exp(−x)\n",
    "                    . As |x|\n",
    "                    increases, g\n",
    "                    ′\n",
    "                    (x) tends to zero. This means that for all dimensions of x = Wu+b except those with small absolute\n",
    "                    values, the gradient flowing down to u will vanish and the\n",
    "                    model will train slowly. However, since x is affected by\n",
    "                    W, b and the parameters of all the layers below, changes\n",
    "                    to those parameters during training will likely move many\n",
    "                    dimensions of x into the saturated regime of the nonlinearity and slow down the convergence. This effect is\n",
    "                    amplified as the network depth increases. In practice,\n",
    "                    the saturation problem and the resulting vanishing gradients are usually addressed by using Rectified Linear Units\n",
    "                    (Nair & Hinton, 2010) ReLU(x) = max(x, 0), careful\n",
    "                    initialization (Bengio & Glorot, 2010; Saxe et al., 2013),\n",
    "                    and small learning rates. If, however, we could ensure\n",
    "                    that the distribution of nonlinearity inputs remains more\n",
    "                    stable as the network trains, then the optimizer would be\n",
    "                    less likely to get stuck in the saturated regime, and the\n",
    "                    training would accelerate.\n",
    "                </p>\n",
    "                <p>\n",
    "                    We refer to the change in the distributions of internal\n",
    "                    nodes of a deep network, in the course of training, as Internal Covariate Shift. Eliminating it offers a promise of\n",
    "                    faster training. We propose a new mechanism, which we\n",
    "                    call Batch Normalization, that takes a step towards reducing internal covariate shift, and in doing so dramatically accelerates the training of deep neural nets. It accomplishes this via a normalization step that fixes the\n",
    "                    means and variances of layer inputs. Batch Normalization\n",
    "                    also has a beneficial effect on the gradient flow through\n",
    "                    the network, by reducing the dependence of gradients\n",
    "                    on the scale of the parameters or of their initial values.\n",
    "                    This allows us to use much higher learning rates without the risk of divergence. Furthermore, batch normalization regularizes the model and reduces the need for\n",
    "                    Dropout (Srivastava et al., 2014). Finally, Batch Normalization makes it possible to use saturating nonlinearities\n",
    "                    by preventing the network from getting stuck in the saturated modes.\n",
    "                </p>\n",
    "                <p>\n",
    "                    In Sec. 4.2, we apply Batch Normalization to the bestperforming ImageNet classification network, and show\n",
    "                    that we can match its performance using only 7% of the\n",
    "                    training steps, and can further exceed its accuracy by a\n",
    "                    substantial margin. Using an ensemble of such networks\n",
    "                    trained with Batch Normalization, we achieve the top-5\n",
    "                    error rate that improves upon the best known results on\n",
    "                    ImageNet classification.\n",
    "                </p>\n",
    "            </td>\n",
    "        </tr>\n",
    "    </tbody>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **2 Towards Reducing Internal Covariate Shift**\n",
    "\n",
    "<table>\n",
    "    <thead>\n",
    "        <tr>\n",
    "            <th>\n",
    "                Towards Reducing Internal Covariate Shift\n",
    "            </th>\n",
    "            <th>\n",
    "                Key Summary\n",
    "            </th>\n",
    "        </tr>\n",
    "    </thead>\n",
    "    <tbody>\n",
    "        <tr>\n",
    "            <td>\n",
    "                <p>\n",
    "                    We define Internal Covariate Shift as the change in the\n",
    "                    distribution of network activations due to the change in\n",
    "                    network parameters during training. To improve the training, we seek to reduce the internal covariate shift. By\n",
    "                    fixing the distribution of the layer inputs x as the training\n",
    "                    progresses, we expect to improve the training speed. It has\n",
    "                    been long known (LeCun et al., 1998b; Wiesler & Ney,\n",
    "                    2011) that the network training converges faster if its inputs are whitened – i.e., linearly transformed to have zero\n",
    "                    means and unit variances, and decorrelated. As each layer\n",
    "                    observes the inputs produced by the layers below, it would\n",
    "                    be advantageous to achieve the same whitening of the inputs of each layer. By whitening the inputs to each layer,\n",
    "                    we would take a step towards achieving the fixed distributions of inputs that would remove the ill effects of the\n",
    "                    internal covariate shift.\n",
    "                </p>\n",
    "                <p>\n",
    "                    We could consider whitening activations at every training step or at some interval, either by modifying the\n",
    "                    network directly or by changing the parameters of the\n",
    "                    optimization algorithm to depend on the network activation values (Wiesler et al., 2014; Raiko et al., 2012;\n",
    "                    Povey et al., 2014; Desjardins & Kavukcuoglu). However, if these modifications are interspersed with the optimization steps, then the gradient descent step may attempt to update the parameters in a way that requires\n",
    "                    the normalization to be updated, which reduces the effect of the gradient step. For example, consider a layer\n",
    "                    with the input u that adds the learned bias b, and normalizes the result by subtracting the mean of the activation\n",
    "                    computed over the training data: xb = x − E[x] where\n",
    "                    x = u + b, X = {x1...N } is the set of values of x over\n",
    "                    the training set, and E[x] = 1\n",
    "                    N\n",
    "                    PN\n",
    "                    i=1 xi\n",
    "                    . If a gradient\n",
    "                    descent step ignores the dependence of E[x] on b, then it\n",
    "                    will update b ← b + ∆b, where ∆b ∝ −∂ℓ/∂xb. Then\n",
    "                    u + (b + ∆b) − E[u + (b + ∆b)] = u + b − E[u + b].\n",
    "                    Thus, the combination of the update to b and subsequent\n",
    "                    change in normalization led to no change in the output\n",
    "                    of the layer nor, consequently, the loss. As the training\n",
    "                    continues, b will grow indefinitely while the loss remains\n",
    "                    fixed. This problem can get worse if the normalization not\n",
    "                    only centers but also scales the activations. We have observed this empirically in initial experiments, where the\n",
    "                    model blows up when the normalization parameters are\n",
    "                    computed outside the gradient descent step.\n",
    "                </p>\n",
    "                <p>\n",
    "                    The issue with the above approach is that the gradient\n",
    "                    descent optimization does not take into account the fact\n",
    "                    that the normalization takes place. To address this issue,\n",
    "                    we would like to ensure that, for any parameter values,\n",
    "                    the network always produces activations with the desired\n",
    "                    distribution. Doing so would allow the gradient of the\n",
    "                    loss with respect to the model parameters to account for\n",
    "                    the normalization, and for its dependence on the model\n",
    "                    parameters Θ. Let again x be a layer input, treated as a\n",
    "                    2\n",
    "                    vector, and X be the set of these inputs over the training\n",
    "                    data set. The normalization can then be written as a transformation\n",
    "                </p>\n",
    "                <img src=\"./imgs/equation6.png\" />\n",
    "                <p>\n",
    "                    which depends not only on the given training example x\n",
    "                    but on all examples X – each of which depends on Θ if\n",
    "                    x is generated by another layer. For backpropagation, we\n",
    "                    would need to compute the Jacobians\n",
    "                </p>\n",
    "                <p>\n",
    "                    ignoring the latter term would lead to the explosion described above. Within this framework, whitening the layer\n",
    "                    inputs is expensive, as it requires computing the covariance matrix Cov[x] = Ex∈X [xxT\n",
    "                    ] − E[x]E[x]T\n",
    "                    and its\n",
    "                    inverse square root, to produce the whitened activations\n",
    "                    Cov[x]−1/2\n",
    "                    (x − E[x]), as well as the derivatives of these\n",
    "                    transforms for backpropagation. This motivates us to seek\n",
    "                    an alternative that performs input normalization in a way\n",
    "                    that is differentiable and does not require the analysis of\n",
    "                    the entire training set after every parameter update\n",
    "                </p>\n",
    "                <p>\n",
    "                    Some of the previous approaches (e.g.\n",
    "                    (Lyu & Simoncelli, 2008)) use statistics computed\n",
    "                    over a single training example, or, in the case of image\n",
    "                    networks, over different feature maps at a given location.\n",
    "                    However, this changes the representation ability of a\n",
    "                    network by discarding the absolute scale of activations.\n",
    "                    We want to a preserve the information in the network, by\n",
    "                    normalizing the activations in a training example relative\n",
    "                    to the statistics of the entire training data.\n",
    "                </p>\n",
    "            </td>\n",
    "        </tr>\n",
    "    </tbody>\n",
    "</table>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **3 Normalization via Mini-Batch Statistics**\n",
    "\n",
    "\n",
    "<table>\n",
    "    <thead>\n",
    "        <tr>\n",
    "            <th>\n",
    "                Normalization via Mini-Batch Statistics\n",
    "            </th>\n",
    "            <th>\n",
    "                Key Summary\n",
    "            </th>\n",
    "        </tr>\n",
    "    </thead>\n",
    "    <tbody>\n",
    "        <tr>\n",
    "            <td>\n",
    "                <p>\n",
    "                    Since the full whitening of each layer’s inputs is costly\n",
    "                    and not everywhere differentiable, we make two necessary simplifications. The first is that instead of whitening\n",
    "                    the features in layer inputs and outputs jointly, we will\n",
    "                    normalize each scalar feature independently, by making it\n",
    "                    have the mean of zero and the variance of 1. For a layer\n",
    "                    with d-dimensional input x = (x\n",
    "                    (1) . . . x(d)\n",
    "                    ), we will normalize each dimension\n",
    "                </p>\n",
    "                <img src=\"./imgs/equation8.png\" />\n",
    "                <p>\n",
    "                    where the expectation and variance are computed over the\n",
    "                    training data set. As shown in (LeCun et al., 1998b), such\n",
    "                    normalization speeds up convergence, even when the features are not decorrelated.\n",
    "                </p>\n",
    "                <p>\n",
    "                    Note that simply normalizing each input of a layer may\n",
    "                    change what the layer can represent. For instance, normalizing the inputs of a sigmoid would constrain them to\n",
    "                    the linear regime of the nonlinearity. To address this, we\n",
    "                    make sure that the transformation inserted in the network\n",
    "                    can represent the identity transform. To accomplish this, we introduce, for each activation x\n",
    "                    (k)\n",
    "                    , a pair of parameters\n",
    "                    γ\n",
    "                    (k)\n",
    "                    , β(k)\n",
    "                    , which scale and shift the normalized value:\n",
    "                </p>\n",
    "                <img src=\"./imgs/equation9.png\" />\n",
    "                <p>\n",
    "                    These parameters are learned along with the original\n",
    "                    model parameters, and restore the representation power\n",
    "                    of the network. Indeed, by setting γ\n",
    "                    (k) =\n",
    "                    root(\n",
    "                    Var[x\n",
    "                    (k))\n",
    "                    ] and\n",
    "                    β\n",
    "                    (k) = E[x\n",
    "                    (k)\n",
    "                    ], we could recover the original activations,\n",
    "                    if that were the optimal thing to do.\n",
    "                </p>\n",
    "                <p>\n",
    "                    In the batch setting where each training step is based on\n",
    "                    the entire training set, we would use the whole set to normalize activations. However, this is impractical when using stochastic optimization. Therefore, we make the second simplification: since we use mini-batches in stochastic gradient training, each mini-batch produces estimates\n",
    "                    of the mean and variance of each activation. This way, the\n",
    "                    statistics used for normalization can fully participate in\n",
    "                    the gradient backpropagation. Note that the use of minibatches is enabled by computation of per-dimension variances rather than joint covariances; in the joint case, regularization would be required since the mini-batch size is\n",
    "                    likely to be smaller than the number of activations being\n",
    "                    whitened, resulting in singular covariance matrices.\n",
    "                </p>\n",
    "                <p>\n",
    "                    Consider a mini-batch B of size m. Since the normalization is applied to each activation independently, let us\n",
    "                    focus on a particular activation x\n",
    "                    (k)\n",
    "                    and omit k for clarity.\n",
    "                    We have m values of this activation in the mini-batch,\n",
    "                </p>\n",
    "                <img src=\"./imgs/equation10.png\" />\n",
    "                <p>\n",
    "                    Let the normalized values be xb1...m, and their linear transformations be y1...m. We refer to the transform\n",
    "                </p>\n",
    "                <img src=\"./imgs/equation11.png\" />\n",
    "                <p>\n",
    "                    as the Batch Normalizing Transform. We present the BN\n",
    "                    Transform in Algorithm 1. In the algorithm, ǫ is a constant\n",
    "                    added to the mini-batch variance for numerical stability\n",
    "                </p>\n",
    "                <img src=\"./imgs/algorithm1.png\" />\n",
    "                <p>\n",
    "                    The BN transform can be added to a network to manipulate any activation. In the notation y = BNγ,β(x), we\n",
    "                    3\n",
    "                    indicate that the parameters γ and β are to be learned,\n",
    "                    but it should be noted that the BN transform does not\n",
    "                    independently process the activation in each training example. Rather, BNγ,β(x) depends both on the training\n",
    "                    example and the other examples in the mini-batch. The\n",
    "                    scaled and shifted values y are passed to other network\n",
    "                    layers. The normalized activations xb are internal to our\n",
    "                    transformation, but their presence is crucial. The distributions of values of any xb has the expected value of 0\n",
    "                    and the variance of 1, as long as the elements of each\n",
    "                    mini-batch are sampled from the same distribution, and\n",
    "                    P\n",
    "                    if we neglect ǫ. This can be seen by observing that\n",
    "                    m\n",
    "                    i=1 xbi = 0 and 1\n",
    "                    m\n",
    "                    Pm\n",
    "                    i=1 xb\n",
    "                    2\n",
    "                    i = 1, and taking expectations. Each normalized activation xb\n",
    "                    (k)\n",
    "                    can be viewed as\n",
    "                    an input to a sub-network composed of the linear transform y\n",
    "                    (k) = γ\n",
    "                    (k)xb\n",
    "                    (k) + β\n",
    "                    (k)\n",
    "                    , followed by the other processing done by the original network. These sub-network\n",
    "                    inputs all have fixed means and variances, and although\n",
    "                    the joint distribution of these normalized xb\n",
    "                    (k)\n",
    "                    can change\n",
    "                    over the course of training, we expect that the introduction of normalized inputs accelerates the training of the\n",
    "                    sub-network and, consequently, the network as a whole.\n",
    "                </p>\n",
    "                <p>\n",
    "                    During training we need to backpropagate the gradient of loss ℓ through this transformation, as well as compute the gradients with respect to the parameters of the\n",
    "                    BN transform. We use chain rule, as follows (before simplification):\n",
    "                </p>\n",
    "                <img src=\"./imgs/figure1.png\" />\n",
    "                <p>\n",
    "                    Thus, BN transform is a differentiable transformation that\n",
    "                    introduces normalized activations into the network. This\n",
    "                    ensures that as the model is training, layers can continue\n",
    "                    learning on input distributions that exhibit less internal covariate shift, thus accelerating the training. Furthermore,\n",
    "                    the learned affine transform applied to these normalized\n",
    "                    activations allows the BN transform to represent the identity transformation and preserves the network capacity.\n",
    "                </p>\n",
    "            </td>\n",
    "        </tr>\n",
    "    </tbody>\n",
    "</table>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **3.1 Training and Inference with BatchNormalized Networks**\n",
    "\n",
    "To Batch-Normalize a network, we specify a subset of activations and insert the BN transform for each of them,\n",
    "according to Alg. 1. Any layer that previously received\n",
    "x as the input, now receives BN(x). A model employing\n",
    "Batch Normalization can be trained using batch gradient\n",
    "descent, or Stochastic Gradient Descent with a mini-batch\n",
    "size m > 1, or with any of its variants such as Adagrad"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
