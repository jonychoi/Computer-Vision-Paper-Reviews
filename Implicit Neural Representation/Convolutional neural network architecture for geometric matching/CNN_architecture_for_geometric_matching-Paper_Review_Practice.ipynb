{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Convolutional neural network architecture for geometric matching**\n",
    "\n",
    "**Authors: Ignacio Rocco  (IDI), Relja Arandjelovic (ENS), Josef Sivic (3CIIRC)**\n",
    "\n",
    "**Official Github**: https://github.com/ignacio-rocco/cnngeometric_pytorch\n",
    "\n",
    "---\n",
    "\n",
    "**Edited By Su Hyung Choi (Key Summary & Code Practice)**\n",
    "\n",
    "If you have any issues on this scripts, please PR to the repository below.\n",
    "\n",
    "**[Github: @JonyChoi - Computer Vision Paper Reviews]** https://github.com/jonychoi/Computer-Vision-Paper-Reviews\n",
    "\n",
    "Edited Jan 10 2022\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Abstract**\n",
    "\n",
    "<table>\n",
    "    <thead>\n",
    "        <tr>\n",
    "            <th>\n",
    "                Abstract\n",
    "            </th>\n",
    "        </tr>\n",
    "    </thead>\n",
    "    <tbody>\n",
    "        <tr>\n",
    "            <td>\n",
    "                <i>\n",
    "                    We address the problem of determining correspondences\n",
    "                    between two images in agreement with a geometric model\n",
    "                    such as an affine or thin-plate spline transformation, and\n",
    "                    estimating its parameters. The contributions of this work\n",
    "                    are three-fold. First, we propose a convolutional neural network architecture for geometric matching. The architecture\n",
    "                    is based on three main components that mimic the standard\n",
    "                    steps of feature extraction, matching and simultaneous inlier detection and model parameter estimation, while being\n",
    "                    trainable end-to-end. Second, we demonstrate that the network parameters can be trained from synthetically generated imagery without the need for manual annotation and\n",
    "                    that our matching layer significantly increases generalization capabilities to never seen before images. Finally, we\n",
    "                    show that the same model can perform both instance-level\n",
    "                    and category-level matching giving state-of-the-art results\n",
    "                    on the challenging Proposal Flow dataset.\n",
    "                </i>\n",
    "            </td>\n",
    "        </tr>\n",
    "    </tbody>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **1. Introduction**\n",
    "\n",
    "\n",
    "<table>\n",
    "    <thead>\n",
    "        <tr>\n",
    "            <th>\n",
    "                Introduction\n",
    "            </th>\n",
    "        </tr>\n",
    "    </thead>\n",
    "    <tbody>\n",
    "        <tr>\n",
    "            <td>\n",
    "                <p>\n",
    "                    Estimating correspondences between images is one of\n",
    "                    the fundamental problems in computer vision [19, 25] with\n",
    "                    applications ranging from large-scale 3D reconstruction [3]\n",
    "                    to image manipulation [21] and semantic segmentation\n",
    "                    [42]. Traditionally, correspondences consistent with a geometric model such as epipolar geometry or planar affine\n",
    "                    transformation, are computed by detecting and matching\n",
    "                    local features (such as SIFT [38] or HOG [12, 22]), followed by pruning incorrect matches using local geometric\n",
    "                    constraints [43, 47] and robust estimation of a global geometric transformation using algorithms such as RANSAC\n",
    "                    [18] or Hough transform [32, 34, 38]. This approach works\n",
    "                    well in many cases but fails in situations that exhibit (i) large\n",
    "                    changes of depicted appearance due to e.g. intra-class variation [22], or (ii) large changes of scene layout or non-rigid deformations that require complex geometric models with\n",
    "                    many parameters which are hard to estimate in a manner\n",
    "                    robust to outliers.\n",
    "                </p>\n",
    "                <table>\n",
    "                    <tbody>\n",
    "                        <tr>\n",
    "                            <td>\n",
    "                                <img src=\"./imgs/figure1.png\" width=\"550px\" />\n",
    "                            </td>\n",
    "                            <td>\n",
    "                                Figure 1: Our trained geometry estimation network automatically\n",
    "                                aligns two images with substantial appearance differences. It is\n",
    "                                able to estimate large deformable transformations robustly in the\n",
    "                                presence of clutter.\n",
    "                            </td>\n",
    "                        </tr>\n",
    "                    </tbody>\n",
    "                </table>\n",
    "                <p>\n",
    "                    In this work we build on the traditional approach and\n",
    "                    develop a convolutional neural network (CNN) architecture\n",
    "                    that mimics the standard matching process. First, we replace the standard local features with powerful trainable\n",
    "                    convolutional neural network features [31, 46], which allows us to handle large changes of appearance between\n",
    "                    the matched images. Second, we develop trainable matching and transformation estimation layers that can cope with\n",
    "                    noisy and incorrect matches in a robust way, mimicking the\n",
    "                    good practices in feature matching such as the second nearest neighbor test [38], neighborhood consensus [43, 47] and\n",
    "                    Hough transform-like estimation [32, 34, 38].\n",
    "                </p>\n",
    "                <p>\n",
    "                    The outcome is a convolutional neural network architecture trainable for the end task of geometric matching,\n",
    "                    which can handle large appearance changes, and is therefore\n",
    "                    suitable for both instance-level and category-level matching\n",
    "                    problems.\n",
    "                </p>\n",
    "            </td>\n",
    "        </tr>\n",
    "    </tbody>\n",
    "</table>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **2. Related Work**\n",
    "\n",
    "\n",
    "<table>\n",
    "    <thead>\n",
    "        <tr>\n",
    "            <th>\n",
    "                Related Work\n",
    "            </th>\n",
    "        </tr>\n",
    "    </thead>\n",
    "    <tbody>\n",
    "        <tr>\n",
    "            <td>\n",
    "                <p>            \n",
    "                    The classical approach for finding correspondences involves identifying interest points and computing local descriptors around these points [10, 11, 24, 37, 38, 39, 43].\n",
    "                    6148\n",
    "                    While this approach performs relatively well for instancelevel matching, the feature detectors and descriptors lack\n",
    "                    the generalization ability for category-level matching.\n",
    "                </p>\n",
    "                <p>\n",
    "                    Recently, convolutional neural networks have been used\n",
    "                    to learn powerful feature descriptors which are more robust\n",
    "                    to appearance changes than the classical descriptors [9, 23,\n",
    "                    28, 45, 52]. However, these works still divide the image into\n",
    "                    a set of local patches and extract a descriptor individually\n",
    "                    from each patch. Extracted descriptors are then compared\n",
    "                    with an appropriate distance measure [9, 28, 45], by directly\n",
    "                    outputting a similarity score [23, 52], or even by directly\n",
    "                    outputting a binary matching/non-matching decision [4].\n",
    "                </p>\n",
    "                <p>\n",
    "                    In this work, we take a different approach, treating the\n",
    "                    image as a whole, instead of a set of patches. Our approach\n",
    "                    has the advantage of capturing the interaction of the different parts of the image in a greater extent, which is not possible when the image is divided into a set of local regions.\n",
    "                </p>\n",
    "                <p>\n",
    "                    Related are also network architectures for estimating\n",
    "                    inter-frame motion in video [17, 48, 50] or instance-level\n",
    "                    homography estimation [14], however their goal is very different from ours, targeting high-precision correspondence\n",
    "                    with very limited appearance variation and background\n",
    "                    clutter. Closer to us is the network architecture of [29]\n",
    "                    which, however, tackles a different problem of fine-grained\n",
    "                    category-level matching (different species of birds) with\n",
    "                    limited background clutter and small translations and scale\n",
    "                    changes, as their objects are largely centered in the image.\n",
    "                    In addition, their architecture is based on a different matching layer, which we show not to perform as well as the\n",
    "                    matching layer used in our work.\n",
    "                </p>\n",
    "                <p>\n",
    "                    Some works, such as [11, 15, 22, 30, 35, 36], have addressed the hard problem of category-level matching, but\n",
    "                    rely on traditional non-trainable optimization for matching\n",
    "                    [11, 15, 30, 35, 36], or guide the matching using object proposals [22]. On the contrary, our approach is fully trainable\n",
    "                    in an end-to-end manner and does not require any optimization procedure at evaluation time, or guidance by object proposals.\n",
    "                </p>\n",
    "                <p>\n",
    "                    Others [33, 44, 53] have addressed the problems of instance and category-level correspondence by performing\n",
    "                    joint image alignment. However, these methods differ from\n",
    "                    ours as they: (i) require class labels; (ii) don’t use CNN features; (iii) jointly align a large set of images, while we align\n",
    "                    image pairs; and (iv) don’t use a trainable CNN architecture\n",
    "                    for alignment as we do.\n",
    "                </p>\n",
    "            </td>\n",
    "        </tr>\n",
    "    </tbody>\n",
    "</table>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **3. Architecture for geometric matching**\n",
    "\n",
    "<table>\n",
    "    <thead>\n",
    "        <tr>\n",
    "            <th>\n",
    "                Architecture for geometric matching\n",
    "            </th>\n",
    "        </tr>\n",
    "    </thead>\n",
    "    <tbody>\n",
    "        <tr>\n",
    "            <td>\n",
    "                <p>\n",
    "                    In this section, we introduce a new convolutional neural network architecture for estimating parameters of a geometric transformation between two input images. The architecture is designed to mimic the classical computer vision pipeline (e.g. [40]), while using differentiable modules\n",
    "                    so that it is trainable end-to-end for the geometry estimation task. The classical approach consists of the following\n",
    "                    stages: (i) local descriptors (e.g. SIFT) are extracted from\n",
    "                    both input images, (ii) the descriptors are matched across\n",
    "                    images to form a set of tentative correspondences, which\n",
    "                    are then used to (iii) robustly estimate the parameters of the\n",
    "                    geometric model using RANSAC or Hough voting.\n",
    "                </p>     \n",
    "                <table>\n",
    "                    <tbody>\n",
    "                        <tr>\n",
    "                            <td>\n",
    "                                <img src=\"./imgs/figure2.png\" width=\"500px\" />\n",
    "                            </td>\n",
    "                            <td>\n",
    "                                <img src=\"./imgs/figure2_description.png\" width=\"500px\" />\n",
    "                            </td>\n",
    "                        </tr>\n",
    "                    </tbody>\n",
    "                </table>\n",
    "                <p>\n",
    "                    Our architecture, illustrated in Fig. 2, mimics this process by: (i) passing input images IA and IB through a\n",
    "                    siamese architecture consisting of convolutional layers, thus\n",
    "                    extracting feature maps fA and fB which are analogous to\n",
    "                    dense local descriptors, (ii) matching the feature maps (“descriptors”) across images into a tentative correspondence\n",
    "                    map fAB, followed by a (iii) regression network which directly outputs the parameters of the geometric model, ˆθ, in\n",
    "                    a robust manner. The inputs to the network are the two images, and the outputs are the parameters of the chosen geometric model, e.g. a 6-D vector for an affine transformation.\n",
    "                </p>\n",
    "                <p>\n",
    "                    In the following, we describe each of the three stages in detail.\n",
    "                </p>\n",
    "            </td>\n",
    "        </tr>\n",
    "    </tbody>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **3.1. Feature extraction**\n",
    "\n",
    "<table>\n",
    "    <thead>\n",
    "        <tr>\n",
    "            <th>\n",
    "                Feature extraction\n",
    "            </th>\n",
    "        </tr>\n",
    "    </thead>\n",
    "    <tbody>\n",
    "        <tr>\n",
    "            <td>\n",
    "                <p>\n",
    "                    The first stage of the pipeline is feature extraction, for\n",
    "                    which we use a standard CNN architecture. A CNN without fully connected layers takes an input image and produces a feature map f ∈ R\n",
    "                    h×w×d\n",
    "                    , which can be interpreted\n",
    "                    as a h × w dense spatial grid of d-dimensional local descriptors. A similar interpretation has been used previously\n",
    "                    in instance retrieval [5, 7, 8, 20] demonstrating high discriminative power of CNN-based descriptors. Thus, for feature extraction we use the VGG-16 network [46], cropped\n",
    "                    at the pool4 layer (before the ReLU unit), followed by\n",
    "                    per-feature L2-normalization. We use a pre-trained model,\n",
    "                    originally trained on ImageNet [13] for the task of image\n",
    "                    classification. As shown in Fig. 2, the feature extraction network is duplicated and arranged in a siamese configuration\n",
    "                    such that the two input images are passed through two identical networks which share parameters.\n",
    "                </p>\n",
    "            </td>\n",
    "        </tr>\n",
    "    </tbody>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **3.2. Matching network**\n",
    "\n",
    "\n",
    "<table>\n",
    "    <thead>\n",
    "        <tr>\n",
    "            <th>\n",
    "                Matching network\n",
    "            </th>\n",
    "        </tr>\n",
    "    </thead>\n",
    "    <tbody>\n",
    "        <tr>\n",
    "            <td>\n",
    "                <p>\n",
    "                    The image features produced by the feature extraction\n",
    "                    networks should be combined into a single tensor as input to\n",
    "                    the regressor network to estimate the geometric transformation. We first describe the classical approach for generating\n",
    "                    tentative correspondences, and then present our matching\n",
    "                    layer which mimics this process.\n",
    "                </p>       \n",
    "                <img src=\"./imgs/figure3.png\" width=\"500px\" />\n",
    "                <img src=\"./imgs/figure3_description.png\" width=\"500px\" />\n",
    "                <p>\n",
    "                    <strong>Tentative matches in classical geometry estimation.</strong>\n",
    "                    Classical methods start by computing similarities between\n",
    "                    all pairs of descriptors across the two images. From this\n",
    "                    point on, the original descriptors are discarded as all the\n",
    "                    necessary information for geometry estimation is contained\n",
    "                    in the pairwise descriptor similarities and their spatial locations. Secondly, the pairs are pruned by either thresholding\n",
    "                    the similarity values, or, more commonly, only keeping the\n",
    "                    matches which involve the nearest (most similar) neighbors.\n",
    "                    Furthermore, the second nearest neighbor test [38] prunes\n",
    "                    the matches further by requiring that the match strength is\n",
    "                    significantly stronger than the second best match involving\n",
    "                    the same descriptor, which is very effective at discarding\n",
    "                    ambiguous matches.\n",
    "                </p>\n",
    "                <p>\n",
    "                    <strong>Matching layer.</strong> Our matching layer applies a similar procedure. Analogously to the classical approach, only descriptor similarities and their spatial locations should be\n",
    "                    considered for geometry estimation, and not the original descriptors themselves.\n",
    "                </p>\n",
    "                <p>\n",
    "                    To achieve this, we propose to use a correlation layer\n",
    "                    followed by normalization. Firstly, all pairs of similarities\n",
    "                    between descriptors are computed in the correlation layer.\n",
    "                    Secondly, similarity scores are processed and normalized\n",
    "                    such that ambiguous matches are strongly down-weighted.\n",
    "                </p>\n",
    "                <p>\n",
    "                    In more detail, given L2-normalized dense feature\n",
    "                    maps fA, fB ∈ R\n",
    "                    h×w×d\n",
    "                    , the correlation map cAB ∈\n",
    "                    R\n",
    "                    h×w×(h×w) outputted by the correlation layer contains at\n",
    "                    each position the scalar product of a pair of individual descriptors fA ∈ fA and fB ∈ fB, as detailed in Eq. (1).\n",
    "                </p>\n",
    "                <img src=\"./imgs/equation1.png\" width=\"500px\" />\n",
    "                <p>\n",
    "                    where (i, j) and (ik, jk) indicate the individual feature positions in the h×w dense feature maps, and k = h(jk−1)+ik\n",
    "                    is an auxiliary indexing variable for (ik, jk).\n",
    "                </p>\n",
    "                <p>\n",
    "                    A diagram of the correlation layer is presented in Fig. 3.\n",
    "                    Note that at a particular position (i, j), the correlation map\n",
    "                    cAB contains the similarities between fB at that position and\n",
    "                    all the features of fA.\n",
    "                </p>\n",
    "                <p>\n",
    "                    As is done in the classical methods for tentative correspondence estimation, it is important to postprocess the\n",
    "                    pairwise similarity scores to remove ambiguous matches.\n",
    "                    To this end, we apply a channel-wise normalization of the\n",
    "                    correlation map at each spatial location to produce the final tentative correspondence map fAB. The normalization\n",
    "                    is performed by ReLU, to zero out negative correlations,\n",
    "                    followed by L2-normalization, which has two desirable effects. First, let us consider the case when descriptor fB correlates well with only a single feature in fA. In this case,\n",
    "                    the normalization will amplify the score of the match, akin\n",
    "                    to the nearest neighbor matching in classical geometry estimation. Second, in the case of the descriptor fB matching\n",
    "                    multiple features in fA due to the existence of clutter or\n",
    "                    repetitive patterns, matching scores will be down-weighted\n",
    "                    similarly to the second nearest neighbor test [38]. However,\n",
    "                    note that both the correlation and the normalization operations are differentiable with respect to the input descriptors,\n",
    "                    which facilitates backpropagation thus enabling end-to-end\n",
    "                    learning.\n",
    "                </p>\n",
    "                <p>\n",
    "                    <strong>Discussion.</strong> The first step of our matching layer, namely\n",
    "                    the correlation layer, is somewhat similar to layers used in\n",
    "                    DeepMatching [50] and FlowNet [17]. However, DeepMatching [50] only uses deep RGB patches and no part\n",
    "                    of their architecture is trainable. FlowNet [17] uses a spatially constrained correlation layer such that similarities are\n",
    "                    are only computed in a restricted spatial neighborhood thus\n",
    "                    limiting the range of geometric transformations that can be\n",
    "                    captured. This is acceptable for their task of learning to estimate optical flow, but is inappropriate for larger transformations that we consider in this work. Furthermore, neither\n",
    "                    of these methods performs score normalization, which we\n",
    "                    find to be crucial in dealing with cluttered scenes.\n",
    "                </p>\n",
    "                <p>\n",
    "                    Previous works have used other matching layers to combine descriptors across images, namely simple concatenation of descriptors along the channel dimension [14] or subtraction [29]. However, these approaches suffer from two\n",
    "                    problems. First, as following layers are typically convolutional, these methods also struggle to handle large transformations as they are unable to detect long-range matches.\n",
    "                    Second, when concatenating or subtracting descriptors, instead of computing pairwise descriptor similarities as is\n",
    "                    commonly done in classical geometry estimation and mimicked by the correlation layer, image content information\n",
    "                    is directly outputted. To further illustrate why this can be\n",
    "                    problematic, consider two pairs of images that are related\n",
    "                    with the same geometric transformation – the concatenation\n",
    "                    and subtraction strategies will produce different outputs for\n",
    "                    the two cases, making it hard for the regressor to deduce the geometric transformation. In contrast, the correlation layer\n",
    "                    output is likely to produce similar correlation maps for the\n",
    "                    two cases, regardless of the image content, thus simplifying the problem for the regressor. In line with this intuition,\n",
    "                    in Sec. 5.5 we show that the concatenation and subtraction\n",
    "                    methods indeed have difficulties generalizing beyond the\n",
    "                    training set, while our correlation layer achieves generalization yielding superior results.\n",
    "                </p>\n",
    "            </td>\n",
    "        </tr>\n",
    "    </tbody>\n",
    "</table>"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "f8574845d26991fb924b9b73a047d47daa16a02e6e1ac35bb3c12f8621974ea3"
  },
  "kernelspec": {
   "display_name": "Python 3.6.13 64-bit ('buddhalight3.6': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.6.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
