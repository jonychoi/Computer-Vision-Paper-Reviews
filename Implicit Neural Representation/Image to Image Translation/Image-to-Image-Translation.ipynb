{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Image to Image Translation**\n",
    "\n",
    "Here, we are going to learn about the basic concept of image to image translation (I2I), with the key methods and backgrounds of the important and current state of the arts researches.\n",
    "\n",
    "**Edited By Su Hyung Choi (Key Summary & Code Practice)**\n",
    "\n",
    "If you have any issues on this scripts, please PR to the repository below.\n",
    "\n",
    "**[Github: @JonyChoi - Computer Vision Paper Reviews]** https://github.com/jonychoi/Computer-Vision-Paper-Reviews\n",
    "\n",
    "Edited Feb 19 2022\n",
    "\n",
    "---\n",
    "\n",
    "### **Index**\n",
    "\n",
    "#### **1. Image to Image Translation**\n",
    "- Pix2Pix\n",
    "- CycleGan\n",
    "\n",
    "#### **2. Multi-Domain Image to Image Translation**\n",
    "- StarGAN\n",
    "- RelGAN\n",
    "\n",
    "\n",
    "#### **3. Unsupervised Image to Image Translation**\n",
    "- UNIT\n",
    "- MUNIT\n",
    "- FUNIT\n",
    "\n",
    "#### **4. Others**\n",
    "- CUT\n",
    "- Swapping AutoEncoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Image to Image Translation**\n",
    "---\n",
    "\n",
    "\n",
    "### **1. Background of appearance**\n",
    "\n",
    "Image-to-image translation (I2I) aims to transfer images from a source domain to a target domain while preserving the content representations. I2I has drawn increasing attention and made tremendous progress in recent years because of its wide range of applications in many computer vision and image processing problems, such as image synthesis, segmentation, style transfer, restoration, and pose estimation.\n",
    "\n",
    "*Reference: Image-to-Image Translation: Methods and Applications - CVPR 2021, Yingxue Pang et al. https://arxiv.org/pdf/2101.08629.pdf*\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Pix2Pix: Image-to-Image Translation with Conditional Adversarial Networks**\n",
    "\n",
    "*Authors: Phillip Isola, Jun-Yan Zhu, Tinghui Zhou, Alexei A. Efros* https://arxiv.org/abs/1611.07004\n",
    "\n",
    "***Abstract***\n",
    "\n",
    "We investigate conditional adversarial networks as a general-purpose solution to image-to-image translation problems. These networks not only learn the mapping from input image to output image, but also learn a loss function to train this mapping. This makes it possible to apply the same generic approach to problems that traditionally would require very different loss formulations. We demonstrate that this approach is effective at synthesizing photos from label maps, reconstructing objects from edge maps, and colorizing images, among other tasks. Indeed, since the release of the pix2pix software associated with this paper, a large number of internet users (many of them artists) have posted their own experiments with our system, further demonstrating its wide applicability and ease of adoption without the need for parameter tweaking. As a community, we no longer hand-engineer our mapping functions, and this work suggests we can achieve reasonable results without hand-engineering our loss functions either.\n",
    "\n",
    "---\n",
    "\n",
    "#### **1. Overview**\n",
    "\n",
    "#### **2. Previous Problem**\n",
    "\n",
    "#### **3. Contribution**\n",
    "\n",
    "#### **4. Keywords**\n",
    "\n",
    "#### **5. Methods**\n",
    "\n",
    "#### **6. Conclusion**\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Multi-Domain Image to Image Translation**\n",
    "---\n",
    "\n",
    "### **1. Background of appearance**\n",
    "\n",
    "- Existing image-to-image translation studies (Pix2Pix, CycleGAN, DiscoGAN, etc.) do not operate reliably in more than three domains.\n",
    "- There is a disadvantage in that the number of generators increases depending on the number of domains. (If k is the number of domains, learn k(k-1) generators.)\n",
    "\n",
    "*Reference: https://velog.io/@tobigs-gm1/Multidomain-ImageTranslation*\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **StarGAN :Unified Generative Adversarial Networks for Multi-Domain Image-to-Image Translation**\n",
    "\n",
    "*Authors: Yunjey Choi, Minje Choi, Munyoung Kim, Jung-Woo Ha, Sunghun Kim, Jaegul Choo* - CVPR 2018, [Arxiv](https://arxiv.org/abs/1711.09020)\n",
    "\n",
    "**Abstract**\n",
    "\n",
    "Recent studies have shown remarkable success in image-to-image translation for two domains. However, existing\n",
    "approaches have limited scalability and robustness in handling more than two domains, since different models should\n",
    "be built independently for every pair of image domains. To\n",
    "address this limitation, we propose StarGAN, a novel and\n",
    "scalable approach that can perform image-to-image translations for multiple domains using only a single model.\n",
    "Such a unified model architecture of StarGAN allows simultaneous training of multiple datasets with different domains\n",
    "within a single network. This leads to StarGANâ€™s superior\n",
    "quality of translated images compared to existing models as\n",
    "well as the novel capability of flexibly translating an input\n",
    "image to any desired target domain. We empirically demonstrate the effectiveness of our approach on a facial attribute\n",
    "transfer and a facial expression synthesis tasks.\n",
    "\n",
    "---\n",
    "\n",
    "#### **1. Overview**\n",
    "\n",
    "#### **2. Previous Problem**\n",
    "\n",
    "#### **3. Contribution**\n",
    "\n",
    "#### **4. Keywords**\n",
    "\n",
    "#### **5. Methods**\n",
    "\n",
    "#### **6. Conclusion**\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **RelGAN: Multi-Domain Image-to-Image Translation via Relative Attributes**\n",
    "---\n",
    "\n",
    "#### **1. Overview**\n",
    "\n",
    "#### **2. Previous Problem**\n",
    "\n",
    "#### **3. Contribution**\n",
    "\n",
    "#### **4. Keywords**\n",
    "\n",
    "#### **5. Methods**\n",
    "\n",
    "#### **6. Conclusion**\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Unsupervised Image to Image Translation**\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **UNIT: Unsupervised Image-to-Image Translation Networks**\n",
    "\n",
    "*Authors: Ming-Yu Liu, Thomas Breuel, Jan Kautz* - CVPR 2017, [Arxiv](https://arxiv.org/abs/1703.00848)\n",
    "\n",
    "**Abstract**\n",
    "\n",
    "Unsupervised image-to-image translation aims at learning a joint distribution of images in different domains by using images from the marginal distributions in individual domains. Since there exists an infinite set of joint distributions that can arrive the given marginal distributions, one could infer nothing about the joint distribution from the marginal distributions without additional assumptions. To address the problem, we make a shared-latent space assumption and propose an unsupervised image-to-image translation framework based on Coupled GANs. We compare the proposed framework with competing approaches and present high quality image translation results on various challenging unsupervised image translation tasks, including street scene image translation, animal image translation, and face image translation. We also apply the proposed framework to domain adaptation and achieve state-of-the-art performance on benchmark datasets. Code and additional results are available in [this https URL](https://github.com/mingyuliutw/unit).\n",
    "\n",
    "#### **1. Overview**\n",
    "\n",
    "#### **2. Previous Problem**\n",
    "\n",
    "#### **3. Contribution**\n",
    "\n",
    "#### **4. Keywords**\n",
    "\n",
    "#### **5. Methods**\n",
    "\n",
    "#### **6. Conclusion**\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **MUNIT: Multimodal Unsupervised Image-to-Image Translation**\n",
    "\n",
    "*Authors: Xun Huang, Ming-Yu Liu, Serge Belongie, Jan Kautz* - CVPR 2018, [Arxiv](https://arxiv.org/abs/1804.04732)\n",
    "\n",
    "**Abstract**\n",
    "\n",
    "Unsupervised image-to-image translation is an important and challenging problem in computer vision. Given an image in the source domain, the goal is to learn the conditional distribution of corresponding images in the target domain, without seeing any pairs of corresponding images. While this conditional distribution is inherently multimodal, existing approaches make an overly simplified assumption, modeling it as a deterministic one-to-one mapping. As a result, they fail to generate diverse outputs from a given source domain image. To address this limitation, we propose a Multimodal Unsupervised Image-to-image Translation (MUNIT) framework. We assume that the image representation can be decomposed into a content code that is domain-invariant, and a style code that captures domain-specific properties. To translate an image to another domain, we recombine its content code with a random style code sampled from the style space of the target domain. We analyze the proposed framework and establish several theoretical results. Extensive experiments with comparisons to the state-of-the-art approaches further demonstrates the advantage of the proposed framework. Moreover, our framework allows users to control the style of translation outputs by providing an example style image. Code and pretrained models are available at [this https URL](https://arxiv.org/abs/1804.04732)\n",
    "\n",
    "#### **1. Overview**\n",
    "\n",
    "#### **2. Previous Problem**\n",
    "\n",
    "#### **3. Contribution**\n",
    "\n",
    "#### **4. Keywords**\n",
    "\n",
    "#### **5. Methods**\n",
    "\n",
    "#### **6. Conclusion**\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **FUNIT: Few-Shot Unsupervised Image-to-Image Translation**\n",
    "\n",
    "*Authors: Ming-Yu Liu, Xun Huang, Arun Mallya, Tero Karras, Timo Aila, Jaakko Lehtinen, Jan Kautz\n",
    "Unsupervised image-to-image translation methods learn to map images in a given class to an analogous image in a different class, drawing on unstructured (non-registered) datasets of images. While remarkably successful, current methods require access to many images in both source and destination classes at training time. We argue this greatly limits their use. Drawing inspiration from the human capability of picking up the essence of a novel object from a small number of examples and generalizing from there, we seek a few-shot, unsupervised image-to-image translation algorithm that works on previously unseen target classes that are specified, at test time, only by a few example images. Our model achieves this few-shot generation capability by coupling an adversarial training scheme with a novel network design. Through extensive experimental validation and comparisons to several baseline methods on benchmark datasets, we verify the effectiveness of the proposed framework. Our implementation and datasets are available at this https URL .\n",
    "\n",
    "#### **1. Overview**\n",
    "\n",
    "#### **2. Previous Problem**\n",
    "\n",
    "#### **3. Contribution**\n",
    "\n",
    "#### **4. Keywords**\n",
    "\n",
    "#### **5. Methods**\n",
    "\n",
    "#### **6. Conclusion**\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **TUNIT: Rethinking the Truly Unsupervised Image-to-Image Translation**\n",
    "\n",
    "#### **1. Overview**\n",
    "\n",
    "#### **2. Previous Problem**\n",
    "\n",
    "#### **3. Contribution**\n",
    "\n",
    "#### **4. Keywords**\n",
    "\n",
    "#### **5. Methods**\n",
    "\n",
    "#### **6. Conclusion**\n",
    "\n",
    "---"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "3efb3a3e09a38f93b1d9bfa4b9b8ce66a167f6ada77b64bc003d7d6d298d81d5"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 ('buddhalight')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
