{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Image to Image Translation**\n",
    "\n",
    "Su Hyung Choi\n",
    "\n",
    "Related Works"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "##### **Image-to-image translation related work flow**\n",
    "\n",
    "- **Image-to-image translation**: \n",
    "    - multi-modal across multi-domain을 위해 i2i를 처음 제시한 conditional class label부터 ~까지 user에게 다양한 interface를 통해 i2i를 manipulation하는 방향으로 진화되고 있음.\n",
    "    - Earlier image-to-image translation (supervised)\n",
    "\n",
    "- **Unsupervised image-to-image translation**\n",
    "\n",
    "- **Few-shot unsupervised image-to-image translation**\n",
    "\n",
    "##### **Below are the paper list ordered by the flow of writes**\n",
    "\n",
    "**Aspects of Image-to-image translation: user interfacable to Multi-modal across Multi-Domain**\n",
    "\n",
    "- Pix2Pix: \n",
    "- DALL.E: \n",
    "\n",
    "**Supervised/Earlier Image-to-Image translation**\n",
    "\n",
    "- DRPAN:\n",
    "- SelectionGAN:\n",
    "- SPADE: \n",
    "- CoCosNet:\n",
    "- ASAPNet:\n",
    "\n",
    "**Unsupervised Image-to-image translation**\n",
    "\n",
    "**Few-shot unsupervised image-to-image translation**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Related works**\n",
    "\n",
    "**Image-to-image translation**. The numerous of image-to-image translation works has appeared due to its million potentials to be adopted across image synthesis, segmentation, style transfer, restoration etc. By enabling translating images to the desired output, the image-to-image translation works appeared the aspects of its evolution by supporting various human interfered interfaces from conditional class input [Pix2Pix], recently to text-prompt setting and [DALL.E] that enabled multi-modal synthesis across multi-domains with manipulation that gradually engaging the wide range of various fields such as computer vision and natural language processing.\n",
    "\n",
    "The earlier I2I works, various image-to-image translation methods have proposed [DRPAN, SelectionGAN, SPADE, CoCosNet, ASAPNet] and the additional works [BicycleGAN, PixelNN] enabled the multi-modal that varies diversity of translated images between the source domain and target domain.\n",
    "\n",
    "**Unsupervised Image-to-image translation**. However the earlier I2I works based on supervision setting that are trained on the aligned image pairs has reached the limitation due to its difficulty of acquiring the large paried training images and its high cost of obtaining the dataset. For overcome those limitations, the unsupervised image-to-image translation has proposed [] that enable learning conditional image generation that maps the input-image of source class to output-image of target class without supervision and those approach have held the dominate position to its I2I field. The cycle consistency constraint has inducted many \n",
    "\n",
    "Unlike the previous works tackled on two domains - a source and target domain - and not stably functioned beyond the two, the unsupervised Image-to-image translation has extended to multi-domain generation which enabling scalable translation to three or more various targets. the shared-latent space assumption proposed [] by et al. explained the \n",
    "\n",
    "**Few-shot Unsupervised Image-to-image translation**. Beyond the unsupervised image-to-image translation, the few-shot approach has proposed from the "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
