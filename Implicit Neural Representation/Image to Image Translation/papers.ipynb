{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Image to Image Translation**\n",
    "\n",
    "Su Hyung Choi\n",
    "\n",
    "Related Works"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "##### **Image-to-image translation related work flow**\n",
    "\n",
    "Truly Unsupervised + CLIP guidance이기 때문에 최대한 제 생각에서 이에 맞게 써봤습니다.\n",
    "\n",
    "- **Image-to-image translation**: \n",
    "    - I2I has appeared its aspects of evolution to support various interfaces for users to enable multi-modal across multi-domains\n",
    "    - Earlier image-to-image translation (supervised)\n",
    "<br></br>\n",
    "- **Unsupervised image-to-image translation**\n",
    "\n",
    "- **Few-shot unsupervised image-to-image translation**\n",
    "\n",
    "##### **Below are the paper list ordered by the flow of writes**\n",
    "\n",
    "**Aspects of Image-to-image translation: user interfacable to Multi-modal across Multi-Domain**\n",
    "\n",
    "- Pix2Pix: \n",
    "- StyleCLIP: \n",
    "\n",
    "**Supervised/Earlier Image-to-Image translation**\n",
    "\n",
    "- DRPAN:\n",
    "- SelectionGAN:\n",
    "- SPADE: \n",
    "- CoCosNet:\n",
    "- ASAPNet:\n",
    "\n",
    "**Unsupervised Image-to-image translation**\n",
    "\n",
    "**Two Domain I2I**\n",
    "\n",
    "Using Cycle Consistency Constraints\n",
    "\n",
    "- DTN:\n",
    "- DualGAN:\n",
    "- DiscoGAN:\n",
    "- CycleGAN:\n",
    "- UNIT:\n",
    "\n",
    "Beyond Cycle Consistency Constraints\n",
    "\n",
    "- DistanceGAN:\n",
    "- GANimorph:\n",
    "- GCGAN:\n",
    "- CUT:\n",
    "- LPTN:\n",
    "\n",
    "**Multi Domain I2I**\n",
    "\n",
    "- StarGAN:\n",
    "- AttGAN:\n",
    "- RelGAN:\n",
    "- STGAN:\n",
    "- CollaGAN:\n",
    "\n",
    "**Few-shot unsupervised image-to-image translation**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Related works**\n",
    "\n",
    "**Image-to-image translation**. The numerous of image-to-image translation works has appeared due to its multiple potential usages across image synthesis, segmentation, style transfer, restoration etc. Enabling translating images to the desired output, the image-to-image translation works has appeared its aspects of evolution by supporting various human interfered interfaces from conditional class input [Pix2Pix], recently to text-prompt input [StyleCLIP] that enabled multi-modal synthesis across multi-domains gradually engaging the wide range of various fields such as computer vision and natural language processing.\n",
    "\n",
    "The earlier I2I works, various image-to-image translation methods have proposed [DRPAN, SelectionGAN, SPADE, CoCosNet, ASAPNet] and the additional works [BicycleGAN, PixelNN] enabled the multi-modal that varies diversity of translated images between the source domain and target domain.\n",
    "\n",
    "**Unsupervised Image-to-image translation**. However the earlier I2I works based on supervision setting that are trained on the aligned image pairs has reached the limitation due to its difficulty of acquiring the large paried training images and its high cost of obtaining the dataset. To overcome those limitations, the cycle-consistency constraint proposed and inducted in many methods [DTN, DualGAN, DiscoGAN, CycleGAN, UNIT] that enable training without supervised paired images. However the cycle-consistency constraint tends to work unsuccessful when two domains are heterogeneity, when they have large shape and context differences between domains, [DistanceGAN, GANimorph, GCGAN, CUT, LPTN] are proposed to solve the heterogeneity problem resulting well as not only preserving semantic information but also translating images realistic sufficiently.\n",
    "Unlike the previous works tackled on two domains and not stably functioned beyond the two, the unsupervised Image-to-image translation has extended to multi-domain generation which enabling scalable translation to three or more various targets. [StarGAN] [AttGAN, RelGAN, STGAN, CollaGAN]\n",
    "\n",
    "**Few-shot Unsupervised Image-to-image translation**. Even in an unsupervised set, there are challenges to collect the data to training the network, that is still a bottleneck with extremely limited data to task image-to-image translation. Beyond the unsupervised methods, the few-shot approach has proposed from the "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "3efb3a3e09a38f93b1d9bfa4b9b8ce66a167f6ada77b64bc003d7d6d298d81d5"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 ('buddhalight')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
