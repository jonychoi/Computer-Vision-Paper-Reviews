{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Image to Image Translation**\n",
    "\n",
    "Su Hyung Choi\n",
    "\n",
    "Related Works"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "##### **Image-to-image translation related work flow**\n",
    "\n",
    "Truly Unsupervised + CLIP guidance이기 때문에 최대한 제 생각에서 이에 맞게 써봤습니다.\n",
    "\n",
    "- **Image-to-image translation**: \n",
    "    - I2I has appeared its aspects of evolution to support various interfaces for users to enable multi-modal across multi-domains\n",
    "    - Earlier image-to-image translation (supervised)\n",
    "<br></br>\n",
    "- **Unsupervised image-to-image translation**\n",
    "\n",
    "- **Few-shot unsupervised image-to-image translation**\n",
    "\n",
    "##### **Below are the paper list ordered by the flow of writes**\n",
    "\n",
    "**Aspects of Image-to-image translation: user interfacable to Multi-modal across Multi-Domain**\n",
    "\n",
    "- [Pix2Pix]: P. Isola, J.-Y. Zhu, T. Zhou, and A. A. Efros, “Image-to-image\n",
    "translation with conditional adversarial networks,” in Proceedings of\n",
    "19\n",
    "the IEEE conference on computer vision and pattern recognition, 2017,\n",
    "pp. 1125–1134.\n",
    "- [StyleCLIP]: Or Patashnik, Zongze Wu, Eli Shechtman, Daniel\n",
    "Cohen-Or, and Dani Lischinski. Styleclip: Text-driven\n",
    "manipulation of stylegan imagery. arXiv preprint\n",
    "arXiv:2103.17249, 2021.\n",
    "\n",
    "\n",
    "**Supervised/Earlier Image-to-Image translation**\n",
    "\n",
    "- [DRPAN]: C. Wang, H. Zheng, Z. Yu, Z. Zheng, Z. Gu, and B. Zheng, “Discriminative region proposal adversarial networks for high-quality imageto-image translation,” in Proceedings of the European Conference on\n",
    "Computer Vision (ECCV), September 2018.\n",
    "- [SelectionGAN]: H. Tang, D. Xu, N. Sebe, Y. Wang, J. J. Corso, and Y. Yan, “Multichannel attention selection gan with cascaded semantic guidance for\n",
    "cross-view image translation,” in Proceedings of the IEEE Conference\n",
    "on Computer Vision and Pattern Recognition, 2019, pp. 2417–2426.\n",
    "- [SPADE]: T. Park, M.-Y. Liu, T.-C. Wang, and J.-Y. Zhu, “Semantic image\n",
    "synthesis with spatially-adaptive normalization,” in Proceedings of the\n",
    "IEEE/CVF Conference on Computer Vision and Pattern Recognition\n",
    "(CVPR), June 2019.\n",
    "- [CoCosNet]: P. Zhang, B. Zhang, D. Chen, L. Yuan, and F. Wen, “Cross-domain\n",
    "correspondence learning for exemplar-based image translation,” in\n",
    "Proceedings of the IEEE/CVF Conference on Computer Vision and\n",
    "Pattern Recognition, 2020, pp. 5143–5153.\n",
    "- [ASAPNet]: \"Spatially-Adaptive Pixelwise Networks for Fast Image Translation\"\n",
    "\n",
    "**Unsupervised Image-to-image translation**\n",
    "\n",
    "**Two Domain I2I**\n",
    "\n",
    "Using Cycle Consistency Constraints\n",
    "\n",
    "- [DTN]: Y. Taigman, A. Polyak, and L. Wolf, “Unsupervised cross-domain\n",
    "image generation,” in 5th International Conference on Learning Representations, ICLR 2017, Toulon, France, April 24-26, 2017, Conference\n",
    "Track Proceedings\n",
    "- [DualGAN]: Z. Yi, H. Zhang, P. Tan, and M. Gong, “Dualgan: Unsupervised dual\n",
    "learning for image-to-image translation,” in Proceedings of the IEEE\n",
    "international conference on computer vision, 2017, pp. 2849–2857.\n",
    "- [DiscoGAN]: T. Kim, M. Cha, H. Kim, J. K. Lee, and J. Kim, “Learning to\n",
    "discover cross-domain relations with generative adversarial networks,”\n",
    "in International Conference on Machine Learning. PMLR, 2017, pp.\n",
    "1857–1865\n",
    "- [CycleGAN]: J.-Y. Zhu, T. Park, P. Isola, and A. A. Efros, “Unpaired image-to-image\n",
    "translation using cycle-consistent adversarial networks,” in Proceedings\n",
    "of the IEEE international conference on computer vision, 2017, pp.\n",
    "2223–2232.\n",
    "- [UNIT]: M.-Y. Liu, T. Breuel, and J. Kautz, “Unsupervised image-to-image\n",
    "translation networks,” in Advances in neural information processing\n",
    "systems, 2017, pp. 700–708\n",
    "\n",
    "Beyond Cycle Consistency Constraints\n",
    "\n",
    "- [DistanceGAN]: S. Benaim and L. Wolf, “One-sided unsupervised domain mapping,” in\n",
    "Advances in Neural Information Processing Systems, I. Guyon, U. V.\n",
    "Luxburg, S. Bengio, H. Wallach, R. Fergus, S. Vishwanathan, and\n",
    "R. Garnett, Eds., vol. 30. Curran Associates, Inc., 2017.\n",
    "- [GANimorph]: A. Gokaslan, V. Ramanujan, D. Ritchie, K. In Kim, and J. Tompkin,\n",
    "“Improving shape deformation in unsupervised image-to-image translation,” in Proceedings of the European Conference on Computer Vision\n",
    "(ECCV), 2018, pp. 649–665.\n",
    "- [GCGAN]: H. Fu, M. Gong, C. Wang, K. Batmanghelich, K. Zhang, and D. Tao,\n",
    "“Geometry-consistent generative adversarial networks for one-sided\n",
    "unsupervised domain mapping,” in Proceedings of the IEEE/CVF\n",
    "Conference on Computer Vision and Pattern Recognition, 2019, pp.\n",
    "2427–2436.\n",
    "- [CUT]: T. Park, A. A. Efros, R. Zhang, and J.-Y. Zhu, “Contrastive learning\n",
    "for unpaired image-to-image translation,” in European Conference on\n",
    "Computer Vision. Springer, 2020, pp. 319–345.\n",
    "- [LPTN]: J. Liang, H. Zeng, and L. Zhang, “High-resolution photorealistic image\n",
    "translation in real-time: A laplacian pyramid translation network,”\n",
    "arXiv preprint arXiv:2105.09188, 2021.\n",
    "\n",
    "\n",
    "**Multi Domain I2I**\n",
    "\n",
    "- [StarGAN]: Y. Choi, M. Choi, M. Kim, J.-W. Ha, S. Kim, and J. Choo, “Stargan:\n",
    "Unified generative adversarial networks for multi-domain image-toimage translation,” in Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR), June 2018.\n",
    "- [AttGAN]: Z. He, W. Zuo, M. Kan, S. Shan, and X. Chen, “Attgan: Facial attribute\n",
    "editing by only changing what you want,” IEEE Transactions on Image\n",
    "Processing, vol. 28, no. 11, pp. 5464–5478, 2019.\n",
    "- RelGAN: P.-W. Wu, Y.-J. Lin, C.-H. Chang, E. Y. Chang, and S.-W. Liao,\n",
    "“Relgan: Multi-domain image-to-image translation via relative attributes,” in Proceedings of the IEEE/CVF International Conference\n",
    "on Computer Vision (ICCV), October 2019\n",
    "- [STGAN]: M. Liu, Y. Ding, M. Xia, X. Liu, E. Ding, W. Zuo, and S. Wen, “Stgan:\n",
    "A unified selective transfer network for arbitrary image attribute\n",
    "editing,” in Proceedings of the IEEE/CVF Conference on Computer\n",
    "Vision and Pattern Recognition (CVPR), June 2019.\n",
    "- [CollaGAN]: D. Lee, J. Kim, W.-J. Moon, and J. C. Ye, “Collagan: Collaborative gan\n",
    "for missing image data imputation,” in Proceedings of the IEEE/CVF\n",
    "Conference on Computer Vision and Pattern Recognition (CVPR), June\n",
    "2019\n",
    "\n",
    "Semi-supervised I2I\n",
    "\n",
    "- [AGUIT]: X. Li, J. Hu, S. Zhang, X. Hong, Q. Ye, C. Wu, and R. Ji,\n",
    "“Attribute guided unpaired image-to-image translation with semisupervised learning,” arXiv preprint arXiv:1904.12428, 2019.\n",
    "\n",
    "- [ADAIN]: X. Huang and S. Belongie, “Arbitrary style transfer in real-time\n",
    "with adaptive instance normalization,” in Proceedings of the IEEE\n",
    "International Conference on Computer Vision, 2017, pp. 1501–1510.\n",
    "\n",
    "**Few-shot unsupervised image-to-image translation**\n",
    "\n",
    "- [FUNIT]: M.-Y. Liu, X. Huang, A. Mallya, T. Karras, T. Aila, J. Lehtinen,\n",
    "and J. Kautz, “Few-shot unsupervised image-to-image translation,” in\n",
    "Proceedings of the IEEE/CVF International Conference on Computer\n",
    "Vision (ICCV), October 2019.\n",
    "- [COCO-FUNIT]: K. Saito, K. Saenko, and M.-Y. Liu, “Coco-funit: Few-shot unsupervised image translation with a content conditioned style encoder,” in\n",
    "Computer Vision – ECCV 2020, A. Vedaldi, H. Bischof, T. Brox, and\n",
    "J.-M. Frahm, Eds. Cham: Springer International Publishing, 2020, pp.\n",
    "382–398.\n",
    "- [TUNIT]: K. Baek, Y. Choi, Y. Uh, J. Yoo and H. Shim, \"Appendix: Rethinking the Truly Unsupervised Image-to-Image Translation\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Related works**\n",
    "\n",
    "**Image-to-image translation**. The numerous of image-to-image translation works has appeared due to its multiple potential usages across image synthesis, segmentation, style transfer, restoration etc. Enabling translating images to the desired output, the image-to-image translation works has appeared its aspects of evolution by supporting various human interfered interfaces from conditional class input [Pix2Pix], recently to text-prompt input [StyleCLIP] that enabled multi-modal synthesis across multi-domains gradually engaging the wide range of various fields such as computer vision and natural language processing.\n",
    "\n",
    "The earlier I2I works, various image-to-image translation methods have proposed [DRPAN, SelectionGAN, SPADE, CoCosNet, ASAPNet] and the additional works [BicycleGAN, PixelNN] enabled the multi-modal that varies diversity of translated images between the source domain and target domain.\n",
    "\n",
    "**Unsupervised Image-to-image translation**. However the earlier I2I works based on supervision setting that are trained on the aligned image pairs has reached the limitation due to its difficulty of acquiring the large paried training images and its high cost of obtaining the dataset. To overcome those limitations, the cycle-consistency constraint proposed and inducted in many methods [DTN, DualGAN, DiscoGAN, CycleGAN, UNIT] that enable training without supervised paired images. However the cycle-consistency constraint tends to work unsuccessful when two domains are heterogeneity, when they have large shape and context differences between domains, [DistanceGAN, GANimorph, GCGAN, CUT, LPTN] are proposed to solve the heterogeneity problem resulting well as not only preserving semantic information but also translating images realistic sufficiently.\n",
    "Unlike the previous works tackled on two domains and not stably functioned beyond the two, the unsupervised Image-to-image translation has extended to multi-domain generation which enabling scalable translation to three or more various targets. [StarGAN] enabled training with one generator and discriminator pair introducing  an auxiliary classifier and mask vector. The similar ideas was adopted in [AttGAN, RelGAN, STGAN, CollaGAN] and progressed remarkable resutls as well. The semi-supervised multi domain i2i tasks are also introduced to handle multi-modal across multi-domains, [AGUIT] used representation decomposition to extract content and style features and [Adain] to reconstruct and translate with cycle consistency loss.\n",
    "\n",
    "**Few-shot Unsupervised Image-to-image translation**. Even in an unsupervised set, there are challenges to collect the data to training the network, that is still a bottleneck with extremely limited data to task image-to-image translation. Beyond the unsupervised methods, the few-shot approach has proposed, [FUNIT] successfully translate images from a source label to target labels with limited data. [COCO-FUNIT] redesigned a content-conditioned style encoder to address the content loss problem of previous FUNIT, gain the fine-results. However, [TUNIT] addressed the pseudo labeling issue arguing that FUNIT still needs labels for training, proposing the truly unsupervised image-to-image translation model with introducing guiding network."
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "3efb3a3e09a38f93b1d9bfa4b9b8ce66a167f6ada77b64bc003d7d6d298d81d5"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 ('buddhalight')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
