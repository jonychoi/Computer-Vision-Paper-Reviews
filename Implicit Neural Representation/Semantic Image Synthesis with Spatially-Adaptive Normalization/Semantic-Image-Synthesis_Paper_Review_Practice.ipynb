{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Semantic Image Synthesis with Spatially-Adaptive Normalization**\n",
    "\n",
    "**Authors: Taesung Park, Ming-Yu Liu, Ting-Chun Wang, Jun-Yan ZhuL**\n",
    "\n",
    "**Original Paper**: https://arxiv.org/pdf/1903.07291.pdf\n",
    "\n",
    "**Official Github**: https://github.com/NVlabs/SPADE\n",
    "\n",
    "---\n",
    "\n",
    "**Edited By Su Hyung Choi (Key Summary & Code Practice)**\n",
    "\n",
    "If you have any issues on this scripts, please PR to the repository below.\n",
    "\n",
    "**[Github: @JonyChoi - Computer Vision Paper Reviews]** https://github.com/jonychoi/Computer-Vision-Paper-Reviews\n",
    "\n",
    "Edited Jan 16 2022\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./imgs/figure1.png\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Abstract**\n",
    "<p>\n",
    "We propose spatially-adaptive normalization, a simple\n",
    "but effective layer for synthesizing photorealistic images\n",
    "given an input semantic layout. Previous methods directly\n",
    "feed the semantic layout as input to the deep network, which\n",
    "is then processed through stacks of convolution, normalization, and nonlinearity layers. We show that this is suboptimal as the normalization layers tend to “wash away” semantic information. To address the issue, we propose using\n",
    "the input layout for modulating the activations in normalization layers through a spatially-adaptive, learned transformation. Experiments on several challenging datasets\n",
    "demonstrate the advantage of the proposed method over existing approaches, regarding both visual fidelity and alignment with input layouts. Finally, our model allows user\n",
    "control over both semantic and style. Code is available at https://github.com/NVlabs/SPADE.\n",
    "</p>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **1. Introduction**\n",
    "<p>\n",
    "Conditional image synthesis refers to the task of generating photorealistic images conditioning on certain input data. Seminal work computes the output image by\n",
    "stitching pieces from a single image (e.g., Image Analogies [16]) or using an image collection [7, 14, 23, 30, 35].\n",
    "Recent methods directly learn the mapping using neural networks [3, 6, 22, 47, 48, 54, 55, 56]. The latter methods are\n",
    "faster and require no external database of images.\n",
    "</p>\n",
    "<p>\n",
    "    We are interested in a specific form of conditional image synthesis, which is converting a semantic segmentation\n",
    "mask to a photorealistic image. This form has a wide range\n",
    "of applications such as content generation and image editing [6, 22, 48]. We refer to this form as semantic image\n",
    "synthesis. In this paper, we show that the conventional network architecture [22, 48], which is built by stacking convolutional, normalization, and nonlinearity layers, is at best arXiv:1903.07291v2 [cs.CV] 5 Nov 2019\n",
    "suboptimal because their normalization layers tend to “wash\n",
    "away” information contained in the input semantic masks.\n",
    "To address the issue, we propose spatially-adaptive normalization, a conditional normalization layer that modulates the\n",
    "activations using input semantic layouts through a spatiallyadaptive, learned transformation and can effectively propagate the semantic information throughout the network.\n",
    "</p>\n",
    "<p>\n",
    "    We conduct experiments on several challenging datasets\n",
    "including the COCO-Stuff [4, 32], the ADE20K [58], and\n",
    "the Cityscapes [9]. We show that with the help of our\n",
    "spatially-adaptive normalization layer, a compact network\n",
    "can synthesize significantly better results compared to several state-of-the-art methods. Additionally, an extensive ablation study demonstrates the effectiveness of the proposed\n",
    "normalization layer against several variants for the semantic\n",
    "image synthesis task. Finally, our method supports multimodal and style-guided image synthesis, enabling controllable, diverse outputs, as shown in Figure 1. Also, please\n",
    "see our SIGGRAPH 2019 Real-Time Live demo and try our\n",
    "online demo by yourself.\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **2. Related Work**\n",
    "<p>\n",
    "<strong>Deep generative models</strong> can learn to synthesize images.\n",
    "Recent methods include generative adversarial networks\n",
    "(GANs) [13] and variational autoencoder (VAE) [28]. Our\n",
    "work is built on GANs but aims for the conditional image\n",
    "synthesis task. The GANs consist of a generator and a discriminator where the goal of the generator is to produce realistic images so that the discriminator cannot tell the synthesized images apart from the real ones.\n",
    "</p>\n",
    "<p>\n",
    "<strong>Conditional image synthesis</strong> exists in many forms that differ in the type of input data. For example, class-conditional\n",
    "models [3, 36, 37, 39, 41] learn to synthesize images given\n",
    "category labels. Researchers have explored various models\n",
    "for generating images based on text [18,44,52,55]. Another\n",
    "widely-used form is image-to-image translation based on a\n",
    "type of conditional GANs [20, 22, 24, 25, 33, 57, 59, 60],\n",
    "where both input and output are images. Compared to\n",
    "earlier non-parametric methods [7, 16, 23], learning-based\n",
    "methods typically run faster during test time and produce\n",
    "more realistic results. In this work, we focus on converting\n",
    "segmentation masks to photorealistic images. We assume\n",
    "the training dataset contains registered segmentation masks\n",
    "and images. With the proposed spatially-adaptive normalization, our compact network achieves better results compared to leading methods.\n",
    "</p>\n",
    "<p>\n",
    "<strong>Unconditional normalization layers</strong> have been an important component in modern deep networks and can be found\n",
    "in various classifiers, including the Local Response Normalization in the AlexNet [29] and the Batch Normalization (BatchNorm) in the Inception-v2 network [21]. Other\n",
    "popular normalization layers include the Instance Normalization (InstanceNorm) [46], the Layer Normalization [2],\n",
    "the Group Normalization [50], and the Weight Normalization [45]. We label these normalization layers as unconditional as they do not depend on external data in contrast to\n",
    "the conditional normalization layers discussed below.\n",
    "</p>\n",
    "<p>\n",
    "<strong>Conditional normalization layers</strong> include the Conditional\n",
    "Batch Normalization (Conditional BatchNorm) [11] and\n",
    "Adaptive Instance Normalization (AdaIN) [19]. Both were\n",
    "first used in the style transfer task and later adopted in various vision tasks [3, 8, 10, 20, 26, 36, 39, 42, 49, 54]. Different from the earlier normalization techniques, conditional normalization layers require external data and generally operate as follows. First, layer activations are normalized to zero mean and unit deviation. Then the normalized activations are denormalized by modulating the\n",
    "activation using a learned affine transformation whose parameters are inferred from external data. For style transfer tasks [11, 19], the affine parameters are used to control\n",
    "the global style of the output, and hence are uniform across\n",
    "spatial coordinates. In contrast, our proposed normalization\n",
    "layer applies a spatially-varying affine transformation, making it suitable for image synthesis from semantic masks.\n",
    "Wang et al. proposed a closely related method for image\n",
    "super-resolution [49]. Both methods are built on spatiallyadaptive modulation layers that condition on semantic inputs. While they aim to incorporate semantic information\n",
    "into super-resolution, our goal is to design a generator for\n",
    "style and semantics disentanglement. We focus on providing the semantic information in the context of modulating\n",
    "normalized activations. We use semantic maps in different\n",
    "scales, which enables coarse-to-fine generation. The reader\n",
    "is encouraged to review their work for more details.\n",
    "</p>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **3. Semantic Image Synthesis**\n",
    "<p>\n",
    "Let m ∈ L\n",
    "H×W be a semantic segmentation mask\n",
    "where L is a set of integers denoting the semantic labels,\n",
    "and H and W are the image height and width. Each entry\n",
    "in m denotes the semantic label of a pixel. We aim to learn\n",
    "a mapping function that can convert an input segmentation\n",
    "mask m to a photorealistic image.\n",
    "</p>\n",
    "<p>\n",
    "<table>\n",
    "    <tbody>\n",
    "        <tr>\n",
    "            <td>\n",
    "                <img src=\"./imgs/figure2.png\" width=\"400\" />\n",
    "            </td>\n",
    "        </tr>\n",
    "    </tbody>\n",
    "</table>\n",
    "<strong>Spatially-adaptive denormalization.</strong> Let h\n",
    "i denote the activations of the i-th layer of a deep convolutional network\n",
    "for a batch of N samples. Let C\n",
    "i be the number of channels in the layer. Let Hi\n",
    "and Wi be the height and width\n",
    "of the activation map in the layer. We propose a new conditional normalization method called the SPatially-Adaptive\n",
    "(DE)normalization (1)\n",
    "(SPADE). Similar to the Batch Normalization [21], the activation is normalized in the channelwise manner and then modulated with learned scale and\n",
    "bias. Figure 2 illustrates the SPADE design. The activation value at site (n ∈ N, c ∈ C\n",
    "i\n",
    ", y ∈ Hi\n",
    ", x ∈ Wi\n",
    ") is\n",
    "</p>\n",
    "<table>\n",
    "    <tbody>\n",
    "        <tr>\n",
    "            <td>\n",
    "                <img src=\"./imgs/equation1.png\" width=\"400\" />\n",
    "            </td>\n",
    "        </tr>\n",
    "    </tbody>\n",
    "</table>\n",
    "<p>\n",
    "where h\n",
    "i\n",
    "n,c,y,x is the activation at the site before normalization and µ\n",
    "i\n",
    "c\n",
    "and σ\n",
    "i\n",
    "c\n",
    "are the mean and standard deviation of\n",
    "the activations in channel c:\n",
    "</p>\n",
    "<table>\n",
    "    <tbody>\n",
    "        <tr>\n",
    "            <td>\n",
    "                <img src=\"./imgs/equation2.png\" width=\"400\" />\n",
    "            </td>\n",
    "            <td>\n",
    "                <img src=\"./imgs/equation3.png\" width=\"400\" />\n",
    "            </td>\n",
    "        </tr>\n",
    "    </tbody>\n",
    "</table>\n",
    "<p>\n",
    "The variables γ\n",
    "i\n",
    "c,y,x(m) and β\n",
    "i\n",
    "c,y,x(m) in (1) are the\n",
    "learned modulation parameters of the normalization layer.\n",
    "In contrast to the BatchNorm [21], they depend on the input segmentation mask and vary with respect to the location\n",
    "(y, x). We use the symbol γ\n",
    "i\n",
    "c,y,x and β\n",
    "i\n",
    "c,y,x to denote the\n",
    "functions that convert m to the scaling and bias values at\n",
    "the site (c, y, x) in the i-th activation map. We implement\n",
    "the functions γ\n",
    "i\n",
    "c,y,x and β\n",
    "i\n",
    "c,y,x using a simple two-layer convolutional network, whose design is in the appendix.\n",
    "</p>\n",
    "<p>\n",
    "In fact, SPADE is related to, and is a generalization\n",
    "of several existing normalization layers. First, replacing\n",
    "the segmentation mask m with the image class label and\n",
    "making the modulation parameters spatially-invariant (i.e.,\n",
    "γ\n",
    "i\n",
    "c,y1,x1 ≡ γ\n",
    "i\n",
    "c,y2,x2\n",
    "and β\n",
    "i\n",
    "c,y1,x1 ≡ β\n",
    "i\n",
    "c,y2,x2\n",
    "for any y1, y2 ∈\n",
    "{1, 2, ..., Hi} and x1, x2 ∈ {1, 2, ..., Wi}), we arrive at the\n",
    "form of the Conditional BatchNorm [11]. Indeed, for any\n",
    "spatially-invariant conditional data, our method reduces to\n",
    "the Conditional BatchNorm. Similarly, we can arrive at\n",
    "the AdaIN [19] by replacing m with a real image, making the modulation parameters spatially-invariant, and setting N = 1. As the modulation parameters are adaptive to\n",
    "the input segmentation mask, the proposed SPADE is better\n",
    "suited for semantic image synthesis.\n",
    "</p>\n",
    "<table>\n",
    "    <tbody>\n",
    "        <tr>\n",
    "            <td>\n",
    "                <img src=\"./imgs/figure3.png\" width=\"400\" />\n",
    "            </td>\n",
    "        </tr>\n",
    "    </tbody>\n",
    "</table>\n",
    "<p>\n",
    "<strong>SPADE generator.</strong> With the SPADE, there is no need to\n",
    "feed the segmentation map to the first layer of the generator, since the learned modulation parameters have encoded\n",
    "enough information about the label layout. Therefore, we\n",
    "discard encoder part of the generator, which is commonly\n",
    "used in recent architectures [22, 48]. This simplification results in a more lightweight network. Furthermore, similarly\n",
    "to existing class-conditional generators [36,39,54], the new\n",
    "generator can take a random vector as input, enabling a simple and natural way for multi-modal synthesis [20, 60].\n",
    "</p>\n",
    "<p>\n",
    "Figure 4 illustrates our generator architecture, which employs several ResNet blocks [15] with upsampling layers.\n",
    "The modulation parameters of all the normalization layers\n",
    "are learned using the SPADE. Since each residual block\n",
    "operates at a different scale, we downsample the semantic\n",
    "mask to match the spatial resolution.\n",
    "</p>\n",
    "<p>\n",
    "We train the generator with the same multi-scale discriminator and loss function used in pix2pixHD [48] except that\n",
    "we replace the least squared loss term [34] with the hinge\n",
    "loss term [31,38,54]. We test several ResNet-based discriminators used in recent unconditional GANs [1, 36, 39] but\n",
    "observe similar results at the cost of a higher GPU memory requirement. Adding the SPADE to the discriminator\n",
    "also yields a similar performance. For the loss function, we\n",
    "observe that removing any loss term in the pix2pixHD loss\n",
    "function lead to degraded generation results.\n",
    "</p>\n",
    "<p>\n",
    "<strong>Why does the SPADE work better?</strong> A short answer is that\n",
    "it can better preserve semantic information against common\n",
    "normalization layers. Specifically, while normalization layers such as the InstanceNorm [46] are essential pieces in\n",
    "almost all the state-of-the-art conditional image synthesis\n",
    "models [48], they tend to wash away semantic information\n",
    "when applied to uniform or flat segmentation masks.\n",
    "</p>\n",
    "---\n",
    "(1) Conditional normalization [11, 19] uses external data to denormalize\n",
    "the normalized activations; i.e., the denormalization part is conditional.\n"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "3efb3a3e09a38f93b1d9bfa4b9b8ce66a167f6ada77b64bc003d7d6d298d81d5"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 64-bit ('buddhalight': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
