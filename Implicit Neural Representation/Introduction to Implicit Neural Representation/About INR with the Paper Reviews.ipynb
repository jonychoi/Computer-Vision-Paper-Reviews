{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **About INR with the Paper Reviews**\n",
    "\n",
    "**Edited By Su Hyung Choi (Key Summary & Code Practice)**\n",
    "\n",
    "If you have any issues on this scripts, please PR to the repository below.\n",
    "\n",
    "**[Github: @JonyChoi - Computer Vision Paper Reviews]** https://github.com/jonychoi/Computer-Vision-Paper-Reviews\n",
    "\n",
    "Edited Jan 16 2022\n",
    "\n",
    "---\n",
    "\n",
    "Here, I wrote the writing contains about the *\"what are Implicit Neural Representations'*, the basic concept of INR, and its potential usage and applications. With the understanding of INR, I introduced the its representative papers within few couples of years to apprehend its trend and evolution history."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Overview**\n",
    "\n",
    "1. The Goal of the Writing\n",
    "\n",
    "2. What are Implicit Neural Representations\n",
    "\n",
    "3. The applications and potentials of INR\n",
    "\n",
    "4. Latest Trend of INR\n",
    "\n",
    "5. Representative INR paper reviews\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **1. The Goal of the Writing**\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **2. What are Implicit Neural Representations?**\n",
    "\n",
    "---\n",
    "\n",
    "#### **Implicit neural representations**\n",
    "\n",
    "***\"Implicit neural representations is about parameterizing a continuous differentiable signal with a neural network. The signal is encoded within the neural network, providing a possibly more compact representation.*** *This is a type of [regression](https://hugocisneros.com/notes/machine_learning/) problem. Applications of these learned representations range from simple [compression](https://hugocisneros.com/notes/compression/), to 3D scene reconstruction from 2D images, semantic information inference, etc.[CPPN](https://hugocisneros.com/notes/cppn/) is an early example of a implicit neural representation implementation mainly used for pattern generation. It uses a neural network to generate patterns parameterized by two (or more) coordinates.\"*\n",
    "\n",
    "***by [Hugo Cisneros](https://hugocisneros.com/notes/implicit_neural_representations/#org36a6837)***\n",
    "\n",
    "***\"Implicit Neural Representations (sometimes also referred to as coordinate-based representations) are a novel way to parameterize signals of all kinds.*** Conventional signal representations are usually discrete - *for instance, images are discrete grids of pixels, audio signals are discrete samples of amplitudes, and 3D shapes are usually parameterized as grids of voxels, point clouds, or meshes. In contrast, Implicit Neural Representations parameterize a signal as a ***continuous function*** that maps the domain of the signal (i.e., a coordinate, such as a pixel coordinate for an image) to whatever is at that coordinate (for an image, an R,G,B color). Of course, these functions are usually not analytically tractable - it is impossible to \"write down\" the function that parameterizes a natural image as a mathematical formula. Implicit Neural Representations thus approximate that function via a neural network.\"*\n",
    "\n",
    "***by [Vincent Sitzman](https://github.com/vsitzmann/awesome-implicit-representations#what-are-implicit-neural-representations)***\n",
    "\n",
    "---\n",
    "\n",
    "#### **Why are they interesting?**\n",
    "\n",
    "Implicit Neural Representations have several benefits: First, they are not coupled to spatial resolution anymore, the way, for instance, an image is coupled to the number of pixels. This is because they are continuous functions! Thus, the memory required to parameterize the signal is independent of spatial resolution, and only scales with the complexity of the underyling signal. Another corollary of this is that implicit representations have \"infinite resolution\" - they can be sampled at arbitrary spatial resolutions.\n",
    "\n",
    "This is immediately useful for a number of applications, such as super-resolution, or in parameterizing signals in 3D and higher dimensions, where memory requirements grow intractably fast with spatial resolution. Further, generalizing across neural implicit representations amounts to learning a prior over a space of functions, implemented via learning a prior over the weights of neural networks - this is commonly referred to as meta-learning and is an extremely exciting intersection of two very active research areas! Another exciting overlap is between neural implicit representations and the study of symmetries in neural network architectures - for intance, creating a neural network architecture that is 3D rotation-equivariant immediately yields a viable path to rotation-equivariant generative models via neural implicit representations.\n",
    "\n",
    "Another key promise of implicit neural representations lie in algorithms that directly operate in the space of these representations. In other words: What's the \"convolutional neural network\" equivalent of a neural network operating on images represented by implicit representations?\n",
    "\n",
    "*Reference from [Vincent Sitzman](https://github.com/vsitzmann/awesome-implicit-representations#what-are-implicit-neural-representations)*\n",
    "\n",
    "---\n",
    "\n",
    "#### **Similar Fields related to INR**\n",
    "\n",
    "***Implicit neural representations for high frequency data***\n",
    "\n",
    "*To encode potentially high frequency data such as sound or images, it is much more efficient to start from periodic feature transformations. This can be achieved with periodic activation functions [(Sitzmann et al. 2020)](https://arxiv.org/abs/2006.09661) or by using a Fourier feature mapping [(Tancik et al. 2020)](https://arxiv.org/abs/2006.10739).*\n",
    "\n",
    "***Neural radiance fields***\n",
    "\n",
    "[(Mildenhall et al. 2020)](https://arxiv.org/abs/2003.08934)\n",
    "\n",
    "*Reference From [Hugo Cisneros](https://hugocisneros.com/notes/implicit_neural_representations/#org36a6837)*\n",
    "\n",
    "---\n",
    "\n",
    "#### **Here's my thought.**\n",
    "\n",
    "***The Implicit Neural Representation*** *is a full kinds of* ***mapping function***, *that identifying a specific pattern from existing phenomena and maps to an unspecified object.*\n",
    "\n",
    "*It contains various models of Deep Learning fields, especially the Generative Adversarial Networks are frequently used to construct the function nowadays, previously many approaches suggested such as statistical methods since its purpose is encoding the potential frequency as much as possible from the original phenomenon that can appear.*\n",
    "\n",
    "*One of the difference between* ***Traditional Explicit Representations*** *and* ***Implicit Neural Representation*** *is whether* ***discrete*** *or* ***contionus***, *which its property of 'continous' better represents the kind of mapping function, a distribution sampled from the existing distribution.*\n",
    "\n",
    "*The Deep Learning, which contains the huge amount of* ***techiniques to map the specific function*** *including MLP, CNN, Encoder-Decoder architecture are ***can be seen in the same context.****\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **3. The applications and potentials of INR**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **4. Latest Trend of INR**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **5. Representative INR paper reviews**\n",
    "\n",
    "#### **1. Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift**\n",
    "> **Authors: Sergey Ioffe, Christian Szegedy - Google Inc., {sioffe, szegedy} @google.com**\n",
    "\n",
    "> **Original Paper**: https://arxiv.org/pdf/1502.03167.pdf\n",
    "\n",
    "> **Un official Github**: https://github.com/shuuki4/Batch-Normalization\n",
    "\n",
    "#### **2. Instance Normalization: The Missing Ingredient for Fast Stylization**\n",
    "\n",
    "> **Authors: Dmitry Ulyanov [dmitry.ulyanov@skoltech.ru], Andrea Vedaldi [vedaldi@robots.ox.ac.uk], Victor Lempitsky [lempitsky@skoltech.ru]**\n",
    "\n",
    "> **Original Paper**: https://arxiv.org/pdf/1607.08022.pdf\n",
    "\n",
    "> **Official Github**: https://github.com/DmitryUlyanov/texture_nets\n",
    "\n",
    "#### 3. **Arbitrary Style Transfer in Real-time with Adaptive Instance Normalization**\n",
    "\n",
    "> **Authors: Xun Huang, Serge Belongie - Department of Computer Science & Cornell Tech, Cornell University, {xh258,sjb344}@cornell.edu**\n",
    "\n",
    "> **Official Github**: https://github.com/xunhuang1995/AdaIN-style\n",
    "\n",
    "> **Original Paper**: https://arxiv.org/pdf/1703.06868.pdf\n",
    "\n",
    "#### **4. Semantic Image Synthesis with Spatially-Adaptive Normalization**\n",
    "\n",
    "> **Authors: Taesung Park, Ming-Yu Liu, Ting-Chun Wang, Jun-Yan ZhuL**\n",
    "\n",
    "> **Original Paper**: https://arxiv.org/pdf/1903.07291.pdf\n",
    "\n",
    "> **Official Github**: https://github.com/NVlabs/SPADE\n",
    "\n",
    "#### **5. Universal Style Transfer via Feature Transforms**\n",
    "\n",
    "> **Authors: Yijun Li\n",
    "(UC Merced -\n",
    "yli62@ucmerced.edu),\n",
    "Chen Fang\n",
    "(Adobe Research - \n",
    "cfang@adobe.com)\n",
    "Jimei Yang\n",
    "(Adobe Research -\n",
    "jimyang@adobe.com)\n",
    "Zhaowen Wang\n",
    "(Adobe Research - \n",
    "zhawang@adobe.com)\n",
    "Xin Lu\n",
    "(Adobe Research - \n",
    "xinl@adobe.com)\n",
    "Ming-Hsuan Yang\n",
    "(UC Merced, NVIDIA Research - \n",
    "mhyang@ucmerced.edu)**\n",
    "\n",
    "> **Original Paper**: https://arxiv.org/pdf/1705.08086.pdf\n",
    "\n",
    "> **Official Github**: https://github.com/Yijunmaverick/UniversalStyleTransfer\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Why Internal Covariance Shift Matters?**\n",
    "\n",
    "At the first when I encounter the *'Normalization'* term in INR (Implicit Neural Representation) Field, I thought it is just a one of the common technique(at least the current point of Jan, 2022) to enhance the deep learning models' performance.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **1. Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift**\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "3efb3a3e09a38f93b1d9bfa4b9b8ce66a167f6ada77b64bc003d7d6d298d81d5"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 64-bit ('buddhalight': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
