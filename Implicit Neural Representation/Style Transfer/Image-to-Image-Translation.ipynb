{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **About the Image to Image Translation**\n",
    "\n",
    "#### The Main researches for Image to Image Translation from Supervised to Unsupervised\n",
    "\n",
    "Presentation Subject on 02.18.22\n",
    "\n",
    "**추가예정**\n",
    "#### **Index**\n",
    "\n",
    "- **Image to Image Translation**\n",
    "    - Pix2Pix\n",
    "    - CycleGan\n",
    "<br></br>\n",
    "- **Multi-Domain Image to Image Translation**\n",
    "    - StarGAN\n",
    "    - RelGAN\n",
    "<br></br>\n",
    "\n",
    "- **Unsupervised Image to Image Translation**\n",
    "    - UNIT\n",
    "    - MUNIT\n",
    "    - FUNIT\n",
    "<br></br>\n",
    "\n",
    "- **Others**\n",
    "    - CUT\n",
    "    - Swapping AutoEncoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Image to Image Translation**\n",
    "---\n",
    "\n",
    "**Image-to-Image Translation(pix2pix, CycleGAN) => Multi-domain Image-to-Image Translation(StarGAN, RelGAN)**\n",
    "\n",
    "**=> Unsupervised Image-to-Image Translation(UNIT, MUNIT, FUNIT)**\n",
    "\n",
    "> **Unsupervised means \"unpaired dataset\"**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Multi-Domain Image to Image Translation**\n",
    "---\n",
    "\n",
    "### 1. Background of appearance\n",
    "\n",
    "- Existing image-to-image translation studies (Pix2Pix, CycleGAN, DiscoGAN, etc.) do not operate reliably in more than three domains.\n",
    "- There is a disadvantage in that the number of generators increases depending on the number of domains. (If k is the number of domains, learn k(k-1) generators.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **StarGAN :Unified Generative Adversarial Networks for Multi-Domain Image-to-Image Translation**\n",
    "\n",
    "#### **1. Previous Problem**\n",
    "\n",
    "#### **2. The Core**\n",
    "\n",
    "- Image-to-image translation is possible for multiple domains using only a single model (you can change multiple attributes such as age, gender, etc. in one image at a time).\n",
    "- Datasets with different domains can be learned simultaneously using mask vectors.\n",
    "\n",
    "#### **3. Keywords**\n",
    "\n",
    "- **Attributes**:\n",
    "- **Attribute Value**:\n",
    "- **Domain**:\n",
    "\n",
    "#### **4. Contribution**\n",
    "\n",
    "#### **5. Overview**\n",
    "\n",
    "#### **6. Methods**\n",
    "\n",
    "##### **6-1. Loss**\n",
    "\n",
    "![](https://img1.daumcdn.net/thumb/R1280x0/?scode=mtistory2&fname=https%3A%2F%2Fblog.kakaocdn.net%2Fdn%2FAoafp%2FbtqxxcBT9QT%2FvrwY3ZNkXhZkgXZ7m9KchK%2Fimg.png)\n",
    "\n",
    "##### **6-2. Adversarial Loss**\n",
    "\n",
    "![](https://img1.daumcdn.net/thumb/R1280x0/?scode=mtistory2&fname=https%3A%2F%2Fblog.kakaocdn.net%2Fdn%2FdbA6QV%2Fbtqxx1mlaTX%2FDpNq6QlrfxcGawhPqKjGDk%2Fimg.png)\n",
    "\n",
    "#### **7. Conclusion**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **RelGAN: Multi-Domain Image-to-Image Translation via Relative Attributes**\n",
    "---\n",
    "\n",
    "#### **1. Previous Problem**\n",
    "\n",
    "Previous methods (StarGAN, etc.) use (image, target attributes) as input pairs and generate output images with the desired attributes.\n",
    "\n",
    "Limit 1: Assuming binary value attributes, satisfactory results cannot be obtained for detailed control.\n",
    "\n",
    "Limit 2: Even if most target attributes do not change, you must specify the entire set of target attributes.\n",
    "\n",
    "#### **2. The Core**\n",
    "\n",
    "- Use a relative attribute-based scheme that describes the desired change for the selected attribute. RelGAN can know the entire attribute of the input image according to the change of each attribute. (Overcoming limit 2)\n",
    "- A matching-aware descriptor appears to determine whether the input-output pair matches the relative attribute to learn G subject to the relative attribute.\n",
    "- Assuming binary properties, the interpolation discriminator is proposed to improve interpolation quality, designed for attribute interpolation, and the soft and realistic interpolation between before and after editing can control the intensity of each attribute (ex. brown ratio) in detail (overcoming limit 1).\n",
    "\n",
    "\n",
    "#### **3. Keywords**\n",
    "\n",
    "\n",
    "#### **4. Contribution**\n",
    "\n",
    "#### **5. Overview**\n",
    "\n",
    "#### **6. Methods**\n",
    "\n",
    "##### **6-1. Adversarial Loss**\n",
    "\n",
    "![](https://user-images.githubusercontent.com/68530330/98652543-25fba680-237f-11eb-9157-d302cff5ea9f.png)\n",
    "\n",
    "##### **6-2. Conditional Adversarial Loss**\n",
    "\n",
    "![](https://user-images.githubusercontent.com/68530330/98652585-34e25900-237f-11eb-9008-3624e1e0299a.png)\n",
    "\n",
    "#### **7. Conclusion**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Unsupervised Image to Image Translation**\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **UNIT(Unsupervised Image-to-Image Translation)**\n",
    "\n",
    "#### **Previous Problem**\n",
    "\n",
    "- It is an ill-posed problem to infer the point distribution when there are two marginal distributions for two domains.\n",
    "\n",
    "#### **The Core**\n",
    "\n",
    "- It deals with the unsupervised problem without paired image data showing how one image has been converted into another domain.\n",
    "- To solve the ill-posed problem problem, shared late space acceptance emerged.\n",
    "\n",
    "#### **3. Methods**\n",
    "\n",
    "##### **3-1. Assumption - Shared Latent Space**\n",
    "This can be expressed by the following equations assuming that the image pairs of different domains can be mapped to one same late space.\n",
    "\n",
    "![](https://user-images.githubusercontent.com/39791467/100059195-ff089e80-2e6d-11eb-8dc1-b776b043a36e.png)\n",
    "![](https://user-images.githubusercontent.com/39791467/100060002-4e9b9a00-2e6f-11eb-870b-e6ab247da283.png)\n",
    "![](https://user-images.githubusercontent.com/39791467/100060609-206a8a00-2e70-11eb-89d1-e8ec8967b023.png)\n",
    "\n",
    "##### **3-2. Framework**\n",
    "\n",
    "##### **3-2-1. VAE{E, G}**\n",
    "\n",
    "The encoder-generator pair constitutes the VAE. Map the image to the latent space z through Encoder and restore the image from z through Generator.\n",
    "\n",
    "##### **3-2-2. Weight-sharing**\n",
    "\n",
    "Weight-sharing is used to associate the two VAEs. This is the part corresponding to the dotted line in Figure (b) above. In order to share one late space z, weight is shared in the last feather layers of the two encoders and the first feather layers of the two generators. This alone cannot guarantee that the same late space is shared, but one late space is shared here through adversarial training.\n",
    "\n",
    "##### **3-2-3. GAN {G, D}**\n",
    "\n",
    "Generator generates images in the reconstruction stream and images in the translation stream, and since the reconstruction stream can be learned in a supervised manner, adversarial training is performed only on the translation stream."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **MUNIT(Multimodal Unsupervised Image-to-Image Translation)**\n",
    "\n",
    "---\n",
    "\n",
    "For Multimodal, the latent space for the image is decomposed into content space and style space. Here, it is assumed that the content space is shared regardless of domain and the style space is domain-specific. Through this, it is possible to show the results of various styles.\n",
    "\n",
    "#### **Learning disentangled representations**\n",
    "\n",
    "Content-the-underling spatial structure, style-the rendering of the structure, is said for content and style. The two domains share the same content distribution but have different style distributions.\n",
    "\n",
    "#### **Model**\n",
    "\n",
    "![](https://user-images.githubusercontent.com/39791467/100064830-5a3e8f00-2e76-11eb-8feb-73e45270e76c.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **FUNIT(Few-Shot Unsupervised Image-to-Image Translation)**\n",
    "\n",
    "---\n",
    "\n",
    "It is a model that enables few-shot based on MUNIT described above.\n",
    "1. many source class images but few target class images\n",
    "2. Few target class images exist only in test time and may consist of many different classes.\n",
    "\n",
    "#### **Methods**\n",
    "\n",
    "##### **Model**\n",
    "\n",
    "##### **Few-shot Image Translator**\n",
    "FUNIT's goal is to map the image of the source class to a target class that has never been seen as a pure target class image. In order to learn FUNIT, object classes data (images of various animals without considering pairs between classes) were used, which is called source classes. And in the test, the new images for the new object class are used as the target class.\n",
    "\n",
    "Any source class should be converted to suit the images of the target class.\n",
    "\n",
    "It consists of conditional image generator G and multi-task adversarial descriptor D.\n",
    "\n",
    "In the case of the generator, the content image x and the self of K class images are received as input as follows.\n",
    "When training, pick two classes with random so that they do not overlap with the source classes and teach them.\n",
    "\n",
    "![](https://user-images.githubusercontent.com/39791467/100091002-65ee7d80-2e97-11eb-8204-6a0059a35cd5.png)\n",
    "\n",
    "![](https://user-images.githubusercontent.com/39791467/100070561-6da12880-2e7d-11eb-9b38-46643c5e6f83.png)\n",
    "\n",
    "##### **Multi-task Adversarial Discriminator**\n",
    "\n",
    "Discriminator performs multi-adversarial classification, and each determines whether the input image is real with a binary classification task.\n",
    "\n",
    "When there are S source classes, S outputs appear in the Discriminator, and only when the output of the class is false for the real image, and only when the output of the class is true for the translation fake image.\n",
    "\n",
    "The generator is also updated only when the output of the class is false.\n",
    "\n",
    "\n",
    "##### **Learning**\n",
    "\n",
    "![](https://user-images.githubusercontent.com/39791467/100128343-82081400-2ec3-11eb-9916-c3ae4d2f87b8.png)\n",
    "\n",
    "##### **GAN Loss**\n",
    "\n",
    "![](https://user-images.githubusercontent.com/39791467/100128380-8c2a1280-2ec3-11eb-9ef1-8f1734b050b0.png)\n",
    "\n",
    "In GAN loss, the subscript above D means object class, and loss is calculated only by the binary priority score of class.\n",
    "\n",
    "##### **Content reconstruction loss**\n",
    "\n",
    "![](https://user-images.githubusercontent.com/39791467/100128419-96e4a780-2ec3-11eb-8680-a0f9b85f2f30.png)\n",
    "\n",
    "Use the same image as input content image and input class image (K=1) to create the same output image as input.\n",
    "\n",
    "##### **Feature matching loss**\n",
    "\n",
    "![](https://user-images.githubusercontent.com/39791467/100128465-a7951d80-2ec3-11eb-9767-d52dd0e280af.png)\n",
    "\n",
    "Learn to minimize the difference in feature maps between translation output and class images with feature extractor Df from which the last presentation layer was removed from the discriminator."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
