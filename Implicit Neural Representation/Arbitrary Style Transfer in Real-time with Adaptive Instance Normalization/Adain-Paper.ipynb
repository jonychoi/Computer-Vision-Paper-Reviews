{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Arbitrary Style Transfer in Real-time with Adaptive Instance Normalization**\n",
    "\n",
    "**Authors: Xun Huang, Serge Belongie - Department of Computer Science & Cornell Tech, Cornell University, {xh258,sjb344}@cornell.edu**\n",
    "\n",
    "**Official Github**: https://github.com/xunhuang1995/AdaIN-style\n",
    "\n",
    "**Original Paper**: https://arxiv.org/pdf/1703.06868.pdf\n",
    "\n",
    "---\n",
    "\n",
    "**Edited By Su Hyung Choi (Key Summary & Code Practice)**\n",
    "\n",
    "If you have any issues on this scripts, please PR to the repository below.\n",
    "\n",
    "**[Github: @JonyChoi - Computer Vision Paper Reviews]** https://github.com/jonychoi/Computer-Vision-Paper-Reviews\n",
    "\n",
    "Edited Jan 16 2022\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Abstract**\n",
    "\n",
    "\n",
    "<p><i>Gatys et al. recently introduced a neural algorithm that\n",
    "renders a content image in the style of another image,\n",
    "achieving so-called style transfer. However, their framework requires a slow iterative optimization process, which\n",
    "limits its practical application. Fast approximations with\n",
    "feed-forward neural networks have been proposed to speed\n",
    "up neural style transfer. Unfortunately, the speed improvement comes at a cost: the network is usually tied to a fixed\n",
    "set of styles and cannot adapt to arbitrary new styles. In this\n",
    "paper, we present a simple yet effective approach that for the\n",
    "first time enables arbitrary style transfer in real-time. At the\n",
    "heart of our method is a novel adaptive instance normalization (AdaIN) layer that aligns the mean and variance of the\n",
    "content features with those of the style features. Our method\n",
    "achieves speed comparable to the fastest existing approach,\n",
    "without the restriction to a pre-defined set of styles. In addition, our approach allows flexible user controls such as\n",
    "content-style trade-off, style interpolation, color & spatial\n",
    "controls, all using a single feed-forward neural network.\n",
    "</i>\n",
    "</p>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **1. Introduction**\n",
    "<p>\n",
    "The seminal work of Gatys et al. [16] showed that deep\n",
    "neural networks (DNNs) encode not only the content but\n",
    "also the style information of an image. Moreover, the image style and content are somewhat separable: it is possible\n",
    "to change the style of an image while preserving its content. The style transfer method of [16] is flexible enough to\n",
    "combine content and style of arbitrary images. However, it\n",
    "relies on an optimization process that is prohibitively slow.\n",
    "</p>\n",
    "<p>\n",
    "Significant effort has been devoted to accelerating neural\n",
    "style transfer. [24, 51, 31] attempted to train feed-forward\n",
    "neural networks that perform stylization with a single forward pass. A major limitation of most feed-forward methods is that each network is restricted to a single style. There\n",
    "are some recent works addressing this problem, but they are\n",
    "either still limited to a finite set of styles [11, 32, 55, 5], or\n",
    "much slower than the single-style transfer methods [6].\n",
    "</p>\n",
    "<p>\n",
    "    In this work, we present the first neural style transfer\n",
    "algorithm that resolves this fundamental flexibility-speed\n",
    "dilemma. Our approach can transfer arbitrary new styles\n",
    "in real-time, combining the flexibility of the optimizationbased framework [16] and the speed similar to the fastest\n",
    "feed-forward approaches [24, 52]. Our method is inspired\n",
    "by the instance normalization (IN) [52, 11] layer, which\n",
    "is surprisingly effective in feed-forward style transfer. To\n",
    "explain the success of instance normalization, we propose\n",
    "a new interpretation that instance normalization performs\n",
    "style normalization by normalizing feature statistics, which\n",
    "have been found to carry the style information of an image [16, 30, 33]. Motivated by our interpretation, we introduce a simple extension to IN, namely adaptive instance\n",
    "normalization (AdaIN). Given a content input and a style\n",
    "input, AdaIN simply adjusts the mean and variance of the\n",
    "content input to match those of the style input. Through\n",
    "experiments, we find AdaIN effectively combines the content of the former and the style latter by transferring feature\n",
    "statistics. A decoder network is then learned to generate the\n",
    "final stylized image by inverting the AdaIN output back to\n",
    "the image space. Our method is nearly three orders of magnitude faster than [16], without sacrificing the flexibility of\n",
    "transferring inputs to arbitrary new styles. Furthermore, our\n",
    "approach provides abundant user controls at runtime, without any modification to the training process.\n",
    "</p>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **2. Related Work**\n",
    "\n",
    "<p><strong>Style transfer.</strong> The problem of style transfer has its origin\n",
    "from non-photo-realistic rendering [28], and is closely related to texture synthesis and transfer [13, 12, 14]. Some\n",
    "early approaches include histogram matching on linear filter responses [19] and non-parametric sampling [12, 15].\n",
    "These methods typically rely on low-level statistics and often fail to capture semantic structures. Gatys et al. [16] for\n",
    "the first time demonstrated impressive style transfer results\n",
    "by matching feature statistics in convolutional layers of a\n",
    "DNN. Recently, several improvements to [16] have been\n",
    "proposed. Li and Wand [30] introduced a framework based\n",
    "on markov random field (MRF) in the deep feature space to\n",
    "enforce local patterns. Gatys et al. [17] proposed ways to\n",
    "control the color preservation, the spatial location, and the\n",
    "scale of style transfer. Ruder et al. [45] improved the quality of video style transfer by imposing temporal constraints.\n",
    "</p>\n",
    "<p>\n",
    "The framework of Gatys et al. [16] is based on a slow\n",
    "optimization process that iteratively updates the image to\n",
    "minimize a content loss and a style loss computed by a loss\n",
    "network. It can take minutes to converge even with modern GPUs. On-device processing in mobile applications is\n",
    "therefore too slow to be practical. A common workaround\n",
    "is to replace the optimization process with a feed-forward\n",
    "neural network that is trained to minimize the same objective [24, 51, 31]. These feed-forward style transfer approaches are about three orders of magnitude faster than\n",
    "the optimization-based alternative, opening the door to realtime applications. Wang et al. [53] enhanced the granularity\n",
    "of feed-forward style transfer with a multi-resolution architecture. Ulyanov et al. [52] proposed ways to improve the\n",
    "quality and diversity of the generated samples. However,\n",
    "the above feed-forward methods are limited in the sense that\n",
    "each network is tied to a fixed style. To address this problem, Dumoulin et al. [11] introduced a single network that\n",
    "is able to encode 32 styles and their interpolations. Concurrent to our work, Li et al. [32] proposed a feed-forward\n",
    "architecture that can synthesize up to 300 textures and transfer 16 styles. Still, the two methods above cannot adapt to\n",
    "arbitrary styles that are not observed during training.\n",
    "</p>\n",
    "<p>\n",
    "Very recently, Chen and Schmidt [6] introduced a feedforward method that can transfer arbitrary styles thanks to\n",
    "a style swap layer. Given feature activations of the content\n",
    "and style images, the style swap layer replaces the content\n",
    "features with the closest-matching style features in a patchby-patch manner. Nevertheless, their style swap layer creates a new computational bottleneck: more than 95% of the\n",
    "computation is spent on the style swap for 512 × 512 input\n",
    "images. Our approach also permits arbitrary style transfer,\n",
    "while being 1-2 orders of magnitude faster than [6].\n",
    "</p>\n",
    "<p>\n",
    "    while being 1-2 orders of magnitude faster than [6].\n",
    "Another central problem in style transfer is which style\n",
    "loss function to use. The original framework of Gatys et\n",
    "al. [16] matches styles by matching the second-order statistics between feature activations, captured by the Gram matrix. Other effective loss functions have been proposed,\n",
    "such as MRF loss [30], adversarial loss [31], histogram\n",
    "loss [54], CORAL loss [41], MMD loss [33], and distance\n",
    "between channel-wise mean and variance [33]. Note that all\n",
    "the above loss functions aim to match some feature statistics\n",
    "between the style image and the synthesized image.\n",
    "</p>\n",
    "<p>\n",
    "<strong>Deep generative image modeling.</strong> There are several alternative frameworks for image generation, including variational auto-encoders [27], auto-regressive models [40], and\n",
    "generative adversarial networks (GANs) [18]. Remarkably,\n",
    "GANs have achieved the most impressive visual quality.\n",
    "Various improvements to the GAN framework have been\n",
    "proposed, such as conditional generation [43, 23], multistage processing [9, 20], and better training objectives [46,\n",
    "1]. GANs have also been applied to style transfer [31] and cross-domain image generation [50, 3, 23, 38, 37, 25].\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **3. Background**\n",
    "\n",
    "#### 3.1. Batch Normalization\n",
    "\n",
    "<p>\n",
    "The seminal work of Ioffe and Szegedy [22] introduced\n",
    "a batch normalization (BN) layer that significantly ease the\n",
    "training of feed-forward networks by normalizing feature\n",
    "statistics. BN layers are originally designed to accelerate training of discriminative networks, but have also been\n",
    "found effective in generative image modeling [42]. Given\n",
    "an input batch x ∈ R\n",
    "N×C×H×W , BN normalizes the mean\n",
    "and standard deviation for each individual feature channel:\n",
    "</p>\n",
    "<table>\n",
    "    <tbody>\n",
    "        <tr>\n",
    "            <td>\n",
    "                <img src=\"./imgs/equation1.png\" width=\"400\" />\n",
    "            </td>\n",
    "        </tr>\n",
    "    </tbody>\n",
    "</table>\n",
    "<p>\n",
    "    where γ, β ∈ R\n",
    "C are affine parameters learned from data;\n",
    "µ(x), σ(x) ∈ R\n",
    "C are the mean and standard deviation,\n",
    "computed across batch size and spatial dimensions independently for each feature channel:\n",
    "</p>\n",
    "<table>\n",
    "    <tbody>\n",
    "        <tr>\n",
    "            <td>\n",
    "                <img src=\"./imgs/equation2.png\" width=\"440\" />\n",
    "            </td>\n",
    "            <td>\n",
    "                <img src=\"./imgs/equation3.png\" width=\"400\" />\n",
    "            </td>\n",
    "        </tr>\n",
    "    </tbody>\n",
    "</table>\n",
    "<p>\n",
    "BN uses mini-batch statistics during training and replace\n",
    "them with popular statistics during inference, introducing\n",
    "discrepancy between training and inference. Batch renormalization [21] was recently proposed to address this issue\n",
    "by gradually using popular statistics during training. As\n",
    "another interesting application of BN, Li et al. [34] found\n",
    "that BN can alleviate domain shifts by recomputing popular\n",
    "statistics in the target domain. Recently, several alternative\n",
    "normalization schemes have been proposed to extend BN’s\n",
    "effectiveness to recurrent architectures [35, 2, 47, 8, 29, 44].\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **3.2. Instance Normalization**\n",
    "<p>\n",
    "In the original feed-forward stylization method [51], the\n",
    "style transfer network contains a BN layer after each convolutional layer. Surprisingly, Ulyanov et al. [52] found\n",
    "that significant improvement could be achieved simply by\n",
    "replacing BN layers with IN layers:\n",
    "</p>\n",
    "<table>\n",
    "    <tbody>\n",
    "        <tr>\n",
    "            <td>\n",
    "                <img src=\"./imgs/equation4.png\" width=\"400\" />\n",
    "            </td>\n",
    "        </tr>\n",
    "    </tbody>\n",
    "</table>\n",
    "<p>\n",
    "Different from BN layers, here µ(x) and σ(x) are computed across spatial dimensions independently for each\n",
    "channel <i>and each sample:</i>\n",
    "</p>\n",
    "<table>\n",
    "    <tbody>\n",
    "        <tr>\n",
    "            <td>\n",
    "                <img src=\"./imgs/equation5.png\" width=\"400\" />\n",
    "            </td>\n",
    "        </tr>\n",
    "    </tbody>\n",
    "</table>\n",
    "<img src=\"./imgs/figure1.png\" />\n",
    "<table>\n",
    "    <tbody>\n",
    "        <tr>\n",
    "            <td>\n",
    "                <img src=\"./imgs/equation6.png\" width=\"400\" />\n",
    "            </td>\n",
    "        </tr>\n",
    "    </tbody>\n",
    "</table>\n",
    "<p>\n",
    "Another difference is that IN layers are applied at test\n",
    "time unchanged, whereas BN layers usually replace minibatch statistics with population statistics.\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **3.3. Conditional Instance Normalization**\n",
    "Instead of learning a single set of affine parameters γ\n",
    "and β, Dumoulin et al. [11] proposed a conditional instance\n",
    "normalization (CIN) layer that learns a different set of parameters γ\n",
    "s\n",
    "and β\n",
    "s\n",
    "for each style s:\n",
    "<table>\n",
    "    <tbody>\n",
    "        <tr>\n",
    "            <td>\n",
    "                <img src=\"./imgs/equation7.png\" width=\"400\" />\n",
    "            </td>\n",
    "        </tr>\n",
    "    </tbody>\n",
    "</table>\n",
    "<p>\n",
    "During training, a style image together with its index\n",
    "s are randomly chosen from a fixed set of styles s ∈\n",
    "{1, 2, ..., S} (S = 32 in their experiments). The content image is then processed by a style transfer network\n",
    "in which the corresponding γ*\n",
    "and β*\n",
    "are used in the CIN\n",
    "layers. Surprisingly, the network can generate images in\n",
    "completely different styles by using the same convolutional\n",
    "parameters but <i>different</i> affine parameters in IN layers.\n",
    "</p>\n",
    "<p>\n",
    "Compared with a network without normalization layers,\n",
    "a network with CIN layers requires 2F S additional parameters, where F is the total number of feature maps in the\n",
    "network [11]. Since the number of additional parameters\n",
    "scales linearly with the number of styles, it is challenging to\n",
    "extend their method to model a large number of styles (e.g.,\n",
    "tens of thousands). Also, their approach cannot adapt to\n",
    "arbitrary new styles without re-training the network.\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **4. Interpreting Instance Normalization**\n",
    "\n",
    "<p>\n",
    "Despite the great success of (conditional) instance normalization, the reason why they work particularly well for\n",
    "style transfer remains elusive. Ulyanov <i>et al</i>. [52] attribute the success of IN to its invariance to the contrast of the content image. However, IN takes place in the feature space,\n",
    "therefore it should have more profound impacts than a simple contrast normalization in the pixel space. Perhaps even\n",
    "more surprising is the fact that the affine parameters in IN\n",
    "can completely change the style of the output image.\n",
    "</p>\n",
    "<p>\n",
    "It has been known that the convolutional feature statistics\n",
    "of a DNN can capture the style of an image [16, 30, 33].\n",
    "While Gatys <i>et al</i>. [16] use the second-order statistics as\n",
    "their optimization objective, Li <i>et al</i>. [33] recently showed\n",
    "that matching many other statistics, including channel-wise\n",
    "mean and variance, are also effective for style transfer. Motivated by these observations, we argue that instance normalization performs a form of <i>style normalization</i> by normalizing feature statistics, namely the mean and variance.\n",
    "Although DNN serves as a image <i>descriptor</i> in [16, 33], we\n",
    "believe that the feature statistics of a <i>generator</i> network can\n",
    "also control the style of the generated image.\n",
    "</p>\n",
    "<p>\n",
    "We run the code of improved texture networks [52] to\n",
    "perform single-style transfer, with IN or BN layers. As\n",
    "expected, the model with IN converges faster than the BN\n",
    "model (Fig. 1 (a)). To test the explanation in [52], we then\n",
    "normalize all the training images to the same contrast by\n",
    "performing histogram equalization on the luminance channel. As shown in Fig. 1 (b), IN remains effective, suggesting the explanation in [52] to be incomplete. To verify our hypothesis, we normalize all the training images to\n",
    "the same style (different from the target style) using a pretrained style transfer network provided by [24]. According\n",
    "to Fig. 1 (c), the improvement brought by IN become much\n",
    "smaller when images are already style normalized. The remaining gap can explained by the fact that the style normalization with [24] is not perfect. Also, models with BN\n",
    "trained on style normalized images can converge as fast as\n",
    "models with IN trained on the original images. Our results\n",
    "indicate that IN does perform a kind of style normalization.\n",
    "</p>\n",
    "<p>\n",
    "Since BN normalizes the feature statistics of a batch of\n",
    "samples instead of a single sample, it can be intuitively\n",
    "understood as normalizing a batch of samples to be centered around a single style. Each single sample, however,\n",
    "may still have different styles. This is undesirable when we\n",
    "want to transfer all images to the same style, as is the case\n",
    "in the original feed-forward style transfer algorithm [51].\n",
    "Although the convolutional layers might learn to compensate the intra-batch style difference, it poses additional challenges for training. On the other hand, IN can normalize the\n",
    "style of each individual sample to the target style. Training\n",
    "is facilitated because the rest of the network can focus on\n",
    "content manipulation while discarding the original style information. The reason behind the success of CIN also becomes clear: different affine parameters can normalize the\n",
    "feature statistics to different values, thereby normalizing the\n",
    "output image to different styles.\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **5. Adaptive Instance Normalization**\n",
    "\n",
    "<p>\n",
    "If IN normalizes the input to a single style specified by\n",
    "the affine parameters, is it possible to adapt it to arbitrarily\n",
    "given styles by using adaptive affine transformations? Here,\n",
    "we propose a simple extension to IN, which we call adaptive\n",
    "instance normalization (AdaIN). AdaIN receives a content\n",
    "input x and a style input y, and simply aligns the channelwise mean and variance of x to match those of y. Unlike\n",
    "BN, IN or CIN, AdaIN has no learnable affine parameters.\n",
    "Instead, it adaptively computes the affine parameters from\n",
    "the style input:\n",
    "</p>\n",
    "<table>\n",
    "    <tbody>\n",
    "        <tr>\n",
    "            <td>\n",
    "                <img src=\"./imgs/equation8.png\" width=\"400\" />\n",
    "            </td>\n",
    "        </tr>\n",
    "    </tbody>\n",
    "</table>\n",
    "<p>\n",
    "in which we simply scale the normalized content input\n",
    "with σ(y), and shift it with µ(y). Similar to IN, these statistics are computed across spatial locations.\n",
    "</p>\n",
    "<p>\n",
    "Intuitively, let us consider a feature channel that detects\n",
    "brushstrokes of a certain style. A style image with this kind\n",
    "of strokes will produce a high average activation for this\n",
    "feature. The output produced by AdaIN will have the same\n",
    "high average activation for this feature, while preserving the\n",
    "spatial structure of the content image. The brushstroke feature can be inverted to the image space with a feed-forward\n",
    "decoder, similar to [10]. The variance of this feature channel can encoder more subtle style information, which is also\n",
    "transferred to the AdaIN output and the final output image.\n",
    "</p>\n",
    "<p>\n",
    "    In short, AdaIN performs style transfer in the feature space by transferring feature statistics, specifically the\n",
    "channel-wise mean and variance. Our AdaIN layer plays\n",
    "a similar role as the style swap layer proposed in [6].\n",
    "While the style swap operation is very time-consuming and\n",
    "memory-consuming, our AdaIN layer is as simple as an IN\n",
    "layer, adding almost no computational cost.\n",
    "</p>\n",
    "<table>\n",
    "    <tbody>\n",
    "        <tr>\n",
    "            <td>\n",
    "                <img src=\"./imgs/figure2.png\" width=\"500\" />\n",
    "            </td>\n",
    "        </tr>\n",
    "    </tbody>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **6. Experimental Setup**\n",
    "<p>\n",
    "Fig. 2 shows an overview of our style transfer network based on the proposed AdaIN layer. Code and pretrained models (in Torch 7 [7]) are available at: <a href=\"https://github.com/xunhuang1995/AdaIN-style\">https://github.com/xunhuang1995/AdaIN-style</a>\n",
    "</p>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **6.1. Architecture**\n",
    "Our style transfer network <i>T</i> takes a content image c and\n",
    "an arbitrary style image s as inputs, and synthesizes an output image that recombines the content of the former and the\n",
    "style latter. We adopt a simple encoder-decoder architecture, in which the encoder <i>f</i> is fixed to the first few layers (up to relu4 1) of a pre-trained VGG-19 [48]. After\n",
    "encoding the content and style images in feature space, we\n",
    "feed both feature maps to an AdaIN layer that aligns the\n",
    "mean and variance of the content feature maps to those of\n",
    "the style feature maps, producing the target feature maps t:\\\n",
    "<table>\n",
    "    <tbody>\n",
    "        <tr>\n",
    "            <td>\n",
    "                <img src=\"./imgs/equation9.png\" width=\"400\" />\n",
    "            </td>\n",
    "        </tr>\n",
    "    </tbody>\n",
    "</table>\n",
    "<p>\n",
    "A randomly initialized decoder g is trained to map t back\n",
    "to the image space, generating the stylized image T(c, s):\n",
    "</p>\n",
    "<table>\n",
    "    <tbody>\n",
    "        <tr>\n",
    "            <td>\n",
    "                <img src=\"./imgs/equation10.png\" width=\"400\" />\n",
    "            </td>\n",
    "        </tr>\n",
    "    </tbody>\n",
    "</table>\n",
    "<p>\n",
    "The decoder mostly mirrors the encoder, with all pooling\n",
    "layers replaced by nearest up-sampling to reduce checkerboard effects. We use reflection padding in both f and g\n",
    "to avoid border artifacts. Another important architectural\n",
    "choice is whether the decoder should use instance, batch, or\n",
    "no normalization layers. As discussed in Sec. 4, IN normalizes each sample to a single style while BN normalizes a\n",
    "batch of samples to be centered around a single style. Both\n",
    "are undesirable when we want the decoder to generate images in vastly different styles. Thus, we do not use normalization layers in the decoder. In Sec. 7.1 we will show that\n",
    "IN/BN layers in the decoder indeed hurt performance.\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **6.2. Training**\n",
    "<p>\n",
    "We train our network using MS-COCO [36] as content\n",
    "images and a dataset of paintings mostly collected from\n",
    "WikiArt [39] as style images, following the setting of [6].\n",
    "Each dataset contains roughly 80, 000 training examples.\n",
    "We use the adam optimizer [26] and a batch size of 8\n",
    "content-style image pairs. During training, we first resize\n",
    "the smallest dimension of both images to 512 while preserving the aspect ratio, then randomly crop regions of size\n",
    "256 × 256. Since our network is fully convolutional, it can\n",
    "be applied to images of any size during testing.\n",
    "Similar to [51, 11, 52], we use the pre-trained VGG19 [48] to compute the loss function to train the decoder:\n",
    "</p>\n",
    "<table>\n",
    "    <tbody>\n",
    "        <tr>\n",
    "            <td>\n",
    "            <img src=\"./imgs/equation11.png\" width=\"400\" />\n",
    "            </td>\n",
    "        </tr>\n",
    "    </tbody>\n",
    "</table>\n",
    "<p>\n",
    "which is a weighted combination of the content loss <i>Lc</i>\n",
    "and the style loss <i>Ls</i> with the style loss weight λ. The\n",
    "content loss is the Euclidean distance between the target\n",
    "features and the features of the output image. We use the\n",
    "AdaIN output <i>t</i> as the content target, instead of the commonly used feature responses of the content image. We find\n",
    "this leads to slightly faster convergence and also aligns with\n",
    "our goal of inverting the AdaIN output <i>t</i>.\n",
    "</p>\n",
    "<img src=\"./imgs/equation12.png\" width=\"400\" />\n",
    "<p>\n",
    "    Since our AdaIN layer only transfers the mean and standard deviation of the style features, our style loss only\n",
    "matches these statistics. Although we find the commonly\n",
    "used Gram matrix loss can produce similar results, we\n",
    "match the IN statistics because it is conceptually cleaner.\n",
    "This style loss has also been explored by Li <i>et al</i>. [33].\n",
    "</p>\n",
    "<table>\n",
    "    <tbody>\n",
    "        <tr>\n",
    "            <td>\n",
    "                <img src=\"./imgs/equation13.png\" width=\"400\" />\n",
    "            </td>\n",
    "        </tr>\n",
    "    </tbody>\n",
    "</table>\n",
    "<p>\n",
    "where each φi denotes a layer in VGG-19 used to compute the style loss. In our experiments we use relu1 1,\n",
    "relu2 1, relu3 1, relu4 1 layers with equal weights.\n",
    "</p>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **7. Results**\n",
    "#### 7.1. Comparison with other methods\n",
    "<table>\n",
    "    <tbody>\n",
    "        <tr>\n",
    "            <td>\n",
    "                <img src=\"./imgs/figure3.png\" width=\"500\" />\n",
    "            </td>\n",
    "        </tr>\n",
    "    </tbody>\n",
    "</table>\n",
    "<p>\n",
    "In this subsection, we compare our approach with three\n",
    "types of style transfer methods: 1) the flexible but slow\n",
    "optimization-based method [16], 2) the fast feed-forward\n",
    "method restricted to a single style [52], and 3) the flexible\n",
    "patch-based method of medium speed [6]. If not mentioned\n",
    "otherwise, the results of compared methods are obtained by\n",
    "running their code with the default configurations (1).  For [6], we use a pre-trained inverse network provided by the\n",
    "authors. All the test images are of size 512 × 512.\n",
    "</p>\n",
    "<p>\n",
    "<strong>Qualitative Examples.</strong> In Fig. 4 we show example style\n",
    "transfer results generated by compared methods. Note that\n",
    "all the test style images are never observed during the training of our model, while the results of [52] are obtained by\n",
    "fitting one network to each test style. Even so, the quality of our stylized images is quite competitive with [52]\n",
    "and [16] for many images (e.g., row 1, 2, 3). In some other\n",
    "cases (e.g., row 5) our method is slightly behind the quality of [52] and [16]. This is not unexpected, as we believe there is a three-way trade-off between speed, flexibility, and quality. Compared with [6], our method appears\n",
    "to transfer the style more faithfully for most compared images. The last example clearly illustrates a major limitation of [6], which attempts to match each content patch with\n",
    "the closest-matching style patch. However, if most content\n",
    "patches are matched to a few style patches that are not representative of the target style, the style transfer would fail.\n",
    "We thus argue that matching global feature statistics is a\n",
    "more general solution, although in some cases (e.g., row 3)\n",
    "the method of [6] can also produce appealing results.\n",
    "</p>\n",
    "<p>\n",
    "<strong>Quantitative evaluations.</strong> Does our algorithm trade off\n",
    "some quality for higher speed and flexibility, and if so by\n",
    "how much? To answer this question quantitatively, we compare our approach with the optimization-based method [16]\n",
    "and the fast single-style transfer method [52] in terms of\n",
    "the content and style loss. Because our method uses a style\n",
    "loss based on IN statistics, we also modify the loss function\n",
    "in [16] and [52] accordingly for a fair comparison (their results in Fig. 4 are still obtained with the default Gram matrix\n",
    "loss). The content loss shown here is the same as in [52, 16].\n",
    "The numbers reported are averaged over 10 style images\n",
    "and 50 content images randomly chosen from the test set of\n",
    "the WikiArt dataset [39] and MS-COCO [36].\n",
    "</p>\n",
    "<img src=\"./imgs/figure4.png\" />\n",
    "<p>\n",
    "As shown in Fig. 3, the average content and style loss of\n",
    "our synthesized images are slightly higher but comparable\n",
    "to the single-style transfer method of Ulyanov et al. [52]. In\n",
    "particular, both our method and [52] obtain a style loss similar to that of [16] between 50 and 100 iterations of optimization. This demonstrates the strong generalization ability of\n",
    "our approach, considering that our network has never seen\n",
    "the test styles during training while each network of [52] is\n",
    "specifically trained on a test style. Also, note that our style\n",
    "loss is much smaller than that of the original content image.\n",
    "</p>\n",
    "<p>\n",
    "<strong>Speed analysis.</strong> Most of our computation is spent on content encoding, style encoding, and decoding, each roughly\n",
    "taking one third of the time. In some application scenarios such as video processing, the style image needs to be\n",
    "encoded only once and AdaIN can use the stored style\n",
    "statistics to process all subsequent images. In some other\n",
    "cases (e.g., transferring the same content to different styles),\n",
    "the computation spent on content encoding can be shared.\n",
    "</p>\n",
    "<p>\n",
    "In Tab. 1 we compare the speed of our method with previous ones [16, 52, 11, 6]. Excluding the time for style encoding, our algorithm runs at 56 and 15 FPS for 256 × 256\n",
    "and 512 × 512 images respectively, making it possible to\n",
    "process arbitrary user-uploaded styles in real-time. Among\n",
    "algorithms applicable to arbitrary styles, our method is\n",
    "nearly 3 orders of magnitude faster than [16] and 1-2 orders of magnitude faster than [6]. The speed improvement\n",
    "over [6] is particularly significant for images of higher resolution, since the style swap layer in [6] does not scale well\n",
    "to high resolution style images. Moreover, our approach\n",
    "achieves comparable speed to feed-forward methods limited\n",
    "to a few styles [52, 11]. The slightly longer processing time\n",
    "of our method is mainly due to our larger VGG-based network, instead of methodological limitations. With a more\n",
    "efficient architecture, our speed can be further improved.\n",
    "</p>\n",
    "<table>\n",
    "    <tbody>\n",
    "        <tr>\n",
    "            <td>\n",
    "                <img src=\"./imgs/table1.png\" width=\"500\" />\n",
    "            </td>\n",
    "        </tr>\n",
    "    </tbody>\n",
    "</table>\n",
    "\n",
    "---\n",
    "(1) We run 500 iterations of [16] using Johnson’s public implementation:\n",
    "https://github.com/jcjohnson/neural-style\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **7.2. Additional experiments.**\n",
    "<p>\n",
    "In this subsection, we conduct experiments to justify our\n",
    "important architectural choices. We denote our approach\n",
    "described in Sec. 6 as Enc-AdaIN-Dec. We experiment with\n",
    "a model named Enc-Concat-Dec that replaces AdaIN with\n",
    "concatenation, which is a natural baseline strategy to combine information from the content and style images. In addition, we run models with BN/IN layers in the decoder,\n",
    "denoted as Enc-AdaIN-BNDec and Enc-AdaIN-INDec respectively. Other training settings are kept the same.\n",
    "</p>\n",
    "<p>\n",
    "In Fig. 5 and 6, we show examples and training curves of\n",
    "the compared methods. In the image generated by the EncConcat-Dec baseline (Fig. 5 (d)), the object contours of the\n",
    "style image can be clearly observed, suggesting that the network fails to disentangle the style information from the content of the style image. This is also consistent with Fig. 6,\n",
    "where Enc-Concat-Dec can reach low style loss but fail to\n",
    "decrease the content loss. Models with BN/IN layers also\n",
    "obtain qualitatively worse results and consistently higher\n",
    "losses. The results with IN layers are especially poor. This\n",
    "once again verifies our claim that IN layers tend to normalize the output to a single style and thus should be avoided\n",
    "when we want to generate images in different styles.\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **7.3. Runtime controls**\n",
    "<p>\n",
    "To further highlight the flexibility of our method, we\n",
    "show that our style transfer network allows users to control the degree of stylization, interpolate between different\n",
    "styles, transfer styles while preserving colors, and use different styles in different spatial regions. Note that all these\n",
    "controls are only applied at runtime using the same network,\n",
    "without any modification to the training procedure.\n",
    "</p>\n",
    "<p>\n",
    "<strong>Content-style trade-off</strong>. The degree of style transfer can\n",
    "be controlled during training by adjusting the style weight\n",
    "λ in Eqa. 11. In addition, our method allows content-style\n",
    "trade-off at test time by interpolating between feature maps\n",
    "that are fed to the decoder. Note that this is equivalent to interpolating between the affine parameters of AdaIN.\n",
    "</p>\n",
    "<table>\n",
    "    <tbody>\n",
    "        <tr>\n",
    "            <td>\n",
    "                <img src=\"./imgs/figure5.png\" width=\"500\" />\n",
    "            </td>\n",
    "            <td>\n",
    "                <img src=\"./imgs/figure6.png\" width=\"500\" />\n",
    "            </td>\n",
    "        </tr>\n",
    "    </tbody>\n",
    "</table>\n",
    "<table>\n",
    "    <tbody>\n",
    "        <tr>\n",
    "            <td>\n",
    "                <img src=\"./imgs/equation14.png\" width=\"400\" />\n",
    "            </td>\n",
    "        </tr>\n",
    "    </tbody>\n",
    "</table>\n",
    "<p>\n",
    "The network tries to faithfully reconstruct the content\n",
    "image when α = 0, and to synthesize the most stylized\n",
    "image when α = 1. As shown in Fig. 7, a smooth transition between content-similarity and style-similarity can be\n",
    "observed by changing α from 0 to 1.\n",
    "</p>\n",
    "<p>\n",
    "<strong>Style interpolation.</strong> To interpolate between a set of\n",
    "K style images s1, s2, ..., sK with corresponding weights\n",
    "w1, w2, ..., wK such that PK\n",
    "k=1 wk = 1, we similarly interpolate between feature maps (results shown in Fig. 8):\n",
    "</p>\n",
    "<table>\n",
    "    <tbody>\n",
    "        <tr>\n",
    "            <td>\n",
    "                <img src=\"./imgs/equation15.png\" width=\"400\" />\n",
    "            </td>\n",
    "        </tr>\n",
    "    </tbody>\n",
    "</table>\n",
    "<p>\n",
    "<img src=\"./imgs/figure7.png\" />\n",
    "<table>\n",
    "    <tbody>\n",
    "        <tr>\n",
    "            <td>\n",
    "                <img src=\"./imgs/figure8.png\" />\n",
    "            </td>\n",
    "        </tr>\n",
    "    </tbody>\n",
    "</table>\n",
    "<strong>Spatial and color control.</strong> Gatys et al. [17] recently introduced user controls over color information and spatial locations of style transfer, which can be easily incorporated into\n",
    "our framework. To preserve the color of the content image,\n",
    "we first match the color distribution of the style image to that of the content image (similar to [17]), then perform a\n",
    "normal style transfer using the color-aligned style image as\n",
    "the style input. Examples results are shown in Fig. 9.\n",
    "</p>\n",
    "<p>\n",
    "In Fig. 10 we demonstrate that our method can transfer different regions of the content image to different styles.\n",
    "This is achieved by performing AdaIN separately to different regions in the content feature maps using statistics from\n",
    "different style inputs, similar to [4, 17] but in a completely\n",
    "feed-forward manner. While our decoder is only trained on\n",
    "inputs with homogeneous styles, it generalizes naturally to\n",
    "inputs in which different regions have different styles.\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **8. Discussion and Conclusion**\n",
    "<p>\n",
    "In this paper, we present a simple adaptive instance normalization (AdaIN) layer that for the first time enables arbitrary style transfer in real-time. Beyond the fascinating\n",
    "applications, we believe this work also sheds light on our\n",
    "understanding of deep image representations in general.\n",
    "</p>\n",
    "<p>\n",
    "It is interesting to consider the conceptual differences between our approach and previous neural style transfer methods based on feature statistics. Gatys et al. [16] employ an\n",
    "optimization process to manipulate pixel values to match\n",
    "feature statistics. The optimization process is replaced by\n",
    "feed-forward neural networks in [24, 51, 52]. Still, the network is trained to modify pixel values to indirectly match\n",
    "feature statistics. We adopt a very different approach that\n",
    "directly aligns statistics in the feature space in one shot, then\n",
    "inverts the features back to the pixel space.\n",
    "</p>\n",
    "<p>\n",
    "Given the simplicity of our approach, we believe there is\n",
    "still substantial room for improvement. In future works we\n",
    "plan to explore more advanced network architectures such\n",
    "as the residual architecture [24] or an architecture with additional skip connections from the encoder [23]. We also plan\n",
    "to investigate more complicated training schemes like the\n",
    "incremental training [32]. Moreover, our AdaIN layer only\n",
    "aligns the most basic feature statistics (mean and variance).\n",
    "It is possible that replacing AdaIN with correlation alignment [49] or histogram matching [54] could further improve\n",
    "quality by transferring higher-order statistics. Another interesting direction is to apply AdaIN to texture synthesis.\n",
    "</p>\n",
    "<p><strong>Acknowledgments</strong></p>\n",
    "<p>\n",
    "We would like to thank Andreas Veit for helpful discussions. This work was supported in part by a Google Focused Research Award, AWS Cloud Credits for Research\n",
    "and a Facebook equipment donation.\n",
    "</p>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./imgs/figure11.png\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **References**\n",
    "\n",
    "[1] M. Arjovsky, S. Chintala, and L. Bottou. Wasserstein gan.\n",
    "arXiv preprint arXiv:1701.07875, 2017. 2\n",
    "\n",
    "[2] J. L. Ba, J. R. Kiros, and G. E. Hinton. Layer normalization.\n",
    "arXiv preprint arXiv:1607.06450, 2016. 2\n",
    "\n",
    "[3] K. Bousmalis, N. Silberman, D. Dohan, D. Erhan, and\n",
    "D. Krishnan. Unsupervised pixel-level domain adaptation with generative adversarial networks. arXiv preprint\n",
    "arXiv:1612.05424, 2016. 2\n",
    "\n",
    "[4] A. J. Champandard. Semantic style transfer and turning two-bit doodles into fine artworks. arXiv preprint\n",
    "arXiv:1603.01768, 2016. 8\n",
    "\n",
    "[5] D. Chen, L. Yuan, J. Liao, N. Yu, and G. Hua. Stylebank:\n",
    "An explicit representation for neural image style transfer. In\n",
    "CVPR, 2017. 1\n",
    "\n",
    "[6] T. Q. Chen and M. Schmidt. Fast patch-based style transfer\n",
    "of arbitrary style. arXiv preprint arXiv:1612.04337, 2016. 1,\n",
    "2, 4, 5, 6, 7\n",
    "\n",
    "[7] R. Collobert, K. Kavukcuoglu, and C. Farabet. Torch7:\n",
    "A matlab-like environment for machine learning. In NIPS\n",
    "Workshop, 2011. 4\n",
    "\n",
    "[8] T. Cooijmans, N. Ballas, C. Laurent, C¸ . Gulc¸ehre, and ¨\n",
    "A. Courville. Recurrent batch normalization. In ICLR, 2017.\n",
    "2\n",
    "\n",
    "[9] E. L. Denton, S. Chintala, R. Fergus, et al. Deep generative image models using a laplacian pyramid of adversarial\n",
    "networks. In NIPS, 2015. 2\n",
    "\n",
    "[10] A. Dosovitskiy and T. Brox. Inverting visual representations\n",
    "with convolutional networks. In CVPR, 2016. 4\n",
    "\n",
    "[11] V. Dumoulin, J. Shlens, and M. Kudlur. A learned representation for artistic style. In ICLR, 2017. 1, 2, 3, 5, 6, 7\n",
    "\n",
    "[12] A. A. Efros and W. T. Freeman. Image quilting for texture\n",
    "synthesis and transfer. In SIGGRAPH, 2001. 1\n",
    "\n",
    "[13] A. A. Efros and T. K. Leung. Texture synthesis by nonparametric sampling. In ICCV, 1999. 1\n",
    "\n",
    "[14] M. Elad and P. Milanfar. Style-transfer via texture-synthesis.\n",
    "arXiv preprint arXiv:1609.03057, 2016. 1\n",
    "\n",
    "[15] O. Frigo, N. Sabater, J. Delon, and P. Hellier. Split and\n",
    "match: example-based adaptive patch sampling for unsupervised style transfer. In CVPR, 2016. 1\n",
    "\n",
    "[16] L. A. Gatys, A. S. Ecker, and M. Bethge. Image style transfer\n",
    "using convolutional neural networks. In CVPR, 2016. 1, 2,\n",
    "3, 5, 6, 7, 8\n",
    "\n",
    "[17] L. A. Gatys, A. S. Ecker, M. Bethge, A. Hertzmann, and\n",
    "E. Shechtman. Controlling perceptual factors in neural style\n",
    "transfer. In CVPR, 2017. 1, 7, 8\n",
    "\n",
    "[18] I. Goodfellow, J. Pouget-Abadie, M. Mirza, B. Xu,\n",
    "D. Warde-Farley, S. Ozair, A. Courville, and Y. Bengio. Generative adversarial nets. In NIPS, 2014. 2\n",
    "\n",
    "[19] D. J. Heeger and J. R. Bergen. Pyramid-based texture analysis/synthesis. In SIGGRAPH, 1995. 1\n",
    "\n",
    "[20] X. Huang, Y. Li, O. Poursaeed, J. Hopcroft, and S. Belongie.\n",
    "Stacked generative adversarial networks. In CVPR, 2017. 2\n",
    "\n",
    "[21] S. Ioffe. Batch renormalization: Towards reducing minibatch\n",
    "dependence in batch-normalized models. arXiv preprint\n",
    "arXiv:1702.03275, 2017. 2\n",
    "\n",
    "[22] S. Ioffe and C. Szegedy. Batch normalization: Accelerating\n",
    "deep network training by reducing internal covariate shift. In\n",
    "JMLR, 2015. 2\n",
    "\n",
    "[23] P. Isola, J.-Y. Zhu, T. Zhou, and A. A. Efros. Image-to-image\n",
    "translation with conditional adversarial networks. In CVPR,\n",
    "2017. 2, 8\n",
    "\n",
    "[24] J. Johnson, A. Alahi, and L. Fei-Fei. Perceptual losses for\n",
    "real-time style transfer and super-resolution. In ECCV, 2016.\n",
    "1, 2, 3, 8\n",
    "\n",
    "[25] T. Kim, M. Cha, H. Kim, J. Lee, and J. Kim. Learning to\n",
    "discover cross-domain relations with generative adversarial\n",
    "networks. arXiv preprint arXiv:1703.05192, 2017. 2\n",
    "\n",
    "[26] D. Kingma and J. Ba. Adam: A method for stochastic optimization. In ICLR, 2015. 5\n",
    "\n",
    "[27] D. P. Kingma and M. Welling. Auto-encoding variational\n",
    "bayes. In ICLR, 2014. 2\n",
    "\n",
    "[28] J. E. Kyprianidis, J. Collomosse, T. Wang, and T. Isenberg.\n",
    "State of the” art: A taxonomy of artistic stylization techniques for images and video. TVCG, 2013. 1\n",
    "\n",
    "[29] C. Laurent, G. Pereyra, P. Brakel, Y. Zhang, and Y. Bengio. Batch normalized recurrent neural networks. In ICASSP,\n",
    "2016. 2\n",
    "\n",
    "[30] C. Li and M. Wand. Combining markov random fields and\n",
    "convolutional neural networks for image synthesis. In CVPR,\n",
    "2016. 1, 2, 3\n",
    "\n",
    "[31] C. Li and M. Wand. Precomputed real-time texture synthesis\n",
    "with markovian generative adversarial networks. In ECCV,\n",
    "2016. 1, 2\n",
    "\n",
    "[32] Y. Li, C. Fang, J. Yang, Z. Wang, X. Lu, and M.-H. Yang.\n",
    "Diversified texture synthesis with feed-forward networks. In\n",
    "CVPR, 2017. 1, 2, 8\n",
    "\n",
    "[33] Y. Li, N. Wang, J. Liu, and X. Hou. Demystifying neural\n",
    "style transfer. arXiv preprint arXiv:1701.01036, 2017. 1, 2,\n",
    "3, 5\n",
    "\n",
    "[34] Y. Li, N. Wang, J. Shi, J. Liu, and X. Hou. Revisiting\n",
    "batch normalization for practical domain adaptation. arXiv\n",
    "preprint arXiv:1603.04779, 2016. 2\n",
    "\n",
    "[35] Q. Liao, K. Kawaguchi, and T. Poggio. Streaming normalization: Towards simpler and more biologically-plausible\n",
    "normalizations for online and recurrent learning. arXiv\n",
    "preprint arXiv:1610.06160, 2016. 2\n",
    "\n",
    "[36] T.-Y. Lin, M. Maire, S. Belongie, J. Hays, P. Perona, D. Ramanan, P. Dollar, and C. L. Zitnick. Microsoft coco: Com- ´\n",
    "mon objects in context. In ECCV, 2014. 3, 5\n",
    "\n",
    "[37] M.-Y. Liu, T. Breuel, and J. Kautz. Unsupervised\n",
    "image-to-image translation networks. arXiv preprint\n",
    "arXiv:1703.00848, 2017. 2\n",
    "\n",
    "[38] M.-Y. Liu and O. Tuzel. Coupled generative adversarial networks. In NIPS, 2016. 2\n",
    "\n",
    "[39] K. Nichol. Painter by numbers, wikiart. https://www.\n",
    "kaggle.com/c/painter-by-numbers, 2016. 5\n",
    "\n",
    "[40] A. v. d. Oord, N. Kalchbrenner, and K. Kavukcuoglu. Pixel\n",
    "recurrent neural networks. In ICML, 2016. 2\n",
    "\n",
    "[41] X. Peng and K. Saenko. Synthetic to real adaptation\n",
    "with deep generative correlation alignment networks. arXiv\n",
    "preprint arXiv:1701.05524, 2017. 2\n",
    "\n",
    "[42] A. Radford, L. Metz, and S. Chintala. Unsupervised representation learning with deep convolutional generative adversarial networks. In ICLR, 2016. 2\n",
    "\n",
    "[43] S. Reed, Z. Akata, X. Yan, L. Logeswaran, B. Schiele, and\n",
    "H. Lee. Generative adversarial text to image synthesis. In\n",
    "ICML, 2016. 2\n",
    "\n",
    "[44] M. Ren, R. Liao, R. Urtasun, F. H. Sinz, and R. S. Zemel.\n",
    "Normalizing the normalizers: Comparing and extending network normalization schemes. In ICLR, 2017. 2\n",
    "\n",
    "[45] M. Ruder, A. Dosovitskiy, and T. Brox. Artistic style transfer\n",
    "for videos. In GCPR, 2016. 1\n",
    "\n",
    "[46] T. Salimans, I. Goodfellow, W. Zaremba, V. Cheung, A. Radford, and X. Chen. Improved techniques for training gans. In\n",
    "NIPS, 2016. 2\n",
    "\n",
    "[47] T. Salimans and D. P. Kingma. Weight normalization: A\n",
    "simple reparameterization to accelerate training of deep neural networks. In NIPS, 2016. 2\n",
    "\n",
    "[48] K. Simonyan and A. Zisserman. Very deep convolutional\n",
    "networks for large-scale image recognition. In ICLR, 2015.\n",
    "4, 5\n",
    "\n",
    "[49] B. Sun, J. Feng, and K. Saenko. Return of frustratingly easy\n",
    "domain adaptation. In AAAI, 2016. 8\n",
    "\n",
    "[50] Y. Taigman, A. Polyak, and L. Wolf. Unsupervised crossdomain image generation. In ICLR, 2017. 2\n",
    "\n",
    "[51] D. Ulyanov, V. Lebedev, A. Vedaldi, and V. Lempitsky. Texture networks: Feed-forward synthesis of textures and stylized images. In ICML, 2016. 1, 2, 4, 5, 8\n",
    "\n",
    "[52] D. Ulyanov, A. Vedaldi, and V. Lempitsky. Improved texture\n",
    "networks: Maximizing quality and diversity in feed-forward\n",
    "stylization and texture synthesis. In CVPR, 2017. 1, 2, 3, 5,\n",
    "6, 7, 8\n",
    "\n",
    "[53] X. Wang, G. Oxholm, D. Zhang, and Y.-F. Wang. Multimodal transfer: A hierarchical deep convolutional neural network for fast artistic style transfer. arXiv preprint\n",
    "arXiv:1612.01895, 2016. 2\n",
    "\n",
    "[54] P. Wilmot, E. Risser, and C. Barnes. Stable and controllable\n",
    "neural texture synthesis and style transfer using histogram\n",
    "losses. arXiv preprint arXiv:1701.08893, 2017. 2, 8\n",
    "\n",
    "[55] H. Zhang and K. Dana. Multi-style generative network for\n",
    "real-time transfer. arXiv preprint arXiv:1703.06953, 2017. 1"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "3efb3a3e09a38f93b1d9bfa4b9b8ce66a167f6ada77b64bc003d7d6d298d81d5"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 64-bit ('buddhalight': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
